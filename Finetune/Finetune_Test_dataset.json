[
    {
        "input_method": "def remove_builtin(self, key, orig):<TAB><TAB><TAB>\"\"\"Remove an added builtin and re-set the original.\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>__builtin__.__dict__[key] = orig",
        "target_block": "if orig is BuiltinUndefined:del __builtin__.__dict__[key]else:"
    },
    {
        "input_method": "def allstack(vals, depth=0):<TAB><TAB>\"\"\"<TAB><TAB>If an ndarray has been split into multiple chunks by splitting it along<TAB><TAB>each axis at a number of locations, this function rebuilds the<TAB><TAB>original array from chunks.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>vals : nested lists of ndarrays<TAB><TAB><TAB>each level of nesting of the lists representing a dimension of<TAB><TAB><TAB>the original array.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>return concatenate([allstack(x, depth+1) for x in vals], axis=depth)",
        "target_block": "if type(vals[0]) is ndarray:return concatenate(vals, axis=depth)else:"
    },
    {
        "input_method": "def _normalize_data(stream, date_format=None):<TAB><TAB>\"\"\"<TAB><TAB>This function is meant to normalize data for upload to the Luminoso<TAB><TAB>Analytics system. Currently it only normalizes dates.<TAB><TAB><TAB>If date_format is not specified, or <extra_id_0><TAB><TAB><TAB><TAB><TAB># ValueErrors cover the cases when date_format does not match<TAB><TAB><TAB><TAB><TAB># the actual format of the date, both for epoch and non-epoch<TAB><TAB><TAB><TAB><TAB># times.<TAB><TAB><TAB><TAB><TAB>logger.exception('%s does not match the date format %s;'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> % (doc['date'], date_format))<TAB><TAB><TAB>yield doc",
        "target_block": "if there's no date in a particular doc,the the doc is yielded unchanged.\"\"\"for doc in stream:if 'date' in doc and date_format is not None:try:doc['date'] = _convert_date(doc['date'], date_format)except ValueError:"
    },
    {
        "input_method": "def mel_to_hz(mels, htk=False):<TAB><TAB>\"\"\"Convert mel bin numbers to frequencies<TAB><TAB><TAB>Examples<TAB><TAB>--------<TAB><TAB>>>> librosa.mel_to_hz(3)<TAB><TAB>200.<TAB><TAB><TAB>>>> librosa.mel_to_hz([1,2,3,4,5])<TAB><TAB>array([  66.667,  133.333,  200.   ,  266.667,  333.333])<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>mels<TAB><TAB>  : np.ndarray [shape=(n,)], float<TAB><TAB><TAB>mel bins to convert<TAB><TAB>htk<TAB><TAB>   : bool<TAB><TAB><TAB>use HTK formula instead of Slaney<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>frequencies   : np.ndarray [shape=(n,)]<TAB><TAB><TAB>input mels in Hz<TAB><TAB><TAB>See Also<TAB><TAB>--------<TAB><TAB>hz_to_mel<TAB><TAB>\"\"\"<TAB><TAB><TAB>mels = np.asanyarray(mels)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB># If we have scalar data, check directly<TAB><TAB><TAB>freqs = min_log_hz * np.exp(logstep * (mels - min_log_mel))<TAB><TAB><TAB>return freqs",
        "target_block": "if htk:return 700.0 * (10.0**(mels / 2595.0) - 1.0)# Fill in the linear scalef_min = 0.0f_sp = 200.0 / 3freqs = f_min + f_sp * mels# And now the nonlinear scalemin_log_hz = 1000.0 # beginning of log region (Hz)min_log_mel = (min_log_hz - f_min) / f_sp   # same (Mels)logstep = np.log(6.4) / 27.0# step size for log regionif mels.ndim:# If we have vector data, vectorizelog_t = (mels >= min_log_mel)freqs[log_t] = min_log_hz * np.exp(logstep * (mels[log_t] - min_log_mel))elif mels >= min_log_mel:"
    },
    {
        "input_method": "def setDecode(self, decodeTable):<TAB><TAB><TAB>\"\"\"Store decodeTable,<TAB><TAB><TAB>and compute lengthTable, minLength, maxLength from encodings.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self.decodeTable = decodeTable<TAB><TAB><TAB>#set of symbols with unknown length<TAB><TAB><TAB>todo = set(decodeTable)<TAB><TAB><TAB>#bit size under investigation<TAB><TAB><TAB>maskLength = 0<TAB><TAB><TAB>lengthTable = {}<TAB><TAB><TAB>while todo:<TAB><TAB><TAB><TAB>mask = (1<<maskLength)-1<TAB><TAB><TAB><TAB>#split the encodings that we didn't find yet using b bits<TAB><TAB><TAB><TAB>splitSymbols = defaultdict(list)<TAB><TAB><TAB><TAB>for s in todo: splitSymbols[s&mask].append(s)<TAB><TAB><TAB><TAB>#unique encodings have a length of maskLength bits<TAB><TAB><TAB><TAB>#set length, and remove from todo list<TAB><TAB><TAB><TAB>for s,subset in splitSymbols.items():<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>lengthTable[self.decodeTable[s]] = maskLength<TAB><TAB><TAB><TAB><TAB><TAB>todo.remove(s)<TAB><TAB><TAB><TAB>#now investigate with longer mask<TAB><TAB><TAB><TAB>maskLength +=1<TAB><TAB><TAB>#save result<TAB><TAB><TAB>self.lengthTable = lengthTable<TAB><TAB><TAB>self.minLength = min(lengthTable.values())<TAB><TAB><TAB>self.maxLength = max(lengthTable.values())<TAB><TAB><TAB>self.switchToPrefix()",
        "target_block": "if len(subset)==1:"
    },
    {
        "input_method": "async def close(self, exception=None):<TAB><TAB><TAB>\"\"\"Close down the receiver connection.<TAB><TAB><TAB><TAB>If the receiver has already closed, this operation will do nothing. An optional<TAB><TAB><TAB>exception can be passed in to indicate that the handler was shutdown due to error.<TAB><TAB><TAB>It is recommended to open a handler within a context manager as<TAB><TAB><TAB>opposed to calling the method directly.<TAB><TAB><TAB>The receiver will be implicitly closed on completion of the message iterator,<TAB><TAB><TAB>however this method will need to be called explicitly <extra_id_0><TAB><TAB><TAB><TAB>return<TAB><TAB><TAB>self.running = False<TAB><TAB><TAB>self.receiver_shutdown = True<TAB><TAB><TAB>self._used.set()<TAB><TAB><TAB>await super(Receiver, self).close(exception=exception)",
        "target_block": "if the message iterator is not runto completion... note:: This operation is not thread-safe.:param exception: An optional exception if the handler is closing due to an error.:type exception: ExceptionExample:.. literalinclude:: ../examples/async_examples/test_examples_async.py:start-after: [START open_close_receiver_directly]:end-before: [END open_close_receiver_directly]:language: python:dedent: 4:caption: Iterate then explicitly close a Receiver.\"\"\"if not self.running:"
    },
    {
        "input_method": "def store_properties(fh, props, comment=None, timestamp=True):<TAB>  \"\"\"<TAB><TAB>Writes properties to the file in Java properties format.<TAB><TAB><TAB>:param fh: a writable file-like object<TAB><TAB>:param props: a mapping (dict) or iterable of key/value pairs<TAB><TAB>:param comment: comment to write to the beginning of the file<TAB><TAB>:param timestamp: boolean indicating whether to write a timestamp comment<TAB>  \"\"\"<TAB>  <extra_id_0><TAB><TAB>  write_property(fh, key, value)",
        "target_block": "if comment is not None:write_comment(fh, comment)  if timestamp:write_comment(fh, time.strftime('%a %b %d %H:%M:%S %Z %Y'))  if hasattr(props, 'keys'):for key in props:  write_property(fh, key, props[key])  else:for key, value in props:"
    },
    {
        "input_method": "def search(self, query, method=\"all\"):<TAB><TAB><TAB>\"\"\"Search this address book for contacts matching the query.<TAB><TAB><TAB><TAB>The method can be one of \"all\", \"name\" and \"uid\".  The backend for this<TAB><TAB><TAB>address book migth be load()ed <extra_id_0><TAB><TAB><TAB><TAB>raise ValueError('Only the search methods \"all\", \"name\" and \"uid\" '<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> 'are supported.')<TAB><TAB><TAB>return list(search_function(query))",
        "target_block": "if needed.:param query: the query to search for:type query: str:param method: the type of fileds to use when seaching:type method: str:returns: all found contacts:rtype: list(carddav_object.CarddavObject)\"\"\"logging.debug('address book %s, searching with %s', self.name, query)if not self._loaded:self.load(query)if method == \"all\":search_function = self._search_allelif method == \"name\":search_function = self._search_nameselif method == \"uid\":search_function = self._search_uidelse:"
    },
    {
        "input_method": "def set_verify_depth(self, depth):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Set the maximum depth for the certificate chain verification that shall<TAB><TAB><TAB>be allowed for this Context object.<TAB><TAB><TAB><TAB>:param depth: An integer specifying the verify depth<TAB><TAB><TAB>:return: None<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise TypeError(\"depth must be an integer\")<TAB><TAB><TAB><TAB>_lib.SSL_CTX_set_verify_depth(self._context, depth)",
        "target_block": "if not isinstance(depth, integer_types):"
    },
    {
        "input_method": "def _verify_archive(self):<TAB><TAB><TAB>\"\"\"Check whether the archive is valid or not.<TAB><TAB><TAB><TAB>This method will check <extra_id_0> %s rows\",<TAB><TAB><TAB><TAB><TAB><TAB> self.archive_path, nentries, nmetadata)",
        "target_block": "if tables were created and if theycontain valid data.\"\"\"nentries = self._count_table_rows(self.ARCHIVE_TABLE)nmetadata = self._count_table_rows(self.METADATA_TABLE)if nmetadata > 1:msg = \"archive %s metadata corrupted; multiple metadata entries\" % (self.archive_path)raise ArchiveError(cause=msg)if nmetadata == 0 and nentries > 0:msg = \"archive %s metadata is empty but %s entries were achived\" % (self.archive_path)raise ArchiveError(cause=msg)logger.debug(\"Integrity of archive %s OK; entries: %s rows, metadata:"
    },
    {
        "input_method": "def verify_integrity(self, session=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Verifies the DagRun by checking for removed tasks or tasks that are not in the<TAB><TAB><TAB>database yet. It will set state to removed or add the task <extra_id_0><TAB><TAB><TAB><TAB><TAB>Stats.incr(<TAB><TAB><TAB><TAB><TAB><TAB>\"task_instance_created-{}\".format(task.__class__.__name__),<TAB><TAB><TAB><TAB><TAB><TAB>1, 1)<TAB><TAB><TAB><TAB><TAB>ti = TaskInstance(task, self.execution_date)<TAB><TAB><TAB><TAB><TAB>session.add(ti)<TAB><TAB><TAB><TAB>session.commit()",
        "target_block": "if required.\"\"\"from airflow.models.taskinstance import TaskInstance  # Avoid circular importdag = self.get_dag()tis = self.get_task_instances(session=session)# check for removed or restored taskstask_ids = []for ti in tis:task_ids.append(ti.task_id)task = Nonetry:task = dag.get_task(ti.task_id)except AirflowException:if ti.state == State.REMOVED:pass  # ti has already been removed, just ignore itelif self.state is not State.RUNNING and not dag.partial:self.log.warning(\"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\".format(ti, dag))Stats.incr(\"task_removed_from_dag.{}\".format(dag.dag_id), 1, 1)ti.state = State.REMOVEDis_task_in_dag = task is not Noneshould_restore_task = is_task_in_dag and ti.state == State.REMOVEDif should_restore_task:self.log.info(\"Restoring task '{}' which was previously \"  \"removed from DAG '{}'\".format(ti, dag))Stats.incr(\"task_restored_to_dag.{}\".format(dag.dag_id), 1, 1)ti.state = State.NONE# check for missing tasksfor task in six.itervalues(dag.task_dict):if task.start_date > self.execution_date and not self.is_backfill:continueif task.task_id not in task_ids:"
    },
    {
        "input_method": "def parse_rank_score(rank_score_entry, case_id):<TAB><TAB>\"\"\"Parse the rank score<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>rank_score_entry(str): The raw rank score entry<TAB><TAB><TAB><TAB>case_id(str)<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>rank_score(float)<TAB><TAB>\"\"\"<TAB><TAB>rank_score = None<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>rank_score = float(splitted_info[1])<TAB><TAB>return rank_score",
        "target_block": "if rank_score_entry:for family_info in rank_score_entry.split(','):splitted_info = family_info.split(':')if case_id == splitted_info[0]:"
    },
    {
        "input_method": "def create_startup_config(self):<TAB><TAB><TAB>\"\"\" Startup and shutdown commands config<TAB><TAB><TAB>Used by agent.py on the target<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>cfg_path = \"agent_startup_{}.cfg\".format(self.host)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>logger.error(<TAB><TAB><TAB><TAB><TAB>'Error trying to create monitoring startups config. Malformed? %s',<TAB><TAB><TAB><TAB><TAB>exc,<TAB><TAB><TAB><TAB><TAB>exc_info=True)<TAB><TAB><TAB>return cfg_path",
        "target_block": "if os.path.isfile(cfg_path):logger.info('Found agent startup config file in working directory with the same name as created for host %s.\\n''Creating new one via tempfile. This will affect predictable filenames for agent artefacts',self.host)handle, cfg_path = tempfile.mkstemp('.cfg', 'agent_')os.close(handle)try:config = ConfigParser.RawConfigParser()# FIXME incinerate such a string formatting inside a method call# T_Tconfig.add_section('startup')[config.set('startup', \"cmd%s\" % idx, cmd)for idx, cmd in enumerate(self.startups)]config.add_section('shutdown')[config.set('shutdown', \"cmd%s\" % idx, cmd)for idx, cmd in enumerate(self.shutdowns)]config.add_section('source')[config.set('source', \"file%s\" % idx, path)for idx, path in enumerate(self.sources)]with open(cfg_path, 'w') as fds:config.write(fds)except Exception as exc:"
    },
    {
        "input_method": "def print_inplace(msg):<TAB><TAB>\"\"\"Clears out the previous line and prints a new one.\"\"\"<TAB><TAB>term_width = get_terminal_size().columns<TAB><TAB>spacing = term_width - terminal_width(msg)<TAB><TAB><TAB># On windows we need one less space or we overflow the line for some reason.<TAB><TAB><extra_id_0><TAB><TAB><TAB>spacing -= 1<TAB><TAB><TAB>sys.stderr.write(\"\\r{0}\".format(msg))<TAB><TAB>sys.stderr.write(\" \" * max(0, spacing))<TAB><TAB>sys.stderr.flush()",
        "target_block": "if is_win32:"
    },
    {
        "input_method": "def _fitch_anc(self, **kwargs):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Reconstruct ancestral states using Fitch's algorithm. The method requires<TAB><TAB><TAB>sequences to be assigned to leaves. It implements the iteration from<TAB><TAB><TAB>leaves to the root constructing the Fitch profiles for each character of<TAB><TAB><TAB>the sequence, and then by propagating from the root to the leaves,<TAB><TAB><TAB>reconstructs the sequences of the internal nodes.<TAB><TAB><TAB><TAB>Keyword Args<TAB><TAB><TAB>------------<TAB><TAB><TAB><TAB>Returns<TAB><TAB><TAB>-------<TAB><TAB><TAB>Ndiff : int<TAB><TAB><TAB>   Number of the characters that changed since the previous<TAB><TAB><TAB>   reconstruction. These changes are determined from the pre-set<TAB><TAB><TAB>   sequence attributes of the nodes. If there are no sequences available<TAB><TAB><TAB>   (i.e., no reconstruction has been made before), returns the total<TAB><TAB><TAB>   number of characters in the tree.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB># set fitch profiiles to each terminal node<TAB><TAB><TAB><TAB>for l in self.tree.get_terminals():<TAB><TAB><TAB><TAB>l.state = [[k] for k in l.cseq]<TAB><TAB><TAB><TAB>L = len(self.tree.get_terminals()[0].cseq)<TAB><TAB><TAB><TAB>self.logger(\"TreeAnc._fitch_anc: Walking up the tree, creating the Fitch profiles\",2)<TAB><TAB><TAB>for node in self.tree.get_nonterminals(order='postorder'):<TAB><TAB><TAB><TAB>node.state = [self._fitch_state(node, k) for k in range(L)]<TAB><TAB><TAB><TAB>ambs = [i for i in range(L) <extra_id_0><TAB><TAB><TAB><TAB>node.profile = seq2prof(node.original_cseq, self.gtr.profile_map)<TAB><TAB><TAB>return N_diff",
        "target_block": "if len(self.tree.root.state[i])>1]if len(ambs) > 0:for amb in ambs:self.logger(\"Ambiguous state of the root sequence \"\"in the position %d: %s, \"\"choosing %s\" % (amb, str(self.tree.root.state[amb]), self.tree.root.state[amb][0]), 4)self.tree.root.cseq = np.array([k[np.random.randint(len(k)) if len(k)>1 else 0]   for k in self.tree.root.state])if self.is_vcf:self.tree.root.sequence = self.dict_sequence(self.tree.root)else:self.tree.root.sequence = self.expanded_sequence(self.tree.root)self.logger(\"TreeAnc._fitch_anc: Walking down the self.tree, generating sequences from the \" \"Fitch profiles.\", 2)N_diff = 0for node in self.tree.get_nonterminals(order='preorder'):if node.up != None: # not rootsequence =  np.array([node.up.cseq[i]if node.up.cseq[i] in node.state[i]else node.state[i][0] for i in range(L)])if hasattr(node, 'sequence'):N_diff += (sequence!=node.cseq).sum()else:N_diff += Lnode.cseq = sequenceif self.is_vcf:node.sequence = self.dict_sequence(node)else:node.sequence = self.expanded_sequence(node)node.mutations = self.get_mutations(node)node.profile = seq2prof(node.cseq, self.gtr.profile_map)del node.state # no need to store Fitch statesself.logger(\"Done ancestral state reconstruction\",3)for node in self.tree.get_terminals():"
    },
    {
        "input_method": "def push_chunk(self, x, timestamp=0.0, pushthrough=True):<TAB><TAB><TAB>\"\"\"Push a list of samples into the outlet.<TAB><TAB><TAB><TAB>samples -- A list of samples, either as a list of lists or a list of  <TAB><TAB><TAB><TAB><TAB>   multiplexed values.<TAB><TAB><TAB>timestamp -- Optionally the capture time of the most recent sample, in <TAB><TAB><TAB><TAB><TAB><TAB> agreement with local_clock(); <extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>raise ValueError(\"each sample must have the same number of \"<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> \"channels.\")",
        "target_block": "if omitted, the current  time is used. The time stamps of other samples are  automatically derived according to the sampling rate of  the stream. (default 0.0)pushthrough Whether to push the chunk through to the receivers instead of buffering it with subsequent samples. Note that the chunk_size, if specified at outlet construction, takes precedence over the pushthrough flag. (default True)\"\"\"try:n_values = self.channel_count * len(x)data_buff = (self.value_type * n_values).from_buffer(x)handle_error(self.do_push_chunk(self.obj, data_buff,c_long(n_values),c_double(timestamp),c_int(pushthrough)))except TypeError:if len(x):if type(x[0]) is list:x = [v for sample in x for v in sample]if self.channel_format == cf_string:x = [v.encode('utf-8') for v in x]if len(x) % self.channel_count == 0:constructor = self.value_type*len(x)# noinspection PyCallingNonCallablehandle_error(self.do_push_chunk(self.obj, constructor(*x),c_long(len(x)),c_double(timestamp),c_int(pushthrough)))else:"
    },
    {
        "input_method": "def get_short_uid_dict(self, query=None):<TAB><TAB><TAB>\"\"\"Create a dictionary of shortend UIDs for all contacts.<TAB><TAB><TAB><TAB>All arguments are only used <extra_id_0>same1 + 1]] = self.contacts[item1]<TAB><TAB><TAB>return self._short_uids",
        "target_block": "if the address book is not yet initializedand will just be handed to self.load().:param query: see self.load():type query: str:returns: the contacts mapped by the shortes unique prefix of their UID:rtype: dict(str: CarddavObject)\"\"\"if self._short_uids is None:if not self._loaded:self.load(query)if not self.contacts:self._short_uids = {}elif len(self.contacts) == 1:self._short_uids = {uid[0:1]: contactfor uid, contact in self.contacts.items()}else:self._short_uids = {}sorted_uids = sorted(self.contacts)# Prepare for the loop; the first and last items are handled# seperatly.item0, item1 = sorted_uids[:2]same1 = self._compare_uids(item0, item1)self._short_uids[item0[:same1 + 1]] = self.contacts[item0]for item_new in sorted_uids[2:]:# shift the items and the common prefix lenght one furtheritem0, item1 = item1, item_newsame0, same1 = same1, self._compare_uids(item0, item1)# compute the final prefix length for item1same = max(same0, same1)self._short_uids[item0[:same + 1]] = self.contacts[item0]# Save the last item.self._short_uids[item1[:"
    },
    {
        "input_method": "def _insert_continuation_prompt(self, cursor):<TAB><TAB><TAB>\"\"\" Inserts new continuation prompt using the specified cursor.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self._continuation_prompt = self._insert_html_fetching_plain_text(<TAB><TAB><TAB><TAB><TAB>cursor, self._continuation_prompt_html)",
        "target_block": "if self._continuation_prompt_html is None:self._insert_plain_text(cursor, self._continuation_prompt)else:"
    },
    {
        "input_method": "def compute_labels(X, rank, R, bound_idxs, niter=300):<TAB><TAB>\"\"\"Computes the labels using the bounds.\"\"\"<TAB><TAB><TAB>try:<TAB><TAB><TAB>F, G = cnmf(X, rank, niter=niter, hull=False)<TAB><TAB>except:<TAB><TAB><TAB>return [1]<TAB><TAB><TAB>label_frames = filter_activation_matrix(G.T, R)<TAB><TAB>label_frames = np.asarray(label_frames, dtype=int)<TAB><TAB><TAB>#labels = [label_frames[0]]<TAB><TAB>labels = []<TAB><TAB>bound_inters = zip(bound_idxs[:-1], bound_idxs[1:])<TAB><TAB>for bound_inter in bound_inters:<TAB><TAB><TAB><extra_id_0> bound_inter[1]]))<TAB><TAB><TAB>#print bound_inter, labels[-1]<TAB><TAB>#labels.append(label_frames[-1])<TAB><TAB><TAB>return labels",
        "target_block": "if bound_inter[1] - bound_inter[0] <= 0:labels.append(np.max(label_frames) + 1)else:labels.append(most_frequent(label_frames[bound_inter[0]:"
    },
    {
        "input_method": "def _load_metadata(self):<TAB><TAB><TAB>\"\"\"Load metadata from the archive file\"\"\"<TAB><TAB><TAB><TAB>logger.debug(\"Loading metadata infomation of archive %s\", self.archive_path)<TAB><TAB><TAB><TAB>cursor = self._db.cursor()<TAB><TAB><TAB>select_stmt = \"SELECT origin, backend_name, backend_version, \" \\<TAB><TAB><TAB><TAB><TAB><TAB>  \"category, backend_params, created_on \" \\<TAB><TAB><TAB><TAB><TAB><TAB>  \"FROM \" + self.METADATA_TABLE + \" \" \\<TAB><TAB><TAB><TAB><TAB><TAB>  \"LIMIT 1\"<TAB><TAB><TAB>cursor.execute(select_stmt)<TAB><TAB><TAB>row = cursor.fetchone()<TAB><TAB><TAB>cursor.close()<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>logger.debug(\"Metadata of archive %s was empty\", self.archive_path)<TAB><TAB><TAB><TAB>logger.debug(\"Metadata of archive %s loaded\", self.archive_path)",
        "target_block": "if row:self.origin = row[0]self.backend_name = row[1]self.backend_version = row[2]self.category = row[3]self.backend_params = pickle.loads(row[4])self.created_on = str_to_datetime(row[5])else:"
    },
    {
        "input_method": "def validate_verifier(self, client_key, token, verifier, request):<TAB><TAB><TAB>\"\"\"Validate verifier exists.\"\"\"<TAB><TAB><TAB>log.debug('Validate verifier %r for %r', verifier, client_key)<TAB><TAB><TAB>data = self._verifiergetter(verifier=verifier, token=token)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return data.client_key == client_key<TAB><TAB><TAB>return True",
        "target_block": "if not data:return Falseif not hasattr(data, 'user'):log.debug('Verifier should has user attribute')return Falserequest.user = data.userif hasattr(data, 'client_key'):"
    },
    {
        "input_method": "def HStruct_unpack(structT, data, getDataFn=None, dataWidth=None):<TAB><TAB>\"\"\"<TAB><TAB>opposite of packAxiSFrame<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>assert actual._dtype.bit_length(<TAB><TAB><TAB>) - actualOffset < dataWidth, \"It should be just a padding at the end of frame\"<TAB><TAB><TAB>return val",
        "target_block": "if getDataFn is None:assert dataWidth is not Nonedef _getDataFn(x):return toHVal(x)._auto_cast(Bits(dataWidth))getDataFn = _getDataFnval = structT.fromPy(None)fData = iter(data)# actual is storage variable for items from frameDataactualOffset = 0actual = Nonefor v in walkFlattenFields(val, skipPadding=False):# walk flatten fields and take values from fData and parse them to# fieldrequired = v._dtype.bit_length()if actual is None:actualOffset = 0try:actual = getDataFn(next(fData))except StopIteration:raise Exception(\"Input data too short\")if dataWidth is None:dataWidth = actual._dtype.bit_length()actuallyHave = dataWidthelse:actuallyHave = actual._dtype.bit_length() - actualOffsetwhile actuallyHave < required:# collect data for this fieldtry:d = getDataFn(next(fData))except StopIteration:raise Exception(\"Input data too short\")actual = d._concat(actual)actuallyHave += dataWidthif actuallyHave >= required:# parse value of actual to field# skip padding_v = actual[(required + actualOffset):actualOffset]_v = _v._auto_cast(v._dtype)v.val = _v.valv.vldMask = _v.vldMaskv.updateTime = _v.updateTime# update slice out what was takenactuallyHave -= requiredactualOffset += requiredif actuallyHave == 0:actual = Noneif actual is not None:"
    },
    {
        "input_method": "def create_build(self, tarball_url, env=None, app_name=None):<TAB><TAB><TAB>\"\"\"Creates an app-setups build. Returns response data as a dict.<TAB><TAB><TAB><TAB>:param tarball_url: URL of a tarball containing an ``app.json``.<TAB><TAB><TAB>:param env: Dict containing environment variable overrides.<TAB><TAB><TAB>:param app_name: Name of the Heroku app to create.<TAB><TAB><TAB>:returns: Response data as a ``dict``.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>data = {<TAB><TAB><TAB><TAB>'source_blob': {<TAB><TAB><TAB><TAB><TAB>'url': tarball_url<TAB><TAB><TAB><TAB>}<TAB><TAB><TAB>}<TAB><TAB><TAB><TAB><extra_id_0> app_name}<TAB><TAB><TAB><TAB>return self.api_request('POST', '/app-setups', data=data)",
        "target_block": "if env:data['overrides'] = {'env': env}if app_name:data['app'] = {'name':"
    },
    {
        "input_method": "def peng_frac(snum):<TAB><TAB>r\"\"\"<TAB><TAB>Return the fractional part of a number represented in engineering notation.<TAB><TAB><TAB>:param snum: Number<TAB><TAB>:type  snum: :ref:`EngineeringNotationNumber`<TAB><TAB><TAB>:rtype: integer<TAB><TAB><TAB>.. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]<TAB><TAB>.. Auto-generated exceptions documentation for<TAB><TAB>.. peng.functions.peng_frac<TAB><TAB><TAB>:raises: RuntimeError (Argument \\`snum\\` is not valid)<TAB><TAB><TAB>.. [[[end]]]<TAB><TAB><TAB>For example:<TAB><TAB><TAB><TAB>>>> import peng<TAB><TAB><TAB>>>> peng.peng_frac(peng.peng(1235.6789E3, 3, False))<TAB><TAB><TAB>236<TAB><TAB>\"\"\"<TAB><TAB>snum = snum.rstrip()<TAB><TAB>pindex = snum.find(\".\")<TAB><TAB><extra_id_0> -1])",
        "target_block": "if pindex == -1:return 0return int(snum[pindex + 1 :] if snum[-1].isdigit() else snum[pindex + 1 :"
    },
    {
        "input_method": "def _applyValues(self) -> Generator[None, None, None]:<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Perform delta step by writing stacked values to signals<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>va = self._valuesToApply<TAB><TAB><TAB>self._applyValPlaned = False<TAB><TAB><TAB><TAB># log <extra_id_0><TAB><TAB><TAB><TAB>self._scheduleApplyValues()<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB>yield",
        "target_block": "if there are items to loglav = self.config.logApplyingValuesif va and lav:lav(self, va)self._valuesToApply = []# apply values to signals, values can overwrite each other# but each signal should be driven by only one process and# it should resolve value collisionaddSp = self._seqProcsToRun.appendfor s, vUpdater, isEventDependent, comesFrom in va:if isEventDependent:# now=0 and this was process initialization or async regaddSp(comesFrom)else:# regular combinational processs.simUpdateVal(self, vUpdater)self._runCombProcesses()# processes triggered from simUpdateVal can add new valuesif self._valuesToApply and not self._applyValPlaned:"
    },
    {
        "input_method": "def stream_command_dicts(commands, parallel=False):<TAB><TAB>\"\"\"<TAB><TAB>Takes a list of dictionaries with keys corresponding to ``stream_command``<TAB><TAB>arguments, and runs all concurrently.<TAB><TAB><TAB>:param commands: A list of dictionaries, the keys of which should line up<TAB><TAB><TAB><TAB><TAB><TAB> with the arguments to ``stream_command`` function.<TAB><TAB>:type commands: ``list`` of ``dict``<TAB><TAB>:param parallel: If true, commands will be run in parallel.<TAB><TAB>:type parallel: ``bool``<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>stream_command(**command)",
        "target_block": "if parallel is True:threads = []for command in commands:target = lambda: stream_command(**command)thread = Thread(target=target)thread.start()threads.append(thread)for t in threads:t.join()else:for command in commands:"
    },
    {
        "input_method": "def _login(self, username, password):<TAB><TAB><TAB>'''login and update cached cookies'''<TAB><TAB><TAB>self.logger.debug('login ...')<TAB><TAB><TAB><TAB>res = self.session.http.get(self.login_url)<TAB><TAB><TAB>input_list = self._input_re.findall(res.text)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.logger.error('Failed to login, check your username/password')<TAB><TAB><TAB><TAB>return False",
        "target_block": "if not input_list:raise PluginError('Missing input data on login website.')data = {}for _input_data in input_list:try:_input_name = self._name_re.search(_input_data).group(1)except AttributeError:continuetry:_input_value = self._value_re.search(_input_data).group(1)except AttributeError:_input_value = ''data[_input_name] = _input_valuelogin_data = {'ctl00$Login1$UserName': username,'ctl00$Login1$Password': password,'ctl00$Login1$LoginButton.x': '0','ctl00$Login1$LoginButton.y': '0'}data.update(login_data)res = self.session.http.post(self.login_url, data=data)for cookie in self.session.http.cookies:self._session_attributes.set(cookie.name, cookie.value, expires=3600 * 24)if self._session_attributes.get('ASP.NET_SessionId') and self._session_attributes.get('.abportail1'):self.logger.debug('New session data')self.set_expires_time_cache()return Trueelse:"
    },
    {
        "input_method": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Write the data encoding the Get response payload to a stream.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>output_stream (stream): A data stream in which to encode object<TAB><TAB><TAB><TAB><TAB>data, supporting a write method; usually a BytearrayStream<TAB><TAB><TAB><TAB><TAB>object.<TAB><TAB><TAB><TAB>kmip_version (KMIPVersion): An enumeration defining the KMIP<TAB><TAB><TAB><TAB><TAB>version with which the object will be encoded. Optional,<TAB><TAB><TAB><TAB><TAB>defaults to KMIP 1.0.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>ValueError: Raised <extra_id_0><TAB><TAB><TAB><TAB>raise ValueError(\"Payload is missing the secret field.\")<TAB><TAB><TAB><TAB>self.length = local_stream.length()<TAB><TAB><TAB>super(GetResponsePayload, self).write(<TAB><TAB><TAB><TAB>output_stream,<TAB><TAB><TAB><TAB>kmip_version=kmip_version<TAB><TAB><TAB>)<TAB><TAB><TAB>output_stream.write(local_stream.buffer)",
        "target_block": "if the object type, unique identifier, orsecret attributes are missing from the payload struct.\"\"\"local_stream = utils.BytearrayStream()if self.object_type:self._object_type.write(local_stream, kmip_version=kmip_version)else:raise ValueError(\"Payload is missing the object type field.\")if self.unique_identifier:self._unique_identifier.write(local_stream,kmip_version=kmip_version)else:raise ValueError(\"Payload is missing the unique identifier field.\")if self.secret:self._secret.write(local_stream, kmip_version=kmip_version)else:"
    },
    {
        "input_method": "def buildjschart(self):<TAB><TAB><TAB>\"\"\"generate javascript code for the chart\"\"\"<TAB><TAB><TAB>self.jschart = ''<TAB><TAB><TAB><TAB># add custom tooltip string in jschart<TAB><TAB><TAB># default condition (<extra_id_0><TAB><TAB><TAB><TAB>self.tooltip_condition_string = 'var y = String(graph.point.y);\\n'<TAB><TAB><TAB><TAB># Include data<TAB><TAB><TAB>self.series_js = json.dumps(self.series)",
        "target_block": "if build_custom_tooltip is not called explicitly with date_flag=True)if self.tooltip_condition_string == '':"
    },
    {
        "input_method": "def best_match(self, req, working_set, installer=None):<TAB><TAB><TAB>\"\"\"Find distribution best matching `req` and usable on `working_set`<TAB><TAB><TAB><TAB>This calls the ``find(req)`` method of the `working_set` to see <extra_id_0><TAB><TAB><TAB><TAB><TAB>return dist<TAB><TAB><TAB># try to download/install<TAB><TAB><TAB>return self.obtain(req, installer)",
        "target_block": "if asuitable distribution is already active.  (This may raise``VersionConflict`` if an unsuitable version of the project is alreadyactive in the specified `working_set`.)  If a suitable distributionisn't active, this method returns the newest distribution in theenvironment that meets the ``Requirement`` in `req`.  If no suitabledistribution is found, and `installer` is supplied, then the result ofcalling the environment's ``obtain(req, installer)`` method will bereturned.\"\"\"dist = working_set.find(req)if dist is not None:return distfor dist in self[req.key]:if dist in req:"
    },
    {
        "input_method": "def crscode_to_string(codetype, code, format):<TAB><TAB>\"\"\"<TAB><TAB>Lookup crscode on spatialreference.org and return in specified format.<TAB><TAB><TAB>Arguments:<TAB><TAB><TAB>- *codetype*: \"epsg\", \"esri\", or \"sr-org\".<TAB><TAB>- *code*: The code.<TAB><TAB>- *format*: The crs format of the returned string. One of \"ogcwkt\", \"esriwkt\", or \"proj4\", but also several others...<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>- Crs string in the specified format. <TAB><TAB>\"\"\"<TAB><TAB>link = 'http://spatialreference.org/ref/%s/%s/%s/' %(codetype,code,format)<TAB><TAB>result = urllib2.urlopen(link).read()<TAB><TAB><extra_id_0><TAB><TAB><TAB>result = result.decode()<TAB><TAB>return result",
        "target_block": "if not isinstance(result, str):"
    },
    {
        "input_method": "def find_file(obj):<TAB><TAB>\"\"\"Find the absolute path to the file where an object was defined.<TAB><TAB><TAB>This is essentially a robust wrapper around `inspect.getabsfile`.<TAB><TAB><TAB>Returns None <extra_id_0><TAB><TAB><TAB>pass<TAB><TAB>return fname",
        "target_block": "if no file can be found.Parameters----------obj : any Python objectReturns-------fname : str  The absolute path to the file where the object was defined.\"\"\"# get source if obj was decorated with @decoratorif hasattr(obj, '__wrapped__'):obj = obj.__wrapped__fname = Nonetry:fname = inspect.getabsfile(obj)except TypeError:# For an instance, the file that matters is where its class was# declared.if hasattr(obj, '__class__'):try:fname = inspect.getabsfile(obj.__class__)except TypeError:# Can happen for builtinspassexcept:"
    },
    {
        "input_method": "def _normalize_name(name):<TAB><TAB><TAB>\"\"\"Converts a name to Http-Header-Case.<TAB><TAB><TAB><TAB>>>> HTTPHeaders._normalize_name(\"coNtent-TYPE\")<TAB><TAB><TAB>'Content-Type'<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>return HTTPHeaders._normalized_headers[name]<TAB><TAB><TAB>except KeyError:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>normalized = \"-\".join([w.capitalize() for w in name.split(\"-\")])<TAB><TAB><TAB><TAB>HTTPHeaders._normalized_headers[name] = normalized<TAB><TAB><TAB><TAB>return normalized",
        "target_block": "if HTTPHeaders._NORMALIZED_HEADER_RE.match(name):normalized = nameelse:"
    },
    {
        "input_method": "def compiler_format_extension(self):<TAB><TAB><TAB>\"\"\"Implicit format extension on the asset by its compilers.\"\"\"<TAB><TAB><TAB>for extension, mimetype in self.environment.mimetypes.items():<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>return extension<TAB><TAB><TAB>return None",
        "target_block": "if mimetype == self.compiler_mimetype:"
    },
    {
        "input_method": "def get_dataset(self, name):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Returns info regarding a particular dataset.<TAB><TAB><TAB><TAB>Arugments:<TAB><TAB><TAB><TAB>name (str): Dataset name<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>dict: Dataset information<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>url = self.url() + \"/resource/dataset/{}\".format(name)<TAB><TAB><TAB>req = self.remote_utils.get_url(url)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return req.json()",
        "target_block": "if req.status_code is not 200:raise RemoteDataNotFoundError('Could not find {}'.format(req.text))else:"
    },
    {
        "input_method": "def dispose_orm():<TAB><TAB>\"\"\" Properly close pooled database connections \"\"\"<TAB><TAB>log.debug(\"Disposing DB connection pool (PID %s)\", os.getpid())<TAB><TAB>global engine<TAB><TAB>global Session<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>engine.dispose()<TAB><TAB><TAB>engine = None",
        "target_block": "if Session:Session.remove()Session = Noneif engine:"
    },
    {
        "input_method": "def parse_args(self, arglist=None):<TAB><TAB><TAB>\"\"\"Parse arguments and update options accordingly.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>arglist (list of str): list of arguments to parse. If set to None,<TAB><TAB><TAB><TAB><TAB>``sys.argv[1:]`` is used.<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>:class:`Namespace`: the argument namespace returned by the<TAB><TAB><TAB><TAB>:class:`argparse.ArgumentParser`.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>args = self._parser.parse_args(args=arglist)<TAB><TAB><TAB>sub_cmd = args.loam_sub_name<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>self._conf[sct][opt] = getattr(args, opt, None)<TAB><TAB><TAB>return args",
        "target_block": "if sub_cmd is None:for opt, sct in self._opt_bare.items():self._conf[sct][opt] = getattr(args, opt, None)else:for opt, sct in self._opt_cmds[sub_cmd].items():"
    },
    {
        "input_method": "def _trj_store_explorations(self, traj):<TAB><TAB><TAB>\"\"\"Stores a all explored parameter names for internal recall\"\"\"<TAB><TAB><TAB>nexplored = len(traj._explored_parameters)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>explorations_table.append(rows)<TAB><TAB><TAB><TAB><TAB>explorations_table.flush()",
        "target_block": "if nexplored > 0:if hasattr(self._overview_group, 'explorations'):explorations_table = self._overview_group._f_get_child('explorations')if len(explorations_table) != nexplored:self._hdf5file.remove_node(where=self._overview_group, name='explorations')if not hasattr(self._overview_group, 'explorations'):explored_list = list(traj._explored_parameters.keys())if explored_list:string_col = self._all_get_table_col('explorations',  explored_list,  'overview.explorations')else:string_col = pt.StringCol(1)description = {'explorations': string_col}explorations_table = self._hdf5file.create_table(where=self._overview_group,   name='explorations',   description=description)rows = [(x.encode('utf-8'),) for x in explored_list]if rows:"
    },
    {
        "input_method": "def convert_attribute_tag_to_name(value):<TAB><TAB>\"\"\"<TAB><TAB>A utility function that converts an attribute tag into the corresponding<TAB><TAB>attribute name string.<TAB><TAB><TAB>For example: enums.Tags.STATE -> 'State'<TAB><TAB><TAB>Args:<TAB><TAB><TAB>value (enum): The Tags enumeration value of the attribute.<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>string: The attribute name string that corresponds to the attribute<TAB><TAB><TAB><TAB>tag.<TAB><TAB><TAB>Raises:<TAB><TAB><TAB>ValueError: <extra_id_0> {}\".format(value))",
        "target_block": "if the attribute tag is not a Tags enumeration or if itis unrecognized attribute tag\"\"\"if not isinstance(value, Tags):raise ValueError(\"The attribute tag must be a Tags enumeration.\")for entry in attribute_name_tag_table:if value == entry[1]:return entry[0]raise ValueError(\"Unrecognized attribute tag:"
    },
    {
        "input_method": "def math_to_image(s, filename_or_obj, prop=None, dpi=None, format=None):<TAB><TAB>\"\"\"<TAB><TAB>Given a math expression, renders it in a closely-clipped bounding<TAB><TAB>box to an image file.<TAB><TAB><TAB>*s*<TAB><TAB>   A math expression.  The math portion should be enclosed in<TAB><TAB>   dollar signs.<TAB><TAB><TAB>*filename_or_obj*<TAB><TAB>   A filepath or writable file-like object to write the image data<TAB><TAB>   to.<TAB><TAB><TAB>*prop*<TAB><TAB>   If provided, a FontProperties() object describing the size and<TAB><TAB>   style of the text.<TAB><TAB><TAB>*dpi*<TAB><TAB>   Override the output dpi, otherwise use the default associated<TAB><TAB>   with the output format.<TAB><TAB><TAB>*format*<TAB><TAB>   The output format, eg. 'svg', 'pdf', 'ps' or 'png'.  If not<TAB><TAB>   provided, will be deduced from the filename.<TAB><TAB>\"\"\"<TAB><TAB>from matplotlib import figure<TAB><TAB># backend_agg supports all of the core output formats<TAB><TAB>from matplotlib.backends import backend_agg<TAB><TAB>from matplotlib.font_manager import FontProperties<TAB><TAB>from matplotlib.mathtext import MathTextParser<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>prop = FontProperties()<TAB><TAB><TAB>parser = MathTextParser('path')<TAB><TAB>width, height, depth, _, _ = parser.parse(s, dpi=72, prop=prop)<TAB><TAB><TAB>fig = figure.Figure(figsize=(width / 72.0, height / 72.0))<TAB><TAB>fig.text(0, depth/height, s, fontproperties=prop)<TAB><TAB>backend_agg.FigureCanvasAgg(fig)<TAB><TAB>fig.savefig(filename_or_obj, dpi=dpi, format=format)<TAB><TAB><TAB>return depth",
        "target_block": "if prop is None:"
    },
    {
        "input_method": "def save_task_result(self, idents, msg):<TAB><TAB><TAB>\"\"\"save the result of a completed task.\"\"\"<TAB><TAB><TAB>client_id = idents[0]<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>msg = self.session.unserialize(msg)<TAB><TAB><TAB>except Exception:<TAB><TAB><TAB><TAB>self.log.error(\"task::invalid task result message send to %r: %r\",<TAB><TAB><TAB><TAB><TAB><TAB>client_id, msg, exc_info=True)<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB><TAB>parent = msg['parent_header']<TAB><TAB><TAB><extra_id_0>unknown task %r finished\", msg_id)",
        "target_block": "if not parent:# print msgself.log.warn(\"Task %r had no parent!\", msg)returnmsg_id = parent['msg_id']if msg_id in self.unassigned:self.unassigned.remove(msg_id)header = msg['header']engine_uuid = header.get('engine', u'')eid = self.by_ident.get(cast_bytes(engine_uuid), None)status = header.get('status', None)if msg_id in self.pending:self.log.info(\"task::task %r finished on %s\", msg_id, eid)self.pending.remove(msg_id)self.all_completed.add(msg_id)if eid is not None:if status != 'aborted':self.completed[eid].append(msg_id)if msg_id in self.tasks[eid]:self.tasks[eid].remove(msg_id)completed = header['date']started = header.get('started', None)result = {'result_header' : header,'result_content': msg['content'],'started' : started,'completed' : completed,'received' : datetime.now(),'engine_uuid': engine_uuid,}result['result_buffers'] = msg['buffers']try:self.db.update_record(msg_id, result)except Exception:self.log.error(\"DB Error saving task request %r\", msg_id, exc_info=True)else:self.log.debug(\"task::"
    },
    {
        "input_method": "def visit_if(self, node):<TAB><TAB><TAB>\"\"\"increments the branches counter and checks boolean expressions\"\"\"<TAB><TAB><TAB>self._check_boolean_expressions(node)<TAB><TAB><TAB>branches = 1<TAB><TAB><TAB># don't double count If nodes coming from some 'elif'<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>branches += 1<TAB><TAB><TAB>self._inc_branch(node, branches)<TAB><TAB><TAB>self._inc_all_stmts(branches)",
        "target_block": "if node.orelse and (len(node.orelse) > 1 or not isinstance(node.orelse[0], If)):"
    },
    {
        "input_method": "def get_token(self,<TAB><TAB><TAB><TAB><TAB>  grant_type,<TAB><TAB><TAB><TAB><TAB>  client_id,<TAB><TAB><TAB><TAB><TAB>  client_secret,<TAB><TAB><TAB><TAB><TAB>  redirect_uri,<TAB><TAB><TAB><TAB><TAB>  code,<TAB><TAB><TAB><TAB><TAB>  **params):<TAB><TAB><TAB>\"\"\"Generate access token HTTP response.<TAB><TAB><TAB><TAB>:param grant_type: Desired grant type. Must be \"authorization_code\".<TAB><TAB><TAB>:type grant_type: str<TAB><TAB><TAB>:param client_id: Client ID.<TAB><TAB><TAB>:type client_id: str<TAB><TAB><TAB>:param client_secret: Client secret.<TAB><TAB><TAB>:type client_secret: str<TAB><TAB><TAB>:param redirect_uri: Client redirect URI.<TAB><TAB><TAB>:type redirect_uri: str<TAB><TAB><TAB>:param code: Authorization code.<TAB><TAB><TAB>:type code: str<TAB><TAB><TAB>:rtype: requests.Response<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB># Ensure proper grant_type<TAB><TAB><TAB><extra_id_0> refresh_token<TAB><TAB><TAB>})",
        "target_block": "if grant_type != 'authorization_code':return self._make_json_error_response('unsupported_grant_type')# Check conditionsis_valid_client_id = self.validate_client_id(client_id)is_valid_client_secret = self.validate_client_secret(client_id, client_secret)is_valid_redirect_uri = self.validate_redirect_uri(client_id,   redirect_uri)scope = params.get('scope', '')is_valid_scope = self.validate_scope(client_id, scope)data = self.from_authorization_code(client_id, code, scope)is_valid_grant = data is not None# Return proper error responses on invalid conditionsif not (is_valid_client_id and is_valid_client_secret):return self._make_json_error_response('invalid_client')if not is_valid_grant or not is_valid_redirect_uri:return self._make_json_error_response('invalid_grant')if not is_valid_scope:return self._make_json_error_response('invalid_scope')# Discard original authorization codeself.discard_authorization_code(client_id, code)# Generate access tokens once all conditions have been metaccess_token = self.generate_access_token()token_type = self.token_typeexpires_in = self.token_expires_inrefresh_token = self.generate_refresh_token()# Save information to be used to validate later requestsself.persist_token_information(client_id=client_id,   scope=scope,   access_token=access_token,   token_type=token_type,   expires_in=expires_in,   refresh_token=refresh_token,   data=data)# Return json responsereturn self._make_json_response({'access_token': access_token,'token_type': token_type,'expires_in': expires_in,'refresh_token':"
    },
    {
        "input_method": "def parse(self):<TAB><TAB><TAB>\"\"\"Parse the Git log stream.\"\"\"<TAB><TAB><TAB><TAB>for line in self.stream:<TAB><TAB><TAB><TAB>line = line.rstrip('\\n')<TAB><TAB><TAB><TAB>parsed = False<TAB><TAB><TAB><TAB>self.nline += 1<TAB><TAB><TAB><TAB><TAB>while not parsed:<TAB><TAB><TAB><TAB><TAB>parsed = self.handlers[self.state](line)<TAB><TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>commit = self._build_commit()<TAB><TAB><TAB><TAB>logger.debug(\"Commit %s parsed\", commit['commit'])<TAB><TAB><TAB><TAB>yield commit",
        "target_block": "if self.state == self.COMMIT and self.commit:commit = self._build_commit()logger.debug(\"Commit %s parsed\", commit['commit'])yield commit# Return the last commit, if anyif self.commit:"
    },
    {
        "input_method": "def refreshSkypeToken(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Take the existing Skype token and refresh it, to extend the expiry time without other credentials.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>.SkypeAuthException: <extra_id_0> if the login form can't be processed<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self.tokens[\"skype\"], self.tokenExpiry[\"skype\"] = SkypeRefreshAuthProvider(self).auth(self.tokens[\"skype\"])<TAB><TAB><TAB>self.getRegToken()",
        "target_block": "if the login request is rejected.SkypeApiException:"
    },
    {
        "input_method": "def weighted(loads):<TAB><TAB>\"\"\"Pick two at random using inverse load as weight.<TAB><TAB><TAB>Return the less loaded of the two.<TAB><TAB>\"\"\"<TAB><TAB># weight 0 a million times more than 1:<TAB><TAB>weights = 1./(1e-6+numpy.array(loads))<TAB><TAB>sums = weights.cumsum()<TAB><TAB>t = sums[-1]<TAB><TAB>x = random()*t<TAB><TAB>y = random()*t<TAB><TAB>idx = 0<TAB><TAB>idy = 0<TAB><TAB>while sums[idx] < x:<TAB><TAB><TAB>idx += 1<TAB><TAB>while sums[idy] < y:<TAB><TAB><TAB>idy += 1<TAB><TAB><extra_id_0><TAB><TAB><TAB>return idx",
        "target_block": "if weights[idy] > weights[idx]:return idyelse:"
    },
    {
        "input_method": "def read(self, input_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Read the data encoding the Nonce struct and decode it into its<TAB><TAB><TAB>constituent parts.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>input_stream (stream): A data stream containing encoded object<TAB><TAB><TAB><TAB><TAB>data, supporting a read method; usually a BytearrayStream<TAB><TAB><TAB><TAB><TAB>object.<TAB><TAB><TAB><TAB>kmip_version (KMIPVersion): An enumeration defining the KMIP<TAB><TAB><TAB><TAB><TAB>version with which the object will be decoded. Optional,<TAB><TAB><TAB><TAB><TAB>defaults to KMIP 1.0.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>ValueError: Raised <extra_id_0><TAB><TAB><TAB><TAB>raise ValueError(<TAB><TAB><TAB><TAB><TAB>\"Nonce encoding missing the nonce value.\"<TAB><TAB><TAB><TAB>)<TAB><TAB><TAB><TAB>self.is_oversized(local_stream)",
        "target_block": "if the nonce ID or nonce value is missing fromthe encoding.\"\"\"super(Nonce, self).read(input_stream, kmip_version=kmip_version)local_stream = BytearrayStream(input_stream.read(self.length))if self.is_tag_next(enums.Tags.NONCE_ID, local_stream):self._nonce_id = primitives.ByteString(tag=enums.Tags.NONCE_ID)self._nonce_id.read(local_stream, kmip_version=kmip_version)else:raise ValueError(\"Nonce encoding missing the nonce ID.\")if self.is_tag_next(enums.Tags.NONCE_VALUE, local_stream):self._nonce_value = primitives.ByteString(tag=enums.Tags.NONCE_VALUE)self._nonce_value.read(local_stream, kmip_version=kmip_version)else:"
    },
    {
        "input_method": "def get_metric(<TAB><TAB><TAB><TAB>self, name: str,<TAB><TAB><TAB><TAB>labels: Union[Dict[str, str], None] = None) -> Metric:<TAB><TAB><TAB>\"\"\"Return a metric, optionally configured with labels.\"\"\"<TAB><TAB><TAB>metric = self._metrics[name]<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return metric.labels(**labels)<TAB><TAB><TAB><TAB>return metric",
        "target_block": "if labels:"
    },
    {
        "input_method": "def with_stdout(self, os_path=None, skip_sub_command=False,<TAB><TAB><TAB><TAB><TAB><TAB>disk_closed_callback=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>A context manager yielding a stdout-suitable file-like object<TAB><TAB><TAB>based on the optional os_path and optionally skipping any<TAB><TAB><TAB>configured sub-command.<TAB><TAB><TAB><TAB>:param os_path: Optional path to base the file-like object<TAB><TAB><TAB><TAB>on.<TAB><TAB><TAB>:param skip_sub_command: Set True to skip any configured<TAB><TAB><TAB><TAB>sub-command filter.<TAB><TAB><TAB>:param disk_closed_callback: If the backing of the file-like<TAB><TAB><TAB><TAB>object is an actual file that will be closed,<TAB><TAB><TAB><TAB>disk_closed_callback (<extra_id_0><TAB><TAB><TAB><TAB><TAB>disk_closed_callback(path)",
        "target_block": "if set) will be called with theon-disk path just after closing it.\"\"\"sub_command = None if skip_sub_command else self.stdout_sub_commandout, path = self._get_out_and_path(self.stdout, self.stdout_root, sub_command, os_path)try:if hasattr(out, 'stdin'):yield out.stdinelse:yield outfinally:if hasattr(out, 'stdin'):self._close(out.stdin)self._wait(out, path)self._close(out)if disk_closed_callback and path:"
    },
    {
        "input_method": "def get_level_str(self):<TAB><TAB><TAB>''' format level str '''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>level_str = self.level<TAB><TAB><TAB>return level_str",
        "target_block": "if self.is_relative:level_str = str(self.level) + \"%\"else:"
    },
    {
        "input_method": "def cat(self, i1, i2, format='html'):<TAB><TAB><TAB>\"\"\"Display the DataFrame from row i1 till i2<TAB><TAB><TAB><TAB>For format, see https://pypi.org/project/tabulate/<TAB><TAB><TAB><TAB>:param int i1: Start row<TAB><TAB><TAB>:param int i2: End row.<TAB><TAB><TAB>:param str format: Format to use, e.g. 'html', 'plain', 'latex'<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>from IPython import display<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>output = self._as_table(i1, i2, format=format)<TAB><TAB><TAB><TAB>print(output)",
        "target_block": "if format == 'html':output = self._as_html_table(i1, i2)display.display(display.HTML(output))else:"
    },
    {
        "input_method": "def extract_header(msg_or_header):<TAB><TAB>\"\"\"Given a message or header, return the header.\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>h = dict(h)<TAB><TAB>return h",
        "target_block": "if not msg_or_header:return {}try:# See if msg_or_header is the entire message.h = msg_or_header['header']except KeyError:try:# See if msg_or_header is just the headerh = msg_or_header['msg_id']except KeyError:raiseelse:h = msg_or_headerif not isinstance(h, dict):"
    },
    {
        "input_method": "def recv_method(self, method, params, id_, randomSeed=None):<TAB><TAB><TAB>\"\"\"DDP method handler.\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>this.random_streams.random_seed = randomSeed<TAB><TAB><TAB><TAB>this.alea_random = alea.Alea(randomSeed)<TAB><TAB><TAB>self.api.method(method, params, id_)<TAB><TAB><TAB>self.reply('updated', methods=[id_])",
        "target_block": "if randomSeed is not None:"
    },
    {
        "input_method": "def _prepare_args_with_initial_vertex(objective_function,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  initial_vertex,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  step_sizes,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  objective_at_initial_vertex,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  batch_evaluate_objective):<TAB>  \"\"\"Constructs a standard axes aligned simplex.\"\"\"<TAB>  dim = tf.size(input=initial_vertex)<TAB>  num_vertices = dim + 1<TAB>  unit_vectors_along_axes = tf.reshape(<TAB><TAB>  tf.eye(dim, dim, dtype=initial_vertex.dtype.base_dtype),<TAB><TAB>  tf.concat([[dim], tf.shape(input=initial_vertex)], axis=0))<TAB><TAB>  # If step_sizes does not broadcast to initial_vertex, the multiplication<TAB>  # in the second term will fail.<TAB>  simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes<TAB>  simplex = tf.concat([tf.expand_dims(initial_vertex, axis=0),<TAB><TAB><TAB><TAB><TAB><TAB>   simplex_face], axis=0)<TAB>  num_evaluations = 0<TAB>  # Evaluate the objective function at the simplex vertices.<TAB>  <extra_id_0><TAB><TAB>objective_at_initial_vertex = objective_function(initial_vertex)<TAB><TAB>num_evaluations += 1<TAB><TAB>  objective_at_simplex_face, num_evals = _evaluate_objective_multiple(<TAB><TAB>  objective_function, simplex_face, batch_evaluate_objective)<TAB>  num_evaluations += num_evals<TAB><TAB>  objective_at_simplex = tf.concat(<TAB><TAB>  [<TAB><TAB><TAB>  tf.expand_dims(objective_at_initial_vertex, axis=0),<TAB><TAB><TAB>  objective_at_simplex_face<TAB><TAB>  ], axis=0)<TAB><TAB>  return (dim,<TAB><TAB><TAB>  num_vertices,<TAB><TAB><TAB>  simplex,<TAB><TAB><TAB>  objective_at_simplex,<TAB><TAB><TAB>  num_evaluations)",
        "target_block": "if objective_at_initial_vertex is None:"
    },
    {
        "input_method": "def _x_credentials_parser(credentials, data):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>We need to override this method to fix Facebooks naming deviation.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB># Facebook returns \"expires\" instead of \"expires_in\".<TAB><TAB><TAB>credentials.expire_in = data.get('expires')<TAB><TAB><TAB><TAB><extra_id_0> cls is not available here, hardcode for now.<TAB><TAB><TAB><TAB>credentials.token_type = 'Bearer'<TAB><TAB><TAB><TAB>return credentials",
        "target_block": "if data.get('token_type') == 'bearer':# TODO:"
    },
    {
        "input_method": "def purge_results(self, jobs=[], targets=[]):<TAB><TAB><TAB>\"\"\"Tell the Hub to forget results.<TAB><TAB><TAB><TAB>Individual results can be purged by msg_id, or the entire<TAB><TAB><TAB>history of specific targets can be purged.<TAB><TAB><TAB><TAB>Use `purge_results('all')` to scrub everything from the Hub's db.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB><TAB>jobs : str or list of str or AsyncResult objects<TAB><TAB><TAB><TAB><TAB>the msg_ids whose results should be forgotten.<TAB><TAB><TAB>targets : int/str/list of ints/strs<TAB><TAB><TAB><TAB><TAB>The targets, by int_id, whose entire history is to be purged.<TAB><TAB><TAB><TAB><TAB><TAB>default : None<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise self._unwrap_exception(content)",
        "target_block": "if not targets and not jobs:raise ValueError(\"Must specify at least one of `targets` and `jobs`\")if targets:targets = self._build_targets(targets)[1]# construct msg_ids from jobsif jobs == 'all':msg_ids = jobselse:msg_ids = []if isinstance(jobs, (basestring,AsyncResult)):jobs = [jobs]bad_ids = filter(lambda obj: not isinstance(obj, (basestring, AsyncResult)), jobs)if bad_ids:raise TypeError(\"Invalid msg_id type %r, expected str or AsyncResult\"%bad_ids[0])for j in jobs:if isinstance(j, AsyncResult):msg_ids.extend(j.msg_ids)else:msg_ids.append(j)content = dict(engine_ids=targets, msg_ids=msg_ids)self.session.send(self._query_socket, \"purge_request\", content=content)idents, msg = self.session.recv(self._query_socket, 0)if self.debug:pprint(msg)content = msg['content']if content['status'] != 'ok':"
    },
    {
        "input_method": "def build_attrs(self, base_attrs, extra_attrs=None):<TAB><TAB><TAB>\"\"\"Build an attribute dictionary.\"\"\"<TAB><TAB><TAB>attrs = base_attrs.copy()<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>attrs.update(extra_attrs)<TAB><TAB><TAB>return attrs",
        "target_block": "if extra_attrs is not None:"
    },
    {
        "input_method": "def read(self, istream, kmip_version=enums.KMIPVersion.KMIP_1_0):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Read the encoding of the BigInteger from the input stream.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>istream (stream): A buffer containing the encoded bytes of the<TAB><TAB><TAB><TAB><TAB>value of a BigInteger. Usually a BytearrayStream object.<TAB><TAB><TAB><TAB><TAB>Required.<TAB><TAB><TAB><TAB>kmip_version (KMIPVersion): An enumeration defining the KMIP<TAB><TAB><TAB><TAB><TAB>version with which the object will be decoded. Optional,<TAB><TAB><TAB><TAB><TAB>defaults to KMIP 1.0.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>InvalidPrimitiveLength: <extra_id_0>]))<TAB><TAB><TAB><TAB># Convert the value back to an integer and reapply the sign.<TAB><TAB><TAB>self.value = int(binary, 2) * sign",
        "target_block": "if the big integer encoding read in hasan invalid encoded length.\"\"\"super(BigInteger, self).read(istream, kmip_version=kmip_version)# Check for a valid length before even trying to parse the value.if self.length % 8:raise exceptions.InvalidPrimitiveLength(\"invalid big integer length read; \"\"expected: multiple of 8, observed: {0}\".format(self.length))sign = 1binary = ''# Read the value byte by byte and convert it into binary, padding each# byte as needed.for _ in range(self.length):byte = struct.unpack('!B', istream.read(1))[0]bits = \"{0:b}\".format(byte)pad = len(bits) % 8if pad:bits = ('0' * (8 - pad)) + bitsbinary += bits# If the value is negative, convert via two's complement.if binary[0] == '1':sign = -1binary = binary.replace('1', 'i')binary = binary.replace('0', '1')binary = binary.replace('i', '0')pivot = binary.rfind('0')binary = binary[0:pivot] + '1' + ('0' * len(binary[pivot + 1:"
    },
    {
        "input_method": "def concatenate(arrays, axis=0):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Join a sequence of arrays together.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>arrays : tuple<TAB><TAB><TAB><TAB>A sequence of array-like e.g. (a1, a2, ...)<TAB><TAB><TAB><TAB>axis : int, optional, default=0<TAB><TAB><TAB><TAB>The axis along which the arrays will be joined.<TAB><TAB><TAB><TAB>Returns<TAB><TAB><TAB>-------<TAB><TAB><TAB>BoltArrayLocal<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise ValueError(\"data type not understood\")<TAB><TAB><TAB>arrays = tuple([asarray(a) for a in arrays])<TAB><TAB><TAB>from numpy import concatenate<TAB><TAB><TAB>return BoltArrayLocal(concatenate(arrays, axis))",
        "target_block": "if not isinstance(arrays, tuple):"
    },
    {
        "input_method": "def get_cumulative_data(self):<TAB>\t\t\"\"\"Get the data as it will be charted.  The first set will be<TAB>\t\tthe actual first data set.  The second will be the sum of the<TAB>\t\tfirst and the second, etc.\"\"\"<TAB>\t\tsets = map(itemgetter('data'), self.data)<TAB>\t\t<extra_id_0><TAB>\t\t\tsum = map(add, sets.pop(0))<TAB>\t\t\tyield sum",
        "target_block": "if not sets:\t\t\treturn\t\tsum = sets.pop(0)\t\tyield sum\t\twhile sets:"
    },
    {
        "input_method": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Write the data encoding the Check response payload to a stream.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>output_stream (stream): A data stream in which to encode object<TAB><TAB><TAB><TAB><TAB>data, supporting a write method; usually a BytearrayStream<TAB><TAB><TAB><TAB><TAB>object.<TAB><TAB><TAB><TAB>kmip_version (KMIPVersion): An enumeration defining the KMIP<TAB><TAB><TAB><TAB><TAB>version with which the object will be encoded. Optional,<TAB><TAB><TAB><TAB><TAB>defaults to KMIP 1.0.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>ValueError: Raised <extra_id_0><TAB><TAB><TAB><TAB>self._lease_time.write(<TAB><TAB><TAB><TAB><TAB>local_stream,<TAB><TAB><TAB><TAB><TAB>kmip_version=kmip_version<TAB><TAB><TAB><TAB>)<TAB><TAB><TAB><TAB>self.length = local_stream.length()<TAB><TAB><TAB>super(CheckResponsePayload, self).write(<TAB><TAB><TAB><TAB>output_stream,<TAB><TAB><TAB><TAB>kmip_version=kmip_version<TAB><TAB><TAB>)<TAB><TAB><TAB>output_stream.write(local_stream.buffer)",
        "target_block": "if the data attribute is not defined.\"\"\"local_stream = utils.BytearrayStream()if self._unique_identifier:self._unique_identifier.write(local_stream,kmip_version=kmip_version)if self._usage_limits_count:self._usage_limits_count.write(local_stream,kmip_version=kmip_version)if self._cryptographic_usage_mask:self._cryptographic_usage_mask.write(local_stream,kmip_version=kmip_version)if self._lease_time:"
    },
    {
        "input_method": "def load_config():<TAB><TAB>\"\"\"Load config.\"\"\"<TAB><TAB>mode = os.environ.get('MODE')<TAB><TAB>try:<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>from .default import Config<TAB><TAB><TAB>return Config",
        "target_block": "if mode == 'PRODUCTION':from .production import ProductionConfigreturn ProductionConfigelif mode == 'TESTING':from .testing import TestingConfigreturn TestingConfigelse:from .development import DevelopmentConfigreturn DevelopmentConfigexcept ImportError:"
    },
    {
        "input_method": "def _match_long_opt(self, opt):<TAB><TAB><TAB>\"\"\"Disable abbreviations.\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise optparse.BadOptionError(opt)<TAB><TAB><TAB>return opt",
        "target_block": "if opt not in self._long_opt:"
    },
    {
        "input_method": "def is_archive_file(name):<TAB><TAB>\"\"\"Return True <extra_id_0><TAB><TAB><TAB>return True<TAB><TAB>return False",
        "target_block": "if `name` is a considered as an archive file.\"\"\"archives = ('.zip', '.tar.gz', '.tar.bz2', '.tgz', '.tar', '.whl')ext = splitext(name)[1].lower()if ext in archives:"
    },
    {
        "input_method": "def os_open(document):<TAB><TAB>\"\"\"Open document by the default handler of the OS, could be a url opened by a browser, a text file by an editor etc\"\"\"<TAB><TAB>osname = platform.system().lower()<TAB><TAB><extra_id_0><TAB><TAB><TAB>os.system(\"start \\\"\" + document + \"\\\"\")",
        "target_block": "if osname == \"darwin\":os.system(\"open \\\"\" + document + \"\\\"\")if osname == \"linux\":cmd = \"xdg-open \\\"\" + document + \"\\\"&\"os.system(cmd)if osname == \"windows\":"
    },
    {
        "input_method": "def end_tag(el):<TAB><TAB>\"\"\" The text representation of an end tag for a tag.  Includes<TAB><TAB>trailing whitespace when appropriate.  \"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>extra = ''<TAB><TAB>return '</%s>%s' % (el.tag, extra)",
        "target_block": "if el.tail and start_whitespace_re.search(el.tail):extra = ' 'else:"
    },
    {
        "input_method": "def delete_site(self, webspace_name, website_name,<TAB><TAB><TAB><TAB><TAB><TAB>delete_empty_server_farm=False, delete_metrics=False):<TAB><TAB><TAB>'''<TAB><TAB><TAB>Delete a website.<TAB><TAB><TAB><TAB>webspace_name:<TAB><TAB><TAB><TAB>The name of the webspace.<TAB><TAB><TAB>website_name:<TAB><TAB><TAB><TAB>The name of the website.<TAB><TAB><TAB>delete_empty_server_farm:<TAB><TAB><TAB><TAB>If the site being deleted is the last web site in a server farm,<TAB><TAB><TAB><TAB>you can delete the server farm by setting this to True.<TAB><TAB><TAB>delete_metrics:<TAB><TAB><TAB><TAB>To also delete the metrics for the site that you are deleting, you<TAB><TAB><TAB><TAB>can set this to True.<TAB><TAB><TAB>'''<TAB><TAB><TAB>path = self._get_sites_details_path(webspace_name, website_name)<TAB><TAB><TAB>query = ''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>path = path + '?' + query.lstrip('&')<TAB><TAB><TAB>return self._perform_delete(path)",
        "target_block": "if delete_empty_server_farm:query += '&deleteEmptyServerFarm=true'if delete_metrics:query += '&deleteMetrics=true'if query:"
    },
    {
        "input_method": "def _satisfies_wolfe(val_0,<TAB><TAB><TAB><TAB><TAB><TAB> val_c,<TAB><TAB><TAB><TAB><TAB><TAB> f_lim,<TAB><TAB><TAB><TAB><TAB><TAB> sufficient_decrease_param,<TAB><TAB><TAB><TAB><TAB><TAB> curvature_param):<TAB>  \"\"\"Checks whether the Wolfe or approx Wolfe conditions are satisfied.<TAB><TAB>  The Wolfe conditions are a set of stopping criteria for an inexact line search<TAB>  algorithm. Let f(a) be the function value along the search direction and<TAB>  df(a) the derivative along the search direction evaluated a distance 'a'.<TAB>  Here 'a' is the distance along the search direction. The Wolfe conditions are:<TAB><TAB><TAB>```None<TAB><TAB>  f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)<TAB><TAB>  df(a) >= sigma * df(0)<TAB><TAB><TAB> (Weak curvature condition)<TAB><TAB>```<TAB>  `delta` and `sigma` are two user supplied parameters satisfying:<TAB>   `0 < delta < sigma <= 1.`. In the following, delta is called<TAB>   `sufficient_decrease_param` and sigma is called `curvature_param`.<TAB><TAB>  On a finite precision machine, the Wolfe conditions are difficult to satisfy<TAB>  when one is close to the minimum. Hence, Hager-Zhang propose replacing<TAB>  the sufficient decrease condition with the following condition on the<TAB>  derivative in the vicinity of a minimum.<TAB><TAB><TAB>```None<TAB><TAB>  df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)<TAB><TAB>```<TAB>  This condition is only used <extra_id_0> A scalar boolean `Tensor` which is True if either the<TAB><TAB>  Wolfe or approximate Wolfe conditions are satisfied.<TAB>  \"\"\"<TAB>  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  (val_c.f - val_0.f) / val_c.x)<TAB>  wolfe_curvature = val_c.df >= curvature_param * val_0.df<TAB>  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature<TAB>  approx_wolfe_applies = val_c.f <= f_lim<TAB>  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   >= val_c.df)<TAB>  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature<TAB>  is_satisfied = exact_wolfe | approx_wolfe<TAB>  return is_satisfied",
        "target_block": "if one is near the minimum. This is tested using```None  f(a) <= f(0) + epsilon * |f(0)|```  The following function checks both the Wolfe and approx Wolfe conditions.  Here, `epsilon` is a small positive constant. In the following, the argument  `f_lim` corresponds to the product: epsilon * |f(0)|.  Args:val_0: A namedtuple, as returned by value_and_gradients_function  evaluated at 0.val_c: A namedtuple, as returned by value_and_gradients_function  evaluated at the point to be tested.f_lim: Scalar `Tensor` of real dtype. The function value threshold for  the approximate Wolfe conditions to be checked.sufficient_decrease_param: Positive scalar `Tensor` of real dtype.  Bounded above by the curvature param. Corresponds to 'delta' in the  terminology of [Hager and Zhang (2006)][2].curvature_param: Positive scalar `Tensor` of real dtype. Bounded above  by `1.`. Corresponds to 'sigma' in the terminology of  [Hager Zhang (2005)][1].  Returns:is_satisfied:"
    },
    {
        "input_method": "def generate_hpo_gene_list(self, *hpo_terms):<TAB><TAB><TAB>\"\"\"Generate a sorted list with namedtuples of hpogenes<TAB><TAB><TAB><TAB><TAB>Each namedtuple of the list looks like (hgnc_id, count)<TAB><TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB><TAB>hpo_terms(iterable(str))<TAB><TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB><TAB>hpo_genes(list(HpoGene))<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>genes = {}<TAB><TAB><TAB>for term in hpo_terms:<TAB><TAB><TAB><TAB>hpo_obj = self.hpo_term(term)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>LOG.warning(\"Term %s could not be found\", term)<TAB><TAB><TAB><TAB>sorted_genes = sorted(genes.items(), key=operator.itemgetter(1), reverse=True)<TAB><TAB><TAB>return sorted_genes",
        "target_block": "if hpo_obj:for hgnc_id in hpo_obj['genes']:if hgnc_id in genes:genes[hgnc_id] += 1else:genes[hgnc_id] = 1else:"
    },
    {
        "input_method": "def run(self, args=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Runs the command passing in the parsed arguments.<TAB><TAB><TAB><TAB>:param args: The arguments to run the command with. If ``None`` the arguments<TAB><TAB><TAB><TAB>are gathered from the argument parser. This is automatically set when calling<TAB><TAB><TAB><TAB>sub commands and in most cases should not be set for the root command.<TAB><TAB><TAB><TAB>:return: The status code of the action (0 on success)<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>args = args or self.parse_args()<TAB><TAB><TAB><TAB>sub_command_name = getattr(args, self.sub_parser_dest_name, None)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>sub_commands = self.get_sub_commands()<TAB><TAB><TAB><TAB>cmd_cls = sub_commands[sub_command_name]<TAB><TAB><TAB><TAB>return cmd_cls(sub_command_name).run(args)<TAB><TAB><TAB><TAB>return self.action(args) or 0",
        "target_block": "if sub_command_name:"
    },
    {
        "input_method": "def protein(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps):<TAB><TAB>\"\"\"<TAB><TAB>Infer proteins and conduct error-rate estimation in different contexts.<TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>outfile = outfile<TAB><TAB><TAB>infer_proteins(infile, outfile, context, parametric, pfdr, pi0_lambda, pi0_method, pi0_smooth_df, pi0_smooth_log_pi0, lfdr_truncate, lfdr_monotone, lfdr_transformation, lfdr_adj, lfdr_eps)",
        "target_block": "if outfile is None:outfile = infileelse:"
    },
    {
        "input_method": "def _stdin_raw_nonblock(self):<TAB><TAB><TAB>\"\"\"Use the raw Win32 handle of sys.stdin to do non-blocking reads\"\"\"<TAB><TAB><TAB># WARNING: This is experimental, and produces inconsistent results.<TAB><TAB><TAB>#<TAB><TAB>  It's possible for the handle not to be appropriate for use<TAB><TAB><TAB>#<TAB><TAB>  with WaitForSingleObject, among other things.<TAB><TAB><TAB>handle = msvcrt.get_osfhandle(sys.stdin.fileno())<TAB><TAB><TAB>result = WaitForSingleObject(handle, 100)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>raise ctypes.WinError()<TAB><TAB><TAB><TAB># This ensures the non-blocking works with an actual console<TAB><TAB><TAB><TAB># Not checking the error, so the processing will still work with<TAB><TAB><TAB><TAB># other handle types<TAB><TAB><TAB><TAB>FlushConsoleInputBuffer(handle)<TAB><TAB><TAB><TAB><TAB>data = data.value<TAB><TAB><TAB><TAB>data = data.replace('\\r\\n', '\\n')<TAB><TAB><TAB><TAB>data = data.replace('\\r', '\\n')<TAB><TAB><TAB><TAB>print(repr(data) + \" \", end='')<TAB><TAB><TAB><TAB>return data",
        "target_block": "if result == WAIT_FAILED:raise ctypes.WinError()elif result == WAIT_TIMEOUT:print(\".\", end='')return Noneelse:data = ctypes.create_string_buffer(256)bytesRead = DWORD(0)print('?', end='')if not ReadFile(handle, data, 256,ctypes.byref(bytesRead), None):"
    },
    {
        "input_method": "async def close(self):<TAB><TAB><TAB>'''Close any of open connections'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>await self.polygon.close()",
        "target_block": "if self._ws is not None:await self._ws.close()if self.polygon is not None:"
    },
    {
        "input_method": "def colorize(msg, color):<TAB><TAB>\"\"\"Given a string add necessary codes to format the string.\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>return \"{}{}{}\".format(COLORS[color], msg, COLORS[\"endc\"])",
        "target_block": "if DONT_COLORIZE:return msgelse:"
    },
    {
        "input_method": "def _serializeExclude_eval(parentUnit, obj, isDeclaration, priv):<TAB><TAB>\"\"\"<TAB><TAB>Always decide not to serialize obj<TAB><TAB><TAB>:param priv: private data for this function first unit of this class<TAB><TAB>:return: tuple (do serialize this object, next priv)<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>priv = parentUnit<TAB><TAB><TAB>return False, priv",
        "target_block": "if isDeclaration:# prepare entity which will not be serializedprepareEntity(obj, parentUnit.__class__.__name__, priv)if priv is None:"
    },
    {
        "input_method": "def start(self):<TAB><TAB><TAB>\"\"\"Start the timer.\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self._t_start = time()<TAB><TAB><TAB><TAB>self._is_running = True<TAB><TAB><TAB>self._t_last = time()",
        "target_block": "if not self._is_running:"
    },
    {
        "input_method": "def _embed_no_none_gradient_check(value_and_gradients_fn):<TAB>  \"\"\"Wraps value and gradients function to assist with None gradients.\"\"\"<TAB>  @functools.wraps(value_and_gradients_fn)<TAB>  def func_wrapped(*args, **kwargs):<TAB><TAB>\"\"\"Wrapped function which checks for None gradients.\"\"\"<TAB><TAB>value, grads = value_and_gradients_fn(*args, **kwargs)<TAB><TAB><extra_id_0><TAB><TAB>  raise ValueError(\"Gradient is None for a state.\")<TAB><TAB>return value, grads<TAB>  return func_wrapped",
        "target_block": "if any(grad is None for grad in grads):"
    },
    {
        "input_method": "def publish(<TAB><TAB><TAB><TAB>self, resource_group_name, automation_account_name, runbook_name, custom_headers=None, raw=False, polling=True, **operation_config):<TAB><TAB><TAB>\"\"\"Publish runbook draft.<TAB><TAB><TAB><TAB>:param resource_group_name: Name of an Azure Resource group.<TAB><TAB><TAB>:type resource_group_name: str<TAB><TAB><TAB>:param automation_account_name: The name of the automation account.<TAB><TAB><TAB>:type automation_account_name: str<TAB><TAB><TAB>:param runbook_name: The parameters supplied to the publish runbook<TAB><TAB><TAB> operation.<TAB><TAB><TAB>:type runbook_name: str<TAB><TAB><TAB>:param dict custom_headers: headers that will be added to the request<TAB><TAB><TAB>:param bool raw: The poller return type is ClientRawResponse, the<TAB><TAB><TAB> direct response alongside the deserialized response<TAB><TAB><TAB>:param polling: True for ARMPolling, False for no polling, or a<TAB><TAB><TAB> polling object for personal polling strategy<TAB><TAB><TAB>:return: An instance of LROPoller that returns None or<TAB><TAB><TAB> ClientRawResponse<None> <extra_id_0> polling_method = polling<TAB><TAB><TAB>return LROPoller(self._client, raw_result, get_long_running_output, polling_method)",
        "target_block": "if raw==True:rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]:raises: :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`\"\"\"raw_result = self._publish_initial(resource_group_name=resource_group_name,automation_account_name=automation_account_name,runbook_name=runbook_name,custom_headers=custom_headers,raw=True,**operation_config)def get_long_running_output(response):if raw:client_raw_response = ClientRawResponse(None, response)client_raw_response.add_headers({'location': 'str',})return client_raw_responselro_delay = operation_config.get('long_running_operation_timeout',self.config.long_running_operation_timeout)if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)elif polling is False: polling_method = NoPolling()else:"
    },
    {
        "input_method": "def decode(self, images, save=None, round=4, names=None, **kwargs):<TAB><TAB><TAB>\"\"\" Decodes a set of images.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB>  images: The images to decode. Can be:<TAB><TAB><TAB><TAB>- A single String specifying the filename of the image to decode<TAB><TAB><TAB><TAB>- A list of filenames<TAB><TAB><TAB><TAB>- A single NumPy array containing the image data<TAB><TAB><TAB>  save: Optional filename to save results to. If None (default), returns<TAB><TAB><TAB><TAB>all results as an array.<TAB><TAB><TAB>  round: Optional integer indicating number of decimals to round result<TAB><TAB><TAB><TAB>to. Defaults to 4.<TAB><TAB><TAB>  names: Optional list of names corresponding to the images in filenames.<TAB><TAB><TAB><TAB>If passed, must be of same length and in same order as filenames.<TAB><TAB><TAB><TAB>By default, the columns in the output will be named using the image<TAB><TAB><TAB><TAB>filenames.<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB>  An n_features x n_files numpy array, where each feature is a row and<TAB><TAB><TAB>  each image is a column. The meaning of the values depends on the<TAB><TAB><TAB>  decoding method used. \"\"\"<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>result.to_csv(save, index_label='Feature')<TAB><TAB><TAB>return result",
        "target_block": "if isinstance(images, string_types):images = [images]if isinstance(images, list):imgs_to_decode = imageutils.load_imgs(images, self.masker)else:imgs_to_decode = imagesmethods = {'pearson': self._pearson_correlation,'dot': self._dot_product,'roi': self._roi_association}result = np.around(methods[self.method](imgs_to_decode, **kwargs), round)# if save is not None:if names is None:if type(images).__module__ == np.__name__:names = ['image_%d' % i for i in range(images.shape[1])]elif self.method == 'roi':names = ['cluster_%d' % i for i in range(result.shape[1])]else:names = imagesresult = pd.DataFrame(result, columns=names, index=self.feature_names)if save is not None:"
    },
    {
        "input_method": "def __forall_files(file_paths, output_dir, op):<TAB><TAB>\"\"\"<TAB><TAB>Applies a function to a set of files and an output directory.<TAB><TAB><TAB>:param str output_dir: Output directory<TAB><TAB>:param list[str] file_paths: Absolute file paths to move<TAB><TAB>\"\"\"<TAB><TAB>for file_path in file_paths:<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise ValueError('Path provided (%s) is relative not absolute.' % file_path)<TAB><TAB><TAB>dest = os.path.join(output_dir, os.path.basename(file_path))<TAB><TAB><TAB>op(file_path, dest)",
        "target_block": "if not file_path.startswith('/'):"
    },
    {
        "input_method": "def _update_current(self):<TAB><TAB><TAB>\"\"\" Updates the current item based on the current text.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>prefix = self._current_text_cursor().selection().toPlainText()<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.hide()",
        "target_block": "if prefix:items = self.findItems(prefix, (QtCore.Qt.MatchStartsWith |QtCore.Qt.MatchCaseSensitive))if items:self.setCurrentItem(items[0])else:self.hide()else:"
    },
    {
        "input_method": "def pivot_by_group(<TAB><TAB><TAB>df,<TAB><TAB><TAB>variable,<TAB><TAB><TAB>value,<TAB><TAB><TAB>new_columns,<TAB><TAB><TAB>groups,<TAB><TAB><TAB>id_cols=None<TAB>):<TAB><TAB>\"\"\"<TAB><TAB>Pivot a dataframe by group of variables<TAB><TAB><TAB>---<TAB><TAB><TAB>### Parameters<TAB><TAB><TAB>*mandatory :*<TAB><TAB>* `variable` (*str*): name of the column used to create the groups.<TAB><TAB>* `value` (*str*): name of the column containing the value to fill the pivoted df.<TAB><TAB>* `new_columns` (*list of str*): names of the new columns.<TAB><TAB>* `groups` (*dict*): names of the groups with their corresponding variables.<TAB><TAB>  **Warning**: the list of variables must have the same order as `new_columns`<TAB><TAB><TAB>*optional :*<TAB><TAB>* `id_cols` (*list of str*) : names of other columns to keep, default `None`.<TAB><TAB><TAB>---<TAB><TAB><TAB>### Example<TAB><TAB><TAB>**Input**<TAB><TAB><TAB>| type |  variable  | montant |<TAB><TAB>|:----:|:----------:|:-------:|<TAB><TAB>|   A  |<TAB>var1<TAB>|<TAB>5<TAB>|<TAB><TAB>|   A  | var1_evol  |   0.3   |<TAB><TAB>|   A  |<TAB>var2<TAB>|<TAB>6<TAB>|<TAB><TAB>|   A  | var2_evol  |   0.2   |<TAB><TAB><TAB>```cson<TAB><TAB>pivot_by_group :<TAB><TAB>  id_cols: ['type']<TAB><TAB>  variable: 'variable'<TAB><TAB>  value: 'montant'<TAB><TAB>  new_columns: ['value', 'variation']<TAB><TAB>  groups:<TAB><TAB><TAB>'Group 1' : ['var1', 'var1_evol']<TAB><TAB><TAB>'Group 2' : ['var2', 'var2_evol']<TAB><TAB>```<TAB><TAB><TAB>**Ouput**<TAB><TAB><TAB>| type |  variable  |  value  | variation |<TAB><TAB>|:----:|:----------:|:-------:|:---------:|<TAB><TAB>|   A  |   Group 1  |<TAB>5<TAB>|<TAB>0.3<TAB>|<TAB><TAB>|   A  |   Group 2  |<TAB>6<TAB>|<TAB>0.2<TAB>|<TAB><TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>df.loc[<TAB><TAB><TAB><TAB>df[temporary_colum].isin(param[column]), temporary_colum] = column<TAB><TAB><TAB>df = pivot(df, index, temporary_colum, value)<TAB><TAB>return df",
        "target_block": "if id_cols is None:index = [variable]else:index = [variable] + id_colsparam = pd.DataFrame(groups, index=new_columns)temporary_colum = 'tmp'df[temporary_colum] = df[variable]for column in param.columns:df.loc[df[variable].isin(param[column]), variable] = columnparam = param.Tfor column in param.columns:"
    },
    {
        "input_method": "def publishCommand(self, typeId, deviceId, commandId, msgFormat, data=None, qos=0, on_publish=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Publish a command to a device<TAB><TAB><TAB><TAB># Parameters<TAB><TAB><TAB>typeId (string) : The type of the device this command is to be published to<TAB><TAB><TAB>deviceId (string): The id of the device this command is to be published to<TAB><TAB><TAB>command (string) : The name of the command<TAB><TAB><TAB>msgFormat (string) : The format of the command payload<TAB><TAB><TAB>data (dict) : The command data<TAB><TAB><TAB>qos (int) : The equivalent MQTT semantics of quality of service using the same constants (optional, defaults to `0`)<TAB><TAB><TAB>on_publish (function) : A function that will be called when receipt of the publication is confirmed.  This has<TAB><TAB><TAB><TAB>different implications depending on the qos:<TAB><TAB><TAB><TAB>- qos 0 : the client has asynchronously begun to send the event<TAB><TAB><TAB><TAB>- qos 1 and 2 : the client has confirmation of delivery from WIoTP<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>return False",
        "target_block": "if self._config.isQuickstart():self.logger.warning(\"QuickStart applications do not support sending commands\")return Falseif not self.connectEvent.wait(timeout=10):return Falseelse:topic = \"iot-2/type/%s/id/%s/cmd/%s/fmt/%s\" % (typeId, deviceId, commandId, msgFormat)# Raise an exception if there is no codec for this msgFormatif self.getMessageCodec(msgFormat) is None:raise MissingMessageEncoderException(msgFormat)payload = self.getMessageCodec(msgFormat).encode(data, datetime.now())result = self.client.publish(topic, payload=payload, qos=qos, retain=False)if result[0] == paho.MQTT_ERR_SUCCESS:# Because we are dealing with aync pub/sub model and callbacks it is possible that# the _onPublish() callback for this mid is called before we obtain the lock to place# the mid into the _onPublishCallbacks list.## _onPublish knows how to handle a scenario where the mid is not present (no nothing)# in this scenario we will need to invoke the callback directly here, because at the time# the callback was invoked the mid was not yet in the list.with self._messagesLock:if result[1] in self._onPublishCallbacks:# paho callback beat this thread so call callback inline nowdel self._onPublishCallbacks[result[1]]if on_publish is not None:on_publish()else:# this thread beat paho callback so set up for call laterself._onPublishCallbacks[result[1]] = on_publishreturn Trueelse:"
    },
    {
        "input_method": "def random_density_matrix(length, rank=None, method='Hilbert-Schmidt', seed=None):<TAB><TAB>\"\"\"<TAB><TAB>Generate a random density matrix rho.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>length (int): the length of the density matrix.<TAB><TAB><TAB>rank (int or None): the rank of the density matrix. The default<TAB><TAB><TAB><TAB>value is full-rank.<TAB><TAB><TAB>method (string): the method to use.<TAB><TAB><TAB><TAB>'Hilbert-Schmidt': sample rho from the Hilbert-Schmidt metric.<TAB><TAB><TAB><TAB>'Bures': sample rho from the Bures metric.<TAB><TAB><TAB>seed (int): Optional. To set a random seed.<TAB><TAB>Returns:<TAB><TAB><TAB>ndarray: rho (length, length) a density matrix.<TAB><TAB>Raises:<TAB><TAB><TAB>QiskitError: <extra_id_0> unrecognized method {}'.format(method))",
        "target_block": "if the method is not valid.\"\"\"if method == 'Hilbert-Schmidt':return __random_density_hs(length, rank, seed)elif method == 'Bures':return __random_density_bures(length, rank, seed)else:raise QiskitError('Error:"
    },
    {
        "input_method": "def layers(self):<TAB><TAB><TAB>\"\"\"Yield a shallow view on a layer of this DAGCircuit for all d layers of this circuit.<TAB><TAB><TAB><TAB>A layer is a circuit whose gates act on disjoint qubits, i.e.<TAB><TAB><TAB>a layer has depth 1. The total number of layers equals the<TAB><TAB><TAB>circuit depth d. The layers are indexed from 0 to d-1 with the<TAB><TAB><TAB>earliest layer at index 0. The layers are constructed using a<TAB><TAB><TAB>greedy algorithm. Each returned layer is a dict containing<TAB><TAB><TAB>{\"graph\": circuit graph, \"partition\": list of qubit lists}.<TAB><TAB><TAB><TAB>TODO: Gates that use the same cbits will end up in different<TAB><TAB><TAB>layers as this is currently implemented. This may not be<TAB><TAB><TAB>the desired behavior.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>graph_layers = self.multigraph_layers()<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>next(graph_layers)  # Remove input nodes<TAB><TAB><TAB>except StopIteration:<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB><TAB>def add_nodes_from(layer, nodes):<TAB><TAB><TAB><TAB>\"\"\" Convert DAGNodes into a format that can be added to a<TAB><TAB><TAB><TAB> multigraph and then add to graph\"\"\"<TAB><TAB><TAB><TAB>layer._multi_graph.add_nodes_from(nodes)<TAB><TAB><TAB><TAB>for graph_layer in graph_layers:<TAB><TAB><TAB><TAB><TAB># Get the op nodes from the layer, removing any input and output nodes.<TAB><TAB><TAB><TAB>op_nodes = [node for node in graph_layer <extra_id_0> support_list}",
        "target_block": "if node.type == \"op\"]# Stop yielding once there are no more op_nodes in a layer.if not op_nodes:return# Construct a shallow copy of selfnew_layer = DAGCircuit()new_layer.name = self.namefor creg in self.cregs.values():new_layer.add_creg(creg)for qreg in self.qregs.values():new_layer.add_qreg(qreg)add_nodes_from(new_layer, self.input_map.values())add_nodes_from(new_layer, self.output_map.values())add_nodes_from(new_layer, op_nodes)# The quantum registers that have an operation in this layer.support_list = [op_node.qargsfor op_node in op_nodesif op_node.name not in {\"barrier\", \"snapshot\", \"save\", \"load\", \"noise\"}]# Now add the edges to the multi_graph# By default we just wire inputs to the outputs.wires = {self.input_map[wire]: self.output_map[wire] for wire in self.wires}# Wire inputs to op nodes, and op nodes to outputs.for op_node in op_nodes:args = self._bits_in_condition(op_node.condition) \\   + op_node.cargs + op_node.qargsarg_ids = (self.input_map[(arg[0], arg[1])] for arg in args)for arg_id in arg_ids:wires[arg_id], wires[op_node] = op_node, wires[arg_id]# Add wiring to/from the operations and between unused inputs & outputs.new_layer._multi_graph.add_edges_from(wires.items())yield {\"graph\": new_layer, \"partition\":"
    },
    {
        "input_method": "def arg_split(s, posix=False):<TAB><TAB>\"\"\"Split a command line's arguments in a shell-like manner returned<TAB><TAB>as a list of lists. Use ';;' with white space to indicate separate<TAB><TAB>commands.<TAB><TAB><TAB>This is a modified version of the standard library's shlex.split()<TAB><TAB>function, but with a default of posix=False for splitting, so that quotes<TAB><TAB>in inputs are respected.<TAB><TAB>\"\"\"<TAB><TAB><TAB>args_list = [[]]<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>args_list[-1].append(arg)<TAB><TAB><TAB><TAB>pass<TAB><TAB><TAB>pass<TAB><TAB>return args_list",
        "target_block": "if isinstance(s, bytes):s = s.decode(\"utf-8\")lex = shlex.shlex(s, posix=posix)lex.whitespace_split = Trueargs = list(lex)for arg in args:if ';;' == arg:args_list.append([])else:"
    },
    {
        "input_method": "def valid_audio(y, mono=True):<TAB><TAB>'''Validate whether a variable contains valid, mono audio data.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>y : np.ndarray<TAB><TAB>  The input data to validate<TAB><TAB><TAB>mono : bool<TAB><TAB>  Whether or not to force monophonic audio<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>valid : bool<TAB><TAB><TAB>True <extra_id_0><TAB><TAB><TAB>raise ParameterError('Audio buffer is not finite everywhere')<TAB><TAB><TAB>return True",
        "target_block": "if all tests passRaises------ParameterErrorIf `y` fails to meet the following criteria:- `type(y)` is `np.ndarray`- `y.dtype` is floating-point- `mono == True` and `y.ndim` is not 1- `mono == False` and `y.ndim` is not 1 or 2- `np.isfinite(y).all()` is not TrueNotes-----This function caches at level 20.Examples-------->>> # Only allow monophonic signals>>> y, sr = librosa.load(librosa.util.example_audio_file())>>> librosa.util.valid_audio(y)True>>> # If we want to allow stereo signals>>> y, sr = librosa.load(librosa.util.example_audio_file(), mono=False)>>> librosa.util.valid_audio(y, mono=False)True'''if not isinstance(y, np.ndarray):raise ParameterError('data must be of type numpy.ndarray')if not np.issubdtype(y.dtype, np.floating):raise ParameterError('data must be floating-point')if mono and y.ndim != 1:raise ParameterError('Invalid shape for monophonic audio: ' 'ndim={:d}, shape={}'.format(y.ndim, y.shape))elif y.ndim > 2 or y.ndim == 0:raise ParameterError('Audio must have shape (samples,) or (channels, samples). ' 'Received shape={}'.format(y.shape))if not np.isfinite(y).all():"
    },
    {
        "input_method": "def remove_subcommand(selected_vcard, force):<TAB><TAB>\"\"\"Remove a contact from the addressbook.<TAB><TAB><TAB>:param selected_vcard: the contact to delete<TAB><TAB>:type selected_vcard: carddav_object.CarddavObject<TAB><TAB>:param force: delete without confirmation<TAB><TAB>:type force: bool<TAB><TAB>:returns: None<TAB><TAB>:rtype: None<TAB><TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>break<TAB><TAB>selected_vcard.delete_vcard_file()<TAB><TAB>print(\"Contact %s deleted successfully\" % selected_vcard.get_full_name())",
        "target_block": "if not force:while True:input_string = input(\"Deleting contact %s from address book %s. Are you sure? \"\"(y/n): \" % (selected_vcard, selected_vcard.address_book))if input_string.lower() in [\"\", \"n\", \"q\"]:print(\"Canceled\")sys.exit(0)if input_string.lower() == \"y\":"
    },
    {
        "input_method": "def get(cls, name=__name__):<TAB><TAB><TAB>\"\"\"Return a Mapper instance with the given name.<TAB><TAB><TAB>   If the name already exist return its instance.<TAB><TAB><TAB><TAB>   Does not work <extra_id_0><TAB><TAB><TAB><TAB>cls.__instances[name] = cls()<TAB><TAB><TAB><TAB>cls.__instances[name]._name = name<TAB><TAB><TAB><TAB>return cls.__instances[name]",
        "target_block": "if a Mapper was created via its constructor.   Using `Mapper.get()`_ is the prefered way.Args:name (str, optional): Name for the newly created instance.Defaults to `__name__`.Returns:Mapper: A mapper instance for the given name.Raises:TypeError: If a invalid name was given.\"\"\"if not isinstance(name, str):raise TypeError('A mapper name must be a string')if name not in cls.__instances:"
    },
    {
        "input_method": "def reserve_ids(self, token, channel, quantity):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Requests a list of next-available-IDs from the server.<TAB><TAB><TAB><TAB>Arguments:<TAB><TAB><TAB><TAB>quantity (int): The number of IDs to reserve<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>int[quantity]: List of IDs you've been granted<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>quantity = str(quantity)<TAB><TAB><TAB>url = self.url(\"{}/{}/reserve/{}/\".format(token, channel, quantity))<TAB><TAB><TAB>req = self.remote_utils.get_url(url)<TAB><TAB><TAB><extra_id_0> ' + req.status_code)<TAB><TAB><TAB>out = req.json()<TAB><TAB><TAB>return [out[0] + i for i in range(out[1])]",
        "target_block": "if req.status_code is not 200:raise RemoteDataNotFoundError('Invalid req:"
    },
    {
        "input_method": "def save_unmet(self, job):<TAB><TAB><TAB>\"\"\"Save a message for later submission when its dependencies are met.\"\"\"<TAB><TAB><TAB>msg_id = job.msg_id<TAB><TAB><TAB>self.depending[msg_id] = job<TAB><TAB><TAB># track the ids in follow or after, but not those already finished<TAB><TAB><TAB>for dep_id in job.after.union(job.follow).difference(self.all_done):<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>self.graph[dep_id] = set()<TAB><TAB><TAB><TAB>self.graph[dep_id].add(msg_id)",
        "target_block": "if dep_id not in self.graph:"
    },
    {
        "input_method": "def list_overview_fmt_gen(self):<TAB><TAB><TAB>\"\"\"Generator for the LIST OVERVIEW.FMT<TAB><TAB><TAB><TAB>See list_overview_fmt() for more information.<TAB><TAB><TAB><TAB>Yields:<TAB><TAB><TAB><TAB>An element in the list returned by list_overview_fmt().<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>code, message = self.command(\"LIST OVERVIEW.FMT\")<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")<TAB><TAB><TAB><TAB>yield (name, suffix == \"full\")",
        "target_block": "if code != 215:raise NNTPReplyError(code, message)for line in self.info_gen(code, message):try:name, suffix = line.rstrip().split(\":\")except ValueError:raise NNTPDataError(\"Invalid LIST OVERVIEW.FMT\")if suffix and not name:name, suffix = suffix, nameif suffix and suffix != \"full\":"
    },
    {
        "input_method": "def asfactor(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Convert columns in the current frame to categoricals.<TAB><TAB><TAB><TAB>:returns: new H2OFrame with columns of the \"enum\" type.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>for colname in self.names:<TAB><TAB><TAB><TAB>t = self.types[colname]<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise H2OTypeError(\"Types are not available in result\")<TAB><TAB><TAB><TAB><TAB><TAB>return fr",
        "target_block": "if t not in {\"bool\", \"int\", \"string\", \"enum\"}:raise H2OValueError(\"Only 'int' or 'string' are allowed for \"\"asfactor(), got %s:%s \" % (colname, t))fr = H2OFrame._expr(expr=ExprNode(\"as.factor\", self), cache=self._ex._cache)if fr._ex._cache.types_valid():fr._ex._cache.types = {name: \"enum\" for name in self.types}else:"
    },
    {
        "input_method": "def get(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Return the HTTP code status.<TAB><TAB><TAB><TAB>:return: The matched and formatted status code.<TAB><TAB><TAB>:rtype: str|int|None<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB># * The extracted http code is not in the list of valid http code.<TAB><TAB><TAB><TAB><TAB># or<TAB><TAB><TAB><TAB><TAB># * The extracted http code is equal to `None`.<TAB><TAB><TAB><TAB><TAB><TAB># We return 3 star in order to mention that we were not eable to extract<TAB><TAB><TAB><TAB><TAB># the http status code.<TAB><TAB><TAB><TAB><TAB>return \"*\" * 3<TAB><TAB><TAB><TAB><TAB># * The extracted http code is in the list of valid http code.<TAB><TAB><TAB><TAB># or<TAB><TAB><TAB><TAB># * The extracted http code is not equal to `None`.<TAB><TAB><TAB><TAB><TAB># We return the extracted http status code.<TAB><TAB><TAB><TAB>return http_code<TAB><TAB><TAB><TAB># The http status code extraction is activated.<TAB><TAB><TAB><TAB># We return None.<TAB><TAB><TAB>return None",
        "target_block": "if PyFunceble.HTTP_CODE[\"active\"]:# The http status code extraction is activated.# We get the http status code.http_code = self._access()# We initiate a variable which will save the list of allowed# http status code.list_of_valid_http_code = []for codes in [PyFunceble.HTTP_CODE[\"list\"][\"up\"],PyFunceble.HTTP_CODE[\"list\"][\"potentially_down\"],PyFunceble.HTTP_CODE[\"list\"][\"potentially_up\"],]:# We loop throught the list of http status code.# We extend the list of valid with the currently read# codes.list_of_valid_http_code.extend(codes)if http_code not in list_of_valid_http_code or http_code is None:"
    },
    {
        "input_method": "def print_workspace(self, name):<TAB><TAB><TAB>\"\"\"Print workspace status.\"\"\"<TAB><TAB><TAB>path_list = find_path(name, self.config)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.print_status(name, path)",
        "target_block": "if len(path_list) == 0:self.logger.error(\"No matches for `%s`\" % name)return Falsefor name, path in path_list.items():"
    },
    {
        "input_method": "def cancelAll(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Cancel all submitted IO blocks.<TAB><TAB><TAB><TAB>Blocks until all submitted transfers have been finalised.<TAB><TAB><TAB>Submitting more transfers or processing completion events while this<TAB><TAB><TAB>method is running produces undefined behaviour.<TAB><TAB><TAB>Returns the list of values returned by individual cancellations.<TAB><TAB><TAB>See \"cancel\" documentation.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>cancel = self.cancel<TAB><TAB><TAB>result = []<TAB><TAB><TAB>for block, _ in self._submitted.itervalues():<TAB><TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB><TAB>result.append(cancel(block))<TAB><TAB><TAB><TAB>except OSError as exc:<TAB><TAB><TAB><TAB><TAB># EINVAL should mean we requested to cancel a not-in-flight<TAB><TAB><TAB><TAB><TAB># transfer - maybe it was just completed and we just did<TAB><TAB><TAB><TAB><TAB># not process its completion event yet.<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>raise<TAB><TAB><TAB>return result",
        "target_block": "if exc.errno != errno.EINVAL:"
    },
    {
        "input_method": "def prepare_classpath():<TAB><TAB>\"\"\"<TAB><TAB>Ensures that certain subfolders of AIRFLOW_HOME are on the classpath<TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>sys.path.append(PLUGINS_FOLDER)",
        "target_block": "if DAGS_FOLDER not in sys.path:sys.path.append(DAGS_FOLDER)# Add ./config/ for loading custom log parsers etc, or# airflow_local_settings etc.config_path = os.path.join(AIRFLOW_HOME, 'config')if config_path not in sys.path:sys.path.append(config_path)if PLUGINS_FOLDER not in sys.path:"
    },
    {
        "input_method": "def verified(self, institute_id):<TAB><TAB><TAB>\"\"\"Return all verified variants for a given institute<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>institute_id(str): institute id<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>res(list): a list with validated variants<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>query = {<TAB><TAB><TAB><TAB>'verb' : 'validate',<TAB><TAB><TAB><TAB>'institute' : institute_id,<TAB><TAB><TAB>}<TAB><TAB><TAB>res = []<TAB><TAB><TAB>validate_events = self.event_collection.find(query)<TAB><TAB><TAB>for validated in list(validate_events):<TAB><TAB><TAB><TAB>case_id = validated['case']<TAB><TAB><TAB><TAB>var_obj = self.variant(case_id=case_id, document_id=validated['variant_id'])<TAB><TAB><TAB><TAB>case_obj = self.case(case_id=case_id)<TAB><TAB><TAB><TAB><extra_id_0> case_obj['individuals']<TAB><TAB><TAB><TAB>}<TAB><TAB><TAB><TAB>res.append(var_obj)<TAB><TAB><TAB><TAB>return res",
        "target_block": "if not case_obj or not var_obj:continue # Take into account that stuff might have been removed from databasevar_obj['case_obj'] = {'display_name' : case_obj['display_name'],'individuals' :"
    },
    {
        "input_method": "def create_bucket(self,<TAB><TAB><TAB><TAB><TAB><TAB>  bucket_name,<TAB><TAB><TAB><TAB><TAB><TAB>  resource=None,<TAB><TAB><TAB><TAB><TAB><TAB>  storage_class='MULTI_REGIONAL',<TAB><TAB><TAB><TAB><TAB><TAB>  location='US',<TAB><TAB><TAB><TAB><TAB><TAB>  project_id=None,<TAB><TAB><TAB><TAB><TAB><TAB>  labels=None<TAB><TAB><TAB><TAB><TAB><TAB>  ):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Creates a new bucket. Google Cloud Storage uses a flat namespace, so<TAB><TAB><TAB>you can't create a bucket with a name that is already in use.<TAB><TAB><TAB><TAB>.. seealso::<TAB><TAB><TAB><TAB>For more information, see Bucket Naming Guidelines:<TAB><TAB><TAB><TAB>https://cloud.google.com/storage/docs/bucketnaming.html#requirements<TAB><TAB><TAB><TAB>:param bucket_name: The name of the bucket.<TAB><TAB><TAB>:type bucket_name: str<TAB><TAB><TAB>:param resource: An optional dict with parameters for creating the bucket.<TAB><TAB><TAB><TAB>For information on available parameters, see Cloud Storage API doc:<TAB><TAB><TAB><TAB>https://cloud.google.com/storage/docs/json_api/v1/buckets/insert<TAB><TAB><TAB>:type resource: dict<TAB><TAB><TAB>:param storage_class: This defines how objects in the bucket are stored<TAB><TAB><TAB><TAB>and determines the SLA and the cost of storage. Values include<TAB><TAB><TAB><TAB><TAB>- ``MULTI_REGIONAL``<TAB><TAB><TAB><TAB>- ``REGIONAL``<TAB><TAB><TAB><TAB>- ``STANDARD``<TAB><TAB><TAB><TAB>- ``NEARLINE``<TAB><TAB><TAB><TAB>- ``COLDLINE``.<TAB><TAB><TAB><TAB><TAB>If this value is not specified when the bucket is<TAB><TAB><TAB><TAB>created, it will default to STANDARD.<TAB><TAB><TAB>:type storage_class: str<TAB><TAB><TAB>:param location: The location of the bucket.<TAB><TAB><TAB><TAB>Object data for objects in the bucket resides in physical storage<TAB><TAB><TAB><TAB>within this region. Defaults to US.<TAB><TAB><TAB><TAB><TAB>.. seealso::<TAB><TAB><TAB><TAB><TAB>https://developers.google.com/storage/docs/bucket-locations<TAB><TAB><TAB><TAB>:type location: str<TAB><TAB><TAB>:param project_id: The ID of the GCP Project.<TAB><TAB><TAB>:type project_id: str<TAB><TAB><TAB>:param labels: User-provided labels, in key/value pairs.<TAB><TAB><TAB>:type labels: dict<TAB><TAB><TAB>:return: If successful, it returns the ``id`` of the bucket.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>self.log.info('Creating Bucket: %s; Location: %s; Storage Class: %s',<TAB><TAB><TAB><TAB><TAB><TAB>  bucket_name, location, storage_class)<TAB><TAB><TAB><TAB>client = self.get_conn()<TAB><TAB><TAB>bucket = client.bucket(bucket_name=bucket_name)<TAB><TAB><TAB>bucket_resource = resource or {}<TAB><TAB><TAB><TAB>for item in bucket_resource:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>bucket._patch_property(name=item, value=resource[item])<TAB><TAB><TAB><TAB>bucket.storage_class = storage_class<TAB><TAB><TAB>bucket.labels = labels or {}<TAB><TAB><TAB>bucket.create(project=project_id, location=location)<TAB><TAB><TAB>return bucket.id",
        "target_block": "if item != \"name\":"
    },
    {
        "input_method": "def _set_bang_to_py_ast(ctx: GeneratorContext, node: SetBang) -> GeneratedPyAST:<TAB><TAB>\"\"\"Return a Python AST Node for a `set!` expression.\"\"\"<TAB><TAB>assert node.op == NodeOp.SET_BANG<TAB><TAB><TAB>val_temp_name = genname(\"set_bang_val\")<TAB><TAB>val_ast = gen_py_ast(ctx, node.val)<TAB><TAB><TAB>target = node.target<TAB><TAB>assert isinstance(<TAB><TAB><TAB>target, (HostField, Local, VarRef)<TAB><TAB>), f\"invalid set! target type {type(target)}\"<TAB><TAB><TAB><extra_id_0> no cover<TAB><TAB><TAB>raise GeneratorException(<TAB><TAB><TAB><TAB>f\"invalid set! target type {type(target)}\", lisp_ast=target<TAB><TAB><TAB>)<TAB><TAB><TAB>return GeneratedPyAST(<TAB><TAB><TAB>node=ast.Name(id=val_temp_name, ctx=ast.Load()),<TAB><TAB><TAB>dependencies=list(<TAB><TAB><TAB><TAB>chain(<TAB><TAB><TAB><TAB><TAB>val_ast.dependencies,<TAB><TAB><TAB><TAB><TAB>[<TAB><TAB><TAB><TAB><TAB><TAB>ast.Assign(<TAB><TAB><TAB><TAB><TAB><TAB><TAB>targets=[ast.Name(id=val_temp_name, ctx=ast.Store())],<TAB><TAB><TAB><TAB><TAB><TAB><TAB>value=val_ast.node,<TAB><TAB><TAB><TAB><TAB><TAB>)<TAB><TAB><TAB><TAB><TAB>],<TAB><TAB><TAB><TAB><TAB>target_ast.dependencies,<TAB><TAB><TAB><TAB><TAB>[ast.Assign(targets=[target_ast.node], value=val_ast.node)],<TAB><TAB><TAB><TAB>)<TAB><TAB><TAB>),<TAB><TAB>)",
        "target_block": "if isinstance(target, HostField):target_ast = _interop_prop_to_py_ast(ctx, target, is_assigning=True)elif isinstance(target, VarRef):target_ast = _var_sym_to_py_ast(ctx, target, is_assigning=True)elif isinstance(target, Local):target_ast = _local_sym_to_py_ast(ctx, target, is_assigning=True)else:  # pragma:"
    },
    {
        "input_method": "def f_get_from_runs(self, name, include_default_run=True, use_indices=False,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>fast_access=False, with_links = True,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>shortcuts=True, max_depth=None, auto_load=False):<TAB><TAB><TAB>\"\"\"Searches for all occurrences of `name` in each run.<TAB><TAB><TAB><TAB>Generates an ordered dictionary with the run names or indices as keys and<TAB><TAB><TAB>found items as values.<TAB><TAB><TAB><TAB>Example:<TAB><TAB><TAB><TAB>>>> traj.f_get_from_runs(self, 'deep.universal_answer', use_indices=True, fast_access=True)<TAB><TAB><TAB>OrderedDict([(0, 42), (1, 42), (2, 'fortytwo), (3, 43)])<TAB><TAB><TAB><TAB><TAB>:param name:<TAB><TAB><TAB><TAB><TAB>String description of the item(s) to find.<TAB><TAB><TAB><TAB>Cannot be full names but the part of the names that are below<TAB><TAB><TAB><TAB>a `run_XXXXXXXXX` group.<TAB><TAB><TAB><TAB>:param include_default_run:<TAB><TAB><TAB><TAB><TAB>If results found under ``run_ALL`` should be accounted for every run or simply be<TAB><TAB><TAB><TAB>ignored.<TAB><TAB><TAB><TAB>:param use_indices:<TAB><TAB><TAB><TAB><TAB>If `True` the keys of the resulting dictionary are the run indices<TAB><TAB><TAB><TAB>(e.g. 0,1,2,3), otherwise the keys are run names (e.g. `run_00000000`,<TAB><TAB><TAB><TAB>`run_000000001`)<TAB><TAB><TAB><TAB>:param fast_access:<TAB><TAB><TAB><TAB><TAB>Whether to return parameter or result instances or the values handled by these.<TAB><TAB><TAB><TAB>:param with_links:<TAB><TAB><TAB><TAB><TAB>If links should be considered<TAB><TAB><TAB><TAB>:param shortcuts:<TAB><TAB><TAB><TAB><TAB>If shortcuts are allowed and the trajectory can *hop* over nodes in the<TAB><TAB><TAB><TAB>path.<TAB><TAB><TAB><TAB>:param max_depth:<TAB><TAB><TAB><TAB><TAB>Maximum depth (relative to start node) how search should progress in tree.<TAB><TAB><TAB><TAB>`None` means no depth limit.  Only relevant <extra_id_0><TAB><TAB><TAB><TAB>self.v_crun = old_crun",
        "target_block": "if `shortcuts` are allowed.:param auto_load:If data should be loaded from the storage service if it cannot be found in thecurrent trajectory tree. Auto-loading will load group and leaf nodes currentlynot in memory and it will load data into empty leaves. Be aware that auto-loadingdoes not work with shortcuts.:return:Ordered dictionary with run names or indices as keys and found items as values.Will only include runs where an item was actually found.\"\"\"result_dict = OrderedDict()old_crun = self.v_cruntry:if len(self._run_parent_groups) > 0:for run_name in self.f_iter_runs():# Iterate over all runsvalue = Nonealready_found = Falsefor run_parent_group in self._run_parent_groups.values():if run_name not in run_parent_group._children:continuetry:value = run_parent_group.f_get(run_name + '.' + name,   fast_access=False,   with_links=with_links,   shortcuts=shortcuts,   max_depth=max_depth,   auto_load=auto_load)if already_found:raise pex.NotUniqueNodeError('`%s` has been found several times ' 'in one run.' % name)else:already_found = Trueexcept (AttributeError, pex.DataNotInStorageError):passif value is None and include_default_run:for run_parent_group in self._run_parent_groups.values():try:value = run_parent_group.f_get(self.f_wildcard('$', -1) +   '.' + name,   fast_access=False,   with_links=with_links,   shortcuts=shortcuts,   max_depth=max_depth,   auto_load=auto_load)if already_found:raise pex.NotUniqueNodeError('`%s` has been found several ' 'times in one run.' % name)else:already_found = Trueexcept (AttributeError, pex.DataNotInStorageError):passif value is not None:if value.v_is_leaf:value = self._nn_interface._apply_fast_access(value, fast_access)if use_indices:key = self.f_idx_to_run(run_name)else:key = run_nameresult_dict[key] = valuereturn result_dictfinally:"
    },
    {
        "input_method": "def write_connection_file(self):<TAB><TAB><TAB>\"\"\"write connection info to JSON file\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>cf = self.connection_file<TAB><TAB><TAB>write_connection_file(cf, ip=self.ip, key=self.session.key,<TAB><TAB><TAB>shell_port=self.shell_port, stdin_port=self.stdin_port, hb_port=self.hb_port,<TAB><TAB><TAB>iopub_port=self.iopub_port)<TAB><TAB><TAB><TAB><TAB><TAB>self._full_connection_file = cf",
        "target_block": "if os.path.basename(self.connection_file) == self.connection_file:cf = os.path.join(self.profile_dir.security_dir, self.connection_file)else:"
    },
    {
        "input_method": "def init_tree(self, tree_alias, context):<TAB><TAB><TAB>\"\"\"Initializes sitetree in memory.<TAB><TAB><TAB><TAB>Returns tuple with resolved tree alias and items on success.<TAB><TAB><TAB><TAB>On fail returns (None, None).<TAB><TAB><TAB><TAB>:param str|unicode tree_alias:<TAB><TAB><TAB>:param Context context:<TAB><TAB><TAB>:rtype: tuple<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>request = context.get('request', None)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return None, None<TAB><TAB><TAB><TAB>return tree_alias, sitetree_items",
        "target_block": "if request is None:raise SiteTreeError('Sitetree requires \"django.core.context_processors.request\" template context processor to be active. ''If it is, check that your view pushes request data into the template.')if id(request) != id(self.current_request):self.init(context)# Resolve tree_alias from the context.tree_alias = self.resolve_var(tree_alias)tree_alias, sitetree_items = self.get_sitetree(tree_alias)if not sitetree_items:"
    },
    {
        "input_method": "def f_dir_data(self):<TAB><TAB><TAB>\"\"\"Returns a list of all children names\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>pass<TAB><TAB><TAB>return list(self._children.keys())",
        "target_block": "if (self._nn_interface is not None andself._nn_interface._root_instance is not Noneand self.v_root.v_auto_load):try:if self.v_is_root:self.f_load(recursive=True, max_depth=1,load_data=pypetconstants.LOAD_SKELETON,with_meta_data=False,with_run_information=False)else:self.f_load(recursive=True, max_depth=1, load_data=pypetconstants.LOAD_SKELETON)except Exception as exc:"
    },
    {
        "input_method": "def _trigger_request(instance, request):<TAB><TAB>\"\"\"<TAB><TAB>Triggers request mock definition methods dynamically based on input<TAB><TAB>keyword arguments passed to `pook.Mock` constructor.<TAB><TAB><TAB>This is used to provide a more Pythonic interface vs chainable API<TAB><TAB>approach.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>getattr(instance, key)(getattr(request, key))",
        "target_block": "if not isinstance(request, Request):raise TypeError('request must be instance of pook.Request')# Register request matchersfor key in request.keys:if hasattr(instance, key):"
    },
    {
        "input_method": "def _x_request_elements_filter(cls, request_type, request_elements,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   credentials):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Google doesn't accept client ID and secret to be at the same time in<TAB><TAB><TAB>request parameters and in the basic authorization header in the access<TAB><TAB><TAB>token request.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>params = request_elements[2]<TAB><TAB><TAB><TAB>del params['client_id']<TAB><TAB><TAB><TAB>del params['client_secret']<TAB><TAB><TAB>return request_elements",
        "target_block": "if request_type is cls.ACCESS_TOKEN_REQUEST_TYPE:"
    },
    {
        "input_method": "def main(args=None):<TAB><TAB>\"\"\"Entry point for pkginfo tool<TAB><TAB>\"\"\"<TAB><TAB>options, paths = _parse_options(args)<TAB><TAB>format = getattr(options, 'output', 'simple')<TAB><TAB>formatter = _FORMATTERS[format](options)<TAB><TAB><TAB>for path in paths:<TAB><TAB><TAB>meta = get_metadata(path, options.metadata_version)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>filename = os.path.basename(path)<TAB><TAB><TAB><TAB><TAB>meta.download_url = '%s/%s' % (options.download_url_prefix,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   filename)<TAB><TAB><TAB><TAB>formatter(meta)<TAB><TAB><TAB>formatter.finish()",
        "target_block": "if meta is None:continueif options.download_url_prefix:if meta.download_url is None:"
    },
    {
        "input_method": "def create(<TAB><TAB><TAB><TAB>self, resource_group_name, account_name, certificate_name, parameters, if_match=None, if_none_match=None, custom_headers=None, raw=False, **operation_config):<TAB><TAB><TAB>\"\"\"Creates a new certificate inside the specified account.<TAB><TAB><TAB><TAB>:param resource_group_name: The name of the resource group that<TAB><TAB><TAB> contains the Batch account.<TAB><TAB><TAB>:type resource_group_name: str<TAB><TAB><TAB>:param account_name: The name of the Batch account.<TAB><TAB><TAB>:type account_name: str<TAB><TAB><TAB>:param certificate_name: The identifier for the certificate. This must<TAB><TAB><TAB> be made up of algorithm and thumbprint separated by a dash, and must<TAB><TAB><TAB> match the certificate data in the request. For example SHA1-a3d1c5.<TAB><TAB><TAB>:type certificate_name: str<TAB><TAB><TAB>:param parameters: Additional parameters for certificate creation.<TAB><TAB><TAB>:type parameters:<TAB><TAB><TAB> ~azure.mgmt.batch.models.CertificateCreateOrUpdateParameters<TAB><TAB><TAB>:param if_match: The entity state (ETag) version of the certificate to<TAB><TAB><TAB> update. A value of \"*\" can be used to apply the operation only <extra_id_0><TAB><TAB><TAB><TAB><TAB>client_raw_response = ClientRawResponse(deserialized, response)<TAB><TAB><TAB><TAB><TAB>client_raw_response.add_headers(header_dict)<TAB><TAB><TAB><TAB><TAB>return client_raw_response<TAB><TAB><TAB><TAB><TAB>return deserialized<TAB><TAB><TAB><TAB>long_running_operation_timeout = operation_config.get(<TAB><TAB><TAB><TAB>'long_running_operation_timeout',<TAB><TAB><TAB><TAB>self.config.long_running_operation_timeout)<TAB><TAB><TAB>return AzureOperationPoller(<TAB><TAB><TAB><TAB>long_running_send, get_long_running_output,<TAB><TAB><TAB><TAB>get_long_running_status, long_running_operation_timeout)",
        "target_block": "if the certificate already exists. If omitted, this operation will always be applied.:type if_match: str:param if_none_match: Set to '*' to allow a new certificate to be created, but to prevent updating an existing certificate. Other values will be ignored.:type if_none_match: str:param dict custom_headers: headers that will be added to the request:param bool raw: returns the direct response alongside the deserialized response:return: An instance of AzureOperationPoller that returns Certificate or ClientRawResponse if raw=true:rtype: ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.batch.models.Certificate] or ~msrest.pipeline.ClientRawResponse:raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\"\"\"raw_result = self._create_initial(resource_group_name=resource_group_name,account_name=account_name,certificate_name=certificate_name,parameters=parameters,if_match=if_match,if_none_match=if_none_match,custom_headers=custom_headers,raw=True,**operation_config)if raw:return raw_result# Construct and send requestdef long_running_send():return raw_result.responsedef get_long_running_status(status_link, headers=None):request = self._client.get(status_link)if headers:request.headers.update(headers)header_parameters = {}header_parameters['x-ms-client-request-id'] = raw_result.response.request.headers['x-ms-client-request-id']return self._client.send(request, header_parameters, stream=False, **operation_config)def get_long_running_output(response):if response.status_code not in [200]:exp = CloudError(response)exp.request_id = response.headers.get('x-ms-request-id')raise expheader_dict = {'ETag': 'str',}deserialized = self._deserialize('Certificate', response)if raw:"
    },
    {
        "input_method": "def get_bounds(self, coordinate_interleaved=None):<TAB><TAB><TAB>\"\"\"Returns the bounds of the index<TAB><TAB><TAB><TAB>:param coordinate_interleaved: If True, the coordinates are turned<TAB><TAB><TAB><TAB>in the form [xmin, ymin, ..., kmin, xmax, ymax, ..., kmax],<TAB><TAB><TAB><TAB>otherwise they are returned as<TAB><TAB><TAB><TAB>[xmin, xmax, ymin, ymax, ..., ..., kmin, kmax].  If not specified,<TAB><TAB><TAB><TAB>the :attr:`interleaved` member of the index is used, which<TAB><TAB><TAB><TAB>defaults to True.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>coordinate_interleaved = self.interleaved<TAB><TAB><TAB>return _get_bounds(<TAB><TAB><TAB><TAB>self.handle, core.rt.Index_GetBounds, coordinate_interleaved)",
        "target_block": "if coordinate_interleaved is None:"
    },
    {
        "input_method": "def _status_new(self):<TAB><TAB><TAB>\"\"\"Print the status of newly finished jobs.<TAB><TAB><TAB><TAB>Return True <extra_id_0>] = []<TAB><TAB><TAB>return new_comp or new_dead",
        "target_block": "if any new jobs are reported.This call resets its own state every time, so it only reports jobswhich have finished since the last time it was called.\"\"\"self._update_status()new_comp = self._group_report(self._comp_report, 'Completed')new_dead = self._group_report(self._dead_report,  'Dead, call jobs.traceback() for details')self._comp_report[:] = []self._dead_report[:"
    },
    {
        "input_method": "def set_errors(self, errors):<TAB><TAB><TAB>\"\"\"Set parameter error estimate \"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.__errors__ = None<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB>self.__errors__ = [asscalar(e) for e in errors]",
        "target_block": "if errors is None:"
    },
    {
        "input_method": "def select_name(source, name):<TAB><TAB>'''<TAB><TAB>Yields all the elements with the given name<TAB><TAB>source - <extra_id_0> x.xml_name == name, select_elements(source))",
        "target_block": "if an element, starts with all child elements in order; can also be any other iteratorname - will yield only elements with this name'''return filter(lambda x:"
    },
    {
        "input_method": "def __new_argv(self, *new_pargs, **new_kargs):<TAB><TAB><TAB>\"\"\"Calculate new argv and extra_argv values resulting from adding<TAB><TAB><TAB>the specified positional and keyword arguments.\"\"\"<TAB><TAB><TAB><TAB>new_argv = self.argv.copy()<TAB><TAB><TAB>new_extra_argv = list(self.extra_argv)<TAB><TAB><TAB><TAB>for v in new_pargs:<TAB><TAB><TAB><TAB>arg_name = None<TAB><TAB><TAB><TAB>for name in self.pargl:<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>raise TypeError(\"%s() got an unexpected keyword argument '%s'\" \\<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>% (self.__name__, k))<TAB><TAB><TAB><TAB>new_argv[k] = v<TAB><TAB><TAB><TAB>return (new_argv, new_extra_argv)",
        "target_block": "if not name in new_argv:arg_name = namebreakif arg_name:new_argv[arg_name] = velif self.var_pargs:new_extra_argv.append(v)else:num_prev_pargs = len([name for name in self.pargl if name in self.argv])raise TypeError(\"%s() takes exactly %d positional arguments (%d given)\" \\% (self.__name__,   len(self.pargl),   num_prev_pargs + len(new_pargs)))for k,v in new_kargs.items():if not (self.var_kargs or (k in self.pargl) or (k in self.kargl)):"
    },
    {
        "input_method": "def split_frame(self, ratios=None, destination_frames=None, seed=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Split a frame into distinct subsets of size determined by the given ratios.<TAB><TAB><TAB><TAB>The number of subsets is always 1 more than the number of ratios given. Note that<TAB><TAB><TAB>this does not give an exact split. H2O is designed to be efficient on big data<TAB><TAB><TAB>using a probabilistic splitting method rather than an exact split. For example<TAB><TAB><TAB>when specifying a split of 0.75/0.25, H2O will produce a test/train split with<TAB><TAB><TAB>an expected value of 0.75/0.25 rather than exactly 0.75/0.25. On small datasets,<TAB><TAB><TAB>the sizes of the resulting splits will deviate from the expected value more than<TAB><TAB><TAB>on big data, where they will be very close to exact.<TAB><TAB><TAB><TAB>:param List[float] ratios: The fractions of rows for each split.<TAB><TAB><TAB>:param List[str] destination_frames: The names of the split frames.<TAB><TAB><TAB>:param int seed: seed for the random number generator<TAB><TAB><TAB><TAB>:returns: A list of H2OFrames<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>assert_is_type(ratios, [numeric], None)<TAB><TAB><TAB>assert_is_type(destination_frames, [str], None)<TAB><TAB><TAB>assert_is_type(seed, int, None)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>destination_frame_id = destination_frames[i]<TAB><TAB><TAB><TAB><TAB>tmp_slice.frame_id = destination_frame_id<TAB><TAB><TAB><TAB><TAB>splits.append(tmp_slice)<TAB><TAB><TAB><TAB><TAB>i += 1<TAB><TAB><TAB><TAB>del tmp_runif<TAB><TAB><TAB>return splits",
        "target_block": "if ratios is None:ratios = [0.75]if not ratios:raise ValueError(\"Ratios array may not be empty\")if destination_frames is not None:if len(ratios) + 1 != len(destination_frames):raise ValueError(\"The number of provided destination_frames must be one more \" \"than the number of provided ratios\")num_slices = len(ratios) + 1boundaries = []last_boundary = 0i = 0while i < num_slices - 1:ratio = ratios[i]if ratio < 0:raise ValueError(\"Ratio must be greater than 0\")boundary = last_boundary + ratioif boundary >= 1.0:raise ValueError(\"Ratios must add up to less than 1.0\")boundaries.append(boundary)last_boundary = boundaryi += 1splits = []tmp_runif = self.runif(seed)tmp_runif.frame_id = \"%s_splitter\" % _py_tmp_key(h2o.connection().session_id)i = 0while i < num_slices:if i == 0:# lower_boundary is 0.0upper_boundary = boundaries[i]tmp_slice = self[(tmp_runif <= upper_boundary), :]elif i == num_slices - 1:lower_boundary = boundaries[i - 1]# upper_boundary is 1.0tmp_slice = self[(tmp_runif > lower_boundary), :]else:lower_boundary = boundaries[i - 1]upper_boundary = boundaries[i]tmp_slice = self[((tmp_runif > lower_boundary) & (tmp_runif <= upper_boundary)), :]if destination_frames is None:splits.append(tmp_slice)else:"
    },
    {
        "input_method": "def reset(cls, uid=None):<TAB><TAB><TAB>\"\"\"Mostly used for unit testing. Allow to use a static uuid and reset<TAB><TAB><TAB>all counter\"\"\"<TAB><TAB><TAB>for instance in cls.instances:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>instance.uid = uid<TAB><TAB><TAB><TAB>instance.generator = instance.get_generator()",
        "target_block": "if uid:"
    },
    {
        "input_method": "def write_to_fil(self, filename_out, *args, **kwargs):<TAB><TAB><TAB>\"\"\" Write data to .fil file.<TAB><TAB><TAB><TAB>It check the file size then decides how to write the file.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>filename_out (str): Name of output file<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>#For timing how long it takes to write a file.<TAB><TAB><TAB>t0 = time.time()<TAB><TAB><TAB><TAB>#Update header<TAB><TAB><TAB>self.__update_header()<TAB><TAB><TAB><TAB><extra_id_0> %2.2fsec' % (t1- t0))",
        "target_block": "if self.container.isheavy():self.__write_to_fil_heavy(filename_out)else:self.__write_to_fil_light(filename_out)t1 = time.time()logger.info('Conversion time:"
    },
    {
        "input_method": "def transformed_value(self, output_name=DEFAULT_OUTPUT):<TAB><TAB><TAB>'''Returns transformed value either for DEFAULT_OUTPUT or for the output<TAB><TAB><TAB>given as output_name. Returns None <extra_id_0><TAB><TAB><TAB><TAB>return None",
        "target_block": "if execution result isn't a success.Reconstructs the pipeline context to materialize value.'''check.str_param(output_name, 'output_name')if not self.solid.definition.has_output(output_name):raise DagsterInvariantViolationError('{output_name} not defined in solid {solid}'.format(output_name=output_name, solid=self.solid.name))if self.success:for result in self.transforms:if (result.is_successful_outputand result.step_output_data.output_name == output_name):with self.reconstruct_context() as context:value = self._get_value(context, result.step_output_data)return valueraise DagsterInvariantViolationError(('Did not find result {output_name} in solid {self.solid.name} ''execution result').format(output_name=output_name, self=self))else:"
    },
    {
        "input_method": "def enable_gui(gui, kernel=None):<TAB><TAB>\"\"\"Enable integration with a given GUI\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>raise RuntimeError(\"Cannot activate multiple GUI eventloops\")<TAB><TAB>kernel.eventloop = loop",
        "target_block": "if gui not in loop_map:raise ValueError(\"GUI %r not supported\" % gui)if kernel is None:if Application.initialized():kernel = getattr(Application.instance(), 'kernel', None)if kernel is None:raise RuntimeError(\"You didn't specify a kernel,\"\" and no IPython Application with a kernel appears to be running.\")loop = loop_map[gui]if kernel.eventloop is not None and kernel.eventloop is not loop:"
    },
    {
        "input_method": "def start_logging(self, dest=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Start logging all API requests to the provided destination.<TAB><TAB><TAB><TAB>:param dest: Where to write the log: either a filename (str), or an open file handle (file). If not given,<TAB><TAB><TAB><TAB>then a new temporary file will be created.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>assert_is_type(dest, None, str, type(sys.stdout))<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>dest = os.path.join(tempfile.mkdtemp(), \"h2o-connection.log\")<TAB><TAB><TAB>self._print(\"Now logging all API requests to file %r\" % dest)<TAB><TAB><TAB>self._is_logging = True<TAB><TAB><TAB>self._logging_dest = dest",
        "target_block": "if dest is None:"
    },
    {
        "input_method": "def add_record(self, schema, _bump_stack_level=False):<TAB><TAB><TAB>\"\"\" Add record class to record store for retrieval at record load time.<TAB><TAB><TAB><TAB><TAB>Can be used as a class decorator<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>full_name = get_full_name(schema)<TAB><TAB><TAB>has_namespace = '.' in full_name<TAB><TAB><TAB>self._force_add(full_name, schema, _bump_stack_level, _raise_on_existing=has_namespace)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self._force_add(schema.__name__, schema, _bump_stack_level)<TAB><TAB><TAB>return schema",
        "target_block": "if has_namespace and schema.__name__ not in self._schema_map:"
    },
    {
        "input_method": "def deobfuscate(request, key, juice=None):<TAB><TAB>\"\"\"<TAB><TAB>Deobfuscates the URL and returns HttpResponse from source view.<TAB><TAB>SEO juice is mostly ignored as it is intended for display purposes only.<TAB><TAB>\"\"\"<TAB><TAB>try:<TAB><TAB><TAB>url = decrypt(str(key),<TAB><TAB><TAB><TAB><TAB><TAB>  settings.UNFRIENDLY_SECRET,<TAB><TAB><TAB><TAB><TAB><TAB>  settings.UNFRIENDLY_IV,<TAB><TAB><TAB><TAB><TAB><TAB>  checksum=settings.UNFRIENDLY_ENFORCE_CHECKSUM)<TAB><TAB>except (CheckSumError, InvalidKeyError):<TAB><TAB><TAB>return HttpResponseNotFound()<TAB><TAB><TAB>try:<TAB><TAB><TAB>url = url.decode('utf-8')<TAB><TAB>except UnicodeDecodeError:<TAB><TAB><TAB>return HttpResponseNotFound()<TAB><TAB><TAB>url_parts = urlparse(unquote(url))<TAB><TAB>path = url_parts.path<TAB><TAB>query = url_parts.query<TAB><TAB><TAB>try:<TAB><TAB><TAB>view, args, kwargs = resolve(path)<TAB><TAB>except Resolver404:<TAB><TAB><TAB>return HttpResponseNotFound()<TAB><TAB><TAB># fix-up the environ object<TAB><TAB>environ = request.environ.copy()<TAB><TAB>environ['PATH_INFO'] = path[len(environ['SCRIPT_NAME']):]<TAB><TAB>environ['QUERY_STRING'] = query<TAB><TAB><TAB># init a new request<TAB><TAB>patched_request = request.__class__(environ)<TAB><TAB><TAB># copy over any missing request attributes - this feels hackish<TAB><TAB>missing_items = set(dir(request)) - set(dir(patched_request))<TAB><TAB>while missing_items:<TAB><TAB><TAB>missing_item = missing_items.pop()<TAB><TAB><TAB>patched_request.__setattr__(missing_item,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>request.__getattribute__(missing_item))<TAB><TAB><TAB># mark this request as obfuscated<TAB><TAB>patched_request.META['obfuscated'] = True<TAB><TAB><TAB>response = view(patched_request, *args, **kwargs)<TAB><TAB><TAB># offer up a friendlier juice-powered filename <extra_id_0><TAB><TAB><TAB>response['Content-Disposition'] = 'inline; filename=%s' % juice<TAB><TAB><TAB>return response",
        "target_block": "if downloadedif juice and not response.has_header('Content-Disposition'):"
    },
    {
        "input_method": "def ensure_running(self):<TAB><TAB><TAB>'''Make sure that semaphore tracker process is running.<TAB><TAB><TAB><TAB>This can be run from any process.  Usually a child process will use<TAB><TAB><TAB>the semaphore created by its parent.'''<TAB><TAB><TAB>with self._lock:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>os.close(r)",
        "target_block": "if self._fd is not None:# semaphore tracker was launched before, is it still running?if self._check_alive():# => still alivereturn# => dead, launch it againos.close(self._fd)try:# Clean-up to avoid dangling processes.os.waitpid(self._pid, 0)except OSError:# The process was terminated or is a child from an ancestor# of the current process.passself._fd = Noneself._pid = Nonewarnings.warn('semaphore_tracker: process died unexpectedly, '  'relaunching.  Some semaphores might leak.')fds_to_pass = []try:fds_to_pass.append(sys.stderr.fileno())except Exception:passr, w = os.pipe()cmd = 'from {} import main; main({}, {})'.format(main.__module__, r, VERBOSE)try:fds_to_pass.append(r)# process will out live us, so no need to wait on pidexe = spawn.get_executable()args = [exe] + util._args_from_interpreter_flags()# In python 3.3, there is a bug which put `-RRRRR..` instead of# `-R` in args. Replace it to get the correct flags.# See https://github.com/python/cpython/blob/3.3/Lib/subprocess.py#L488if sys.version_info[:2] <= (3, 3):import refor i in range(1, len(args)):args[i] = re.sub(\"-R+\", \"-R\", args[i])args += ['-c', cmd]util.debug(\"launching Semaphore tracker: {}\".format(args))# bpo-33613: Register a signal mask that will block the# signals.  This signal mask will be inherited by the child# that is going to be spawned and will protect the child from a# race condition that can make the child die before it# registers signal handlers for SIGINT and SIGTERM. The mask is# unregistered after spawning the child.try:if _HAVE_SIGMASK:signal.pthread_sigmask(signal.SIG_BLOCK,   _IGNORED_SIGNALS)pid = spawnv_passfds(exe, args, fds_to_pass)finally:if _HAVE_SIGMASK:signal.pthread_sigmask(signal.SIG_UNBLOCK,   _IGNORED_SIGNALS)except BaseException:os.close(w)raiseelse:self._fd = wself._pid = pidfinally:"
    },
    {
        "input_method": "def interp_harmonics(x, freqs, h_range, kind='linear', fill_value=0, axis=0):<TAB><TAB>'''Compute the energy at harmonics of time-frequency representation.<TAB><TAB><TAB>Given a frequency-based energy representation such as a spectrogram<TAB><TAB>or tempogram, this function computes the energy at the chosen harmonics<TAB><TAB>of the frequency axis.  (See examples below.)<TAB><TAB>The resulting harmonic array can then be used as input to a salience<TAB><TAB>computation.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>x : np.ndarray<TAB><TAB><TAB>The input energy<TAB><TAB><TAB>freqs : np.ndarray, shape=(X.shape[axis])<TAB><TAB><TAB>The frequency values corresponding to X's elements along the<TAB><TAB><TAB>chosen axis.<TAB><TAB><TAB>h_range : list-like, non-negative<TAB><TAB><TAB>Harmonics to compute.  The first harmonic (1) corresponds to `x`<TAB><TAB><TAB>itself.<TAB><TAB><TAB>Values less than one (e.g., 1/2) correspond to sub-harmonics.<TAB><TAB><TAB>kind : str<TAB><TAB><TAB>Interpolation type.  See `scipy.interpolate.interp1d`.<TAB><TAB><TAB>fill_value : float<TAB><TAB><TAB>The value to fill when extrapolating beyond the observed<TAB><TAB><TAB>frequency range.<TAB><TAB><TAB>axis : int<TAB><TAB><TAB>The axis along which to compute harmonics<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>x_harm : np.ndarray, shape=(len(h_range), [x.shape])<TAB><TAB><TAB>`x_harm[i]` will have the same shape as `x`, and measure<TAB><TAB><TAB>the energy at the `h_range[i]` harmonic of each frequency.<TAB><TAB><TAB>See Also<TAB><TAB>--------<TAB><TAB>scipy.interpolate.interp1d<TAB><TAB><TAB><TAB>Examples<TAB><TAB>--------<TAB><TAB>Estimate the harmonics of a time-averaged tempogram<TAB><TAB><TAB>>>> y, sr = librosa.load(librosa.util.example_audio_file(),<TAB><TAB>...<TAB><TAB><TAB><TAB><TAB>  duration=15, offset=30)<TAB><TAB>>>> # Compute the time-varying tempogram and average over time<TAB><TAB>>>> tempi = np.mean(librosa.feature.tempogram(y=y, sr=sr), axis=1)<TAB><TAB>>>> # We'll measure the first five harmonics<TAB><TAB>>>> h_range = [1, 2, 3, 4, 5]<TAB><TAB>>>> f_tempo = librosa.tempo_frequencies(len(tempi), sr=sr)<TAB><TAB>>>> # Build the harmonic tensor<TAB><TAB>>>> t_harmonics = librosa.interp_harmonics(tempi, f_tempo, h_range)<TAB><TAB>>>> print(t_harmonics.shape)<TAB><TAB>(5, 384)<TAB><TAB><TAB>>>> # And plot the results<TAB><TAB>>>> import matplotlib.pyplot as plt<TAB><TAB>>>> plt.figure()<TAB><TAB>>>> librosa.display.specshow(t_harmonics, x_axis='tempo', sr=sr)<TAB><TAB>>>> plt.yticks(0.5 + np.arange(len(h_range)),<TAB><TAB>...<TAB><TAB><TAB>['{:.3g}'.format(_) for _ in h_range])<TAB><TAB>>>> plt.ylabel('Harmonic')<TAB><TAB>>>> plt.xlabel('Tempo (BPM)')<TAB><TAB>>>> plt.tight_layout()<TAB><TAB><TAB>We can also compute frequency harmonics for spectrograms.<TAB><TAB>To calculate sub-harmonic energy, use values < 1.<TAB><TAB><TAB>>>> h_range = [1./3, 1./2, 1, 2, 3, 4]<TAB><TAB>>>> S = np.abs(librosa.stft(y))<TAB><TAB>>>> fft_freqs = librosa.fft_frequencies(sr=sr)<TAB><TAB>>>> S_harm = librosa.interp_harmonics(S, fft_freqs, h_range, axis=0)<TAB><TAB>>>> print(S_harm.shape)<TAB><TAB>(6, 1025, 646)<TAB><TAB><TAB>>>> plt.figure()<TAB><TAB>>>> for i, _sh in enumerate(S_harm, 1):<TAB><TAB>...<TAB> plt.subplot(3, 2, i)<TAB><TAB>...<TAB> librosa.display.specshow(librosa.amplitude_to_db(_sh,<TAB><TAB>...<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  ref=S.max()),<TAB><TAB>...<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  sr=sr, y_axis='log')<TAB><TAB>...<TAB> plt.title('h={:.3g}'.format(h_range[i-1]))<TAB><TAB>...<TAB> plt.yticks([])<TAB><TAB>>>> plt.tight_layout()<TAB><TAB>'''<TAB><TAB><TAB># X_out will be the same shape as X, plus a leading<TAB><TAB># axis that has length = len(h_range)<TAB><TAB>out_shape = [len(h_range)]<TAB><TAB>out_shape.extend(x.shape)<TAB><TAB><TAB>x_out = np.zeros(out_shape, dtype=x.dtype)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>raise ParameterError('freqs.shape={} does not match '<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> 'input shape={}'.format(freqs.shape, x.shape))<TAB><TAB><TAB>return x_out",
        "target_block": "if freqs.ndim == 1 and len(freqs) == x.shape[axis]:harmonics_1d(x_out, x, freqs, h_range, kind=kind, fill_value=fill_value, axis=axis)elif freqs.ndim == 2 and freqs.shape == x.shape:harmonics_2d(x_out, x, freqs, h_range, kind=kind, fill_value=fill_value, axis=axis)else:"
    },
    {
        "input_method": "def sections(self) -> List['Section']:<TAB><TAB><TAB>\"\"\"Return a list of section in current wikitext.<TAB><TAB><TAB><TAB>The first section will always be the lead section, even <extra_id_0><TAB><TAB><TAB><TAB><TAB>span = old_span<TAB><TAB><TAB><TAB>sections_append(Section(lststr, type_to_spans, span, 'Section'))<TAB><TAB><TAB>return sections",
        "target_block": "if it is anempty string.\"\"\"sections = []  # type: List['Section']sections_append = sections.appendtype_to_spans = self._type_to_spanslststr = self._lststrss, se = _span = self._spantype_spans = type_to_spans.setdefault('Section', [])full_match = SECTIONS_FULLMATCH(self._shadow)section_spans = full_match.spans('section')levels = [len(eq) for eq in full_match.captures('equals')]if not type_spans:# All spans are newspans_append = type_spans.appendfor current_index, (current_level, (s, e)) in enumerate(zip(levels, section_spans), 1):# Add text of the current_section to any parent section.# Note that section 0 is not a parent for any subsection.for section_index, section_level in enumerate(levels[current_index:], current_index):if current_level and section_level > current_level:e = section_spans[section_index][1]else:breakspan = [ss + s, ss + e]spans_append(span)sections_append(Section(lststr, type_to_spans, span, 'Section'))return sections# There are already some spans. Instead of appending new spans# use them when the detected span already exists.span_tuple_to_span = {(s[0], s[1]): s for s in type_spans}.getfor current_index, (current_level, (s, e)) in enumerate(zip(levels, section_spans), 1):# Add text of the current_section to any parent section.# Note that section 0 is not a parent for any subsection.for section_index, section_level in enumerate(levels[current_index:], current_index):if current_level and section_level > current_level:e = section_spans[section_index][1]else:breaks, e = ss + s, ss + eold_span = span_tuple_to_span((s, e))if old_span is None:span = [s, e]insort(type_spans, span)else:"
    },
    {
        "input_method": "def do_GET(self, _id):<TAB><TAB><TAB>\"\"\"Get document from index with its id.<TAB><TAB><TAB>GET 772210180J\"\"\"<TAB><TAB><TAB>doc = doc_by_id(_id)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>return -1<TAB><TAB><TAB><TAB>housenumbers = sorted(doc['housenumbers'].values(), key=sorter)<TAB><TAB><TAB><TAB>print(white('housenumbers'),<TAB><TAB><TAB><TAB><TAB>  magenta(', '.join(v['raw'] for v in housenumbers)))",
        "target_block": "if not doc:return self.error('id \"{}\" not found'.format(_id))for key, value in doc.items():if key == config.HOUSENUMBERS_FIELD:continueprint('{} {}'.format(white(key), magenta(value)))if doc.get('housenumbers'):def sorter(v):try:return int(re.match(r'^\\d+', v['raw']).group())except AttributeError:"
    },
    {
        "input_method": "def finalize(self, remove_all_handlers=True):<TAB><TAB><TAB>\"\"\"Finalizes the manager, closes and removes all handlers <extra_id_0><TAB><TAB><TAB><TAB>self.tabula_rasa()",
        "target_block": "if desired.\"\"\"for tool in self._tools:tool.finalize()self._tools = []self._stdout_to_logger = Nonefor config in (self._sp_config, self._mp_config):if hasattr(config, 'close'):config.close()self._sp_config = Noneself._mp_config = Noneif remove_all_handlers:"
    },
    {
        "input_method": "def _onOutgoingConnected(self, conn):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Callback for when a new connection from this to another node is established. Handles encryption and informs the other node which node this is.<TAB><TAB><TAB>If encryption is disabled, this triggers the onNodeConnected callback and messages are deferred to the onMessageReceived callback.<TAB><TAB><TAB>If encryption is enabled, the first message is handled by _onOutgoingMessageReceived.<TAB><TAB><TAB><TAB>:param conn: connection object<TAB><TAB><TAB>:type conn: TcpConnection<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>conn.send('readonly')<TAB><TAB><TAB><TAB>self._onNodeConnected(self._connToNode(conn))",
        "target_block": "if self._syncObj.encryptor:conn.setOnMessageReceivedCallback(functools.partial(self._onOutgoingMessageReceived, conn)) # So we can process the sendRandKeyconn.recvRandKey = os.urandom(32)conn.send(conn.recvRandKey)else:# The onMessageReceived callback is configured in addNode already.if not self._selfIsReadonlyNode:conn.send(self._selfNode.address)else:"
    },
    {
        "input_method": "def _check_index(self, key: Union[slice, int]) -> (int, int):<TAB><TAB><TAB>\"\"\"Return adjusted start and stop index as tuple.<TAB><TAB><TAB><TAB>Used in  __setitem__ and __delitem__.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>ss, se = self._span<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise IndexError(<TAB><TAB><TAB><TAB><TAB>'stop index out of range or start is after the stop')<TAB><TAB><TAB>return start + ss, stop + ss",
        "target_block": "if isinstance(key, int):if key < 0:key += se - ssif key < 0:raise IndexError('index out of range')elif key >= se - ss:raise IndexError('index out of range')start = ss + keyreturn start, start + 1# isinstance(key, slice)if key.step is not None:raise NotImplementedError('step is not implemented for string setter.')start, stop = key.start or 0, key.stopif start < 0:start += se - ssif start < 0:raise IndexError('start index out of range')if stop is None:stop = se - sselif stop < 0:stop += se - ssif start > stop:"
    },
    {
        "input_method": "def _unregister_engine(self, msg):<TAB><TAB><TAB>\"\"\"Unregister an engine that has died.\"\"\"<TAB><TAB><TAB>content = msg['content']<TAB><TAB><TAB>eid = int(content['id'])<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self._stop_scheduling_tasks()",
        "target_block": "if eid in self._ids:self._ids.remove(eid)uuid = self._engines.pop(eid)self._handle_stranded_msgs(eid, uuid)if self._task_socket and self._task_scheme == 'pure':"
    },
    {
        "input_method": "def running_under_virtualenv():<TAB><TAB>\"\"\"<TAB><TAB>Return True <extra_id_0><TAB><TAB><TAB>return True<TAB><TAB><TAB>return False",
        "target_block": "if we're running inside a virtualenv, False otherwise.\"\"\"if hasattr(sys, 'real_prefix'):return Trueelif sys.prefix != getattr(sys, \"base_prefix\", sys.prefix):"
    },
    {
        "input_method": "def main(sample_id, assembly, min_size):<TAB><TAB>\"\"\"Main executor of the split_fasta template.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>sample_id : str<TAB><TAB><TAB>Sample Identification string.<TAB><TAB>assembly : list<TAB><TAB><TAB>Assembly file.<TAB><TAB>min_size : int<TAB><TAB><TAB>Minimum contig size.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>logger.info(\"Starting script\")<TAB><TAB><TAB>f_open = open(assembly, \"rU\")<TAB><TAB><TAB>entry = (x[1] for x in groupby(f_open, lambda line: line[0] == \">\"))<TAB><TAB><TAB>success = 0<TAB><TAB><TAB>for header in entry:<TAB><TAB><TAB>headerStr = header.__next__()[1:].strip()<TAB><TAB><TAB>seq = \"\".join(s.strip() for s in entry.__next__())<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>output_file.write(\">\" + sample_id + \"_\" + headerStr.replace(\" \",\"_\").replace(\"=\",\"_\") + \"\\\\n\" + seq + \"\\\\n\")<TAB><TAB><TAB><TAB><TAB>success += 1<TAB><TAB><TAB>f_open.close()<TAB><TAB><TAB>logger.info(\"{} sequences sucessfully splitted.\".format(success))",
        "target_block": "if len(seq) >= min_size:with open(sample_id + '_' + headerStr.replace(\" \",\"_\").replace(\"=\",\"_\") + '.fasta', \"w\") as output_file:"
    },
    {
        "input_method": "def parse_enum_results_list(response, return_type, resp_type, item_type):<TAB><TAB><TAB>\"\"\"resp_body is the XML we received<TAB><TAB><TAB>resp_type is a string, such as Containers,<TAB><TAB><TAB>return_type is the type we're constructing, such as ContainerEnumResults<TAB><TAB><TAB>item_type is the type object of the item to be created, such as Container<TAB><TAB><TAB><TAB>This function then returns a ContainerEnumResults object with the<TAB><TAB><TAB>containers member populated with the results.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB># parsing something like:<TAB><TAB><TAB># <EnumerationResults ... ><TAB><TAB><TAB>#   <Queues><TAB><TAB><TAB>#<TAB>   <Queue><TAB><TAB><TAB>#<TAB><TAB>   <Something /><TAB><TAB><TAB>#<TAB><TAB>   <SomethingElse /><TAB><TAB><TAB>#<TAB>   </Queue><TAB><TAB><TAB>#   </Queues><TAB><TAB><TAB># </EnumerationResults><TAB><TAB><TAB>return_obj = return_type()<TAB><TAB><TAB>root = ETree.fromstring(response.body)<TAB><TAB><TAB><TAB>items = []<TAB><TAB><TAB><TAB>for container_element in root.findall(resp_type):<TAB><TAB><TAB><TAB>for item_element in container_element.findall(resp_type[:-1]):<TAB><TAB><TAB><TAB><TAB>items.append(_ETreeXmlToObject.fill_instance_element(item_element, item_type))<TAB><TAB><TAB><TAB>for name, value in vars(return_obj).items():<TAB><TAB><TAB><TAB># queues, Queues, this is the list its self which we populated<TAB><TAB><TAB><TAB># above<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>setattr(return_obj, name, value)<TAB><TAB><TAB><TAB>setattr(return_obj, resp_type.lower(), items)<TAB><TAB><TAB>return return_obj",
        "target_block": "if name == resp_type.lower():# the list its self.continuevalue = _ETreeXmlToObject.fill_data_member(root, name, value)if value is not None:"
    },
    {
        "input_method": "def _directed_changed(self, new):<TAB><TAB><TAB>\"\"\" Sets the connection string for all edges.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>edge.conn = conn",
        "target_block": "if new:conn = \"->\"else:conn = \"--\"for edge in [e for g in self.all_graphs for e in g.edges]:"
    },
    {
        "input_method": "def cut_video(in_file,<TAB><TAB><TAB><TAB>  out_file,<TAB><TAB><TAB><TAB>  start=None,<TAB><TAB><TAB><TAB>  end=None,<TAB><TAB><TAB><TAB>  vcodec=None,<TAB><TAB><TAB><TAB>  acodec=None,<TAB><TAB><TAB><TAB>  log_level='info',<TAB><TAB><TAB><TAB>  print_cmd=False,<TAB><TAB><TAB><TAB>  **kwargs):<TAB><TAB>\"\"\"Cut a clip from a video.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>in_file (str): Input video filename.<TAB><TAB><TAB>out_file (str): Output video filename.<TAB><TAB><TAB>start (None or float): Start time (in seconds).<TAB><TAB><TAB>end (None or float): End time (in seconds).<TAB><TAB><TAB>vcodec (None or str): Output video codec, None for unchanged.<TAB><TAB><TAB>acodec (None or str): Output audio codec, None for unchanged.<TAB><TAB><TAB>log_level (str): Logging level of ffmpeg.<TAB><TAB><TAB>print_cmd (bool): Whether to print the final ffmpeg command.<TAB><TAB>\"\"\"<TAB><TAB>options = {'log_level': log_level}<TAB><TAB><extra_id_0><TAB><TAB><TAB>options['t'] = end - start<TAB><TAB>convert_video(in_file, out_file, print_cmd, **options)",
        "target_block": "if vcodec is None:options['vcodec'] = 'copy'if acodec is None:options['acodec'] = 'copy'if start:options['ss'] = startelse:start = 0if end:"
    },
    {
        "input_method": "async def wait_changed(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Waits until the result set changes. Possible changes can be a result<TAB><TAB><TAB>being added or the result set becoming complete. If the result set is<TAB><TAB><TAB>already completed, this method returns immediately.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>waiter = self._loop.create_future()<TAB><TAB><TAB><TAB>self._waiters.append(waiter)<TAB><TAB><TAB><TAB>await waiter",
        "target_block": "if not self.is_complete():"
    },
    {
        "input_method": "def _check_boolean_expressions(self, node):<TAB><TAB><TAB>\"\"\"Go through \"if\" node `node` and counts its boolean expressions<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.add_message(<TAB><TAB><TAB><TAB><TAB>\"too-many-boolean-expressions\",<TAB><TAB><TAB><TAB><TAB>node=condition,<TAB><TAB><TAB><TAB><TAB>args=(nb_bool_expr, self.config.max_bool_expr),<TAB><TAB><TAB><TAB>)",
        "target_block": "if the \"if\" node test is a BoolOp node\"\"\"condition = node.testif not isinstance(condition, BoolOp):returnnb_bool_expr = _count_boolean_expressions(condition)if nb_bool_expr > self.config.max_bool_expr:"
    },
    {
        "input_method": "def peek_text(self, text: str) -> bool:<TAB><TAB><TAB>\"\"\"Same as readText but doesn't consume the stream.\"\"\"<TAB><TAB><TAB>start = self._stream.index<TAB><TAB><TAB>stop = start + len(text)<TAB><TAB><TAB><extra_id_0>stop] == text",
        "target_block": "if stop > self._stream.eos_index:return Falsereturn self._stream[self._stream.index:"
    },
    {
        "input_method": "def connection_made(self, transport):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Called when a remote worker connection has been found. Finishes setting<TAB><TAB><TAB>up the protocol object.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>logger.debug(\"worker tried to connect while manager was closed\")<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB><TAB>logger.debug(\"new worker connected\")<TAB><TAB><TAB><TAB>self._transport = transport<TAB><TAB><TAB>self._buffer = bytearray()<TAB><TAB><TAB>self._worker = Worker(self._transport, self._manager)<TAB><TAB><TAB>self._workers.add(self._worker)",
        "target_block": "if self._manager.is_closed():"
    },
    {
        "input_method": "def visit(self, node):<TAB><TAB><TAB>\"\"\"launch the visit starting from the given node\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return methods[1](node)<TAB><TAB><TAB>return None",
        "target_block": "if node in self._visited:return Noneself._visited[node] = 1  # FIXME: use set ?methods = self.get_callbacks(node)if methods[0] is not None:methods[0](node)if hasattr(node, \"locals\"):  # skip Instance and other proxyfor local_node in node.values():self.visit(local_node)if methods[1] is not None:"
    },
    {
        "input_method": "def _parser(self, fl):<TAB><TAB><TAB>\"\"\"Parser for a single abricate output file.<TAB><TAB><TAB><TAB>This parser will scan a single Abricate output file and populate<TAB><TAB><TAB>the :py:attr:`Abricate.storage` attribute.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>fl : str<TAB><TAB><TAB><TAB>Path to abricate output file<TAB><TAB><TAB><TAB>Notes<TAB><TAB><TAB>-----<TAB><TAB><TAB>This method will populate the :py:attr:`Abricate.storage` attribute<TAB><TAB><TAB>with all compliant lines in the abricate output file. Entries are<TAB><TAB><TAB>inserted using an arbitrary key that is set by the<TAB><TAB><TAB>:py:attr:`Abricate._key` attribute.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>with open(fl) as fh:<TAB><TAB><TAB><TAB><TAB>for line in fh:<TAB><TAB><TAB><TAB><TAB># Skip header and comment lines<TAB><TAB><TAB><TAB><TAB><extra_id_0> identity<TAB><TAB><TAB><TAB><TAB>}<TAB><TAB><TAB><TAB><TAB><TAB>self._key += 1",
        "target_block": "if line.startswith(\"#\") or line.strip() == \"\":continuefields = line.strip().split(\"\\t\")try:coverage = float(fields[8])except ValueError:coverage = Nonetry:identity = float(fields[9])except ValueError:identity = Nonetry:accession = fields[11]except IndexError:accession = Noneself.storage[self._key] = {\"log_file\": os.path.basename(fl),\"infile\": fields[0],\"reference\": fields[1],\"seq_range\": (int(fields[2]), int(fields[3])),\"gene\": fields[4],\"accession\": accession,\"database\": fields[10],\"coverage\": coverage,\"identity\":"
    },
    {
        "input_method": "def _downsample_one_or_the_other(mask, mask_indexes, stft, stft_indexes):<TAB><TAB>\"\"\"<TAB><TAB>Takes the given `mask` and `stft`, which must be matrices of shape `frequencies, times`<TAB><TAB>and downsamples one of them into the other one's times, so that the time dimensions<TAB><TAB>are equal. Leaves the frequency dimension untouched.<TAB><TAB>\"\"\"<TAB><TAB>assert len(mask.shape) == 2, \"Expected a two-dimensional `mask`, but got one of {} dimensions.\".format(len(mask.shape))<TAB><TAB>assert len(stft.shape) == 2, \"Expected a two-dimensional `stft`, but got one of {} dimensions.\".format(len(stft.shape))<TAB><TAB><TAB><extra_id_0>, indexes]<TAB><TAB><TAB>stft_indexes = np.array(indexes)<TAB><TAB><TAB>return mask, mask_indexes, stft, stft_indexes",
        "target_block": "if mask.shape[1] > stft.shape[1]:downsample_factor = mask.shape[1] / stft.shape[1]indexes = _get_downsampled_indexes(mask, downsample_factor)mask = mask[:, indexes]mask_indexes = np.array(indexes)elif mask.shape[1] < stft.shape[1]:downsample_factor = stft.shape[1] / mask.shape[1]indexes = _get_downsampled_indexes(stft, downsample_factor)stft = stft[:"
    },
    {
        "input_method": "def _get_index_urls_locations(self, project_name):<TAB><TAB><TAB>\"\"\"Returns the locations found via self.index_urls<TAB><TAB><TAB><TAB>Checks the url_name on the main (first in the list) index and<TAB><TAB><TAB>use this url_name to produce all locations<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>def mkurl_pypi_url(url):<TAB><TAB><TAB><TAB>loc = posixpath.join(url, project_url_name)<TAB><TAB><TAB><TAB># For maximum compatibility with easy_install, ensure the path<TAB><TAB><TAB><TAB># ends in a trailing slash.  Although this isn't in the spec<TAB><TAB><TAB><TAB># (and PyPI can handle it without the slash) some other index<TAB><TAB><TAB><TAB># implementations might break <extra_id_0><TAB><TAB><TAB><TAB>return [mkurl_pypi_url(url) for url in self.index_urls]<TAB><TAB><TAB>return []",
        "target_block": "if they relied on easy_install's# behavior.if not loc.endswith('/'):loc = loc + '/'return locproject_url_name = urllib_parse.quote(project_name.lower())if self.index_urls:# Check that we have the url_name correctly spelled:# Only check main index if index URL is givenmain_index_url = Link(mkurl_pypi_url(self.index_urls[0]),trusted=True,)page = self._get_page(main_index_url)if page is None and PyPI.netloc not in str(main_index_url):warnings.warn(\"Failed to find %r at %s. It is suggested to upgrade \"\"your index to support normalized names as the name in \"\"/simple/{name}.\" % (project_name, main_index_url),RemovedInPip8Warning,)project_url_name = self._find_url_name(Link(self.index_urls[0], trusted=True),project_url_name,) or project_url_nameif project_url_name is not None:"
    },
    {
        "input_method": "def update_backend_info(self, interval=60):<TAB><TAB>\"\"\"Updates the monitor info<TAB><TAB>Called from another thread.<TAB><TAB>\"\"\"<TAB><TAB>my_thread = threading.currentThread()<TAB><TAB>current_interval = 0<TAB><TAB>started = False<TAB><TAB>all_dead = False<TAB><TAB>stati = [None]*len(self._backends)<TAB><TAB>while getattr(my_thread, \"do_run\", True) and not all_dead:<TAB><TAB><TAB><extra_id_0>#dc267f'>False</h5>\"<TAB><TAB><TAB><TAB><TAB>started = True<TAB><TAB><TAB><TAB>current_interval = 0<TAB><TAB><TAB>time.sleep(1)<TAB><TAB><TAB>all_dead = not any([wid._is_alive for wid in self.children])<TAB><TAB><TAB>current_interval += 1",
        "target_block": "if current_interval == interval or started is False:for ind, back in enumerate(self._backends):_value = self.children[ind].children[2].value_head = _value.split('<b>')[0]try:_status = back.status()stati[ind] = _statusexcept Exception:  # pylint: disable=W0703self.children[ind].children[2].value = _value.replace(_head, \"<h5 style='color:#ff5c49'>\")self.children[ind]._is_alive = Falseelse:self.children[ind]._is_alive = Trueself.children[ind].children[2].value = _value.replace(_head, \"<h5>\")idx = list(range(len(self._backends)))pending = [s.pending_jobs for s in stati]_, least_idx = zip(*sorted(zip(pending, idx)))# Make sure least pending is operationalfor ind in least_idx:if stati[ind].operational:least_pending_idx = indbreakfor var in idx:if var == least_pending_idx:self.children[var].children[4].value = \"<h5 style='color:#34bc6e'>True</h5>\"else:self.children[var].children[4].value = \"<h5 style='color:#dc267f'>False</h5>\"self.children[var].children[3].children[1].value = pending[var]self.children[var].children[3].children[1].max = max(self.children[var].children[3].children[1].max, pending[var]+10)if stati[var].operational:self.children[var].children[5].value = \"<h5 style='color:#34bc6e'>True</h5>\"else:self.children[var].children[5].value = \"<h5 style='color:"
    },
    {
        "input_method": "def update_rate_limit(self, response):<TAB><TAB><TAB>\"\"\"Update the rate limit and the time to reset<TAB><TAB><TAB>from the response headers.<TAB><TAB><TAB><TAB>:param: response: the response object<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.rate_limit_reset_ts = None",
        "target_block": "if self.rate_limit_header in response.headers:self.rate_limit = int(response.headers[self.rate_limit_header])logger.debug(\"Rate limit: %s\", self.rate_limit)else:self.rate_limit = Noneif self.rate_limit_reset_header in response.headers:self.rate_limit_reset_ts = int(response.headers[self.rate_limit_reset_header])logger.debug(\"Rate limit reset: %s\", self.calculate_time_to_reset())else:"
    },
    {
        "input_method": "def when_label_changed ( self, object, listener, remove ):<TAB><TAB><TAB>\"\"\" Sets up or removes a listener for the label being changed on a<TAB><TAB><TAB><TAB>specified object.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>label = self.label<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>object.on_trait_change( listener, label, remove = remove,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>dispatch = 'ui' )",
        "target_block": "if label[:1] != '=':"
    },
    {
        "input_method": "def _handle_complete_reply(self, rep):<TAB><TAB><TAB>\"\"\" Handle replies for tab completion.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self.log.debug(\"complete: %s\", rep.get('content', ''))<TAB><TAB><TAB>cursor = self._get_cursor()<TAB><TAB><TAB>info = self._request_info.get('complete')<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>text = '.'.join(self._get_context())<TAB><TAB><TAB><TAB>cursor.movePosition(QtGui.QTextCursor.Left, n=len(text))<TAB><TAB><TAB><TAB>self._complete_with_items(cursor, rep['content']['matches'])",
        "target_block": "if info and info.id == rep['parent_header']['msg_id'] and \\info.pos == cursor.position():"
    },
    {
        "input_method": "def decorated_with_property(node: astroid.FunctionDef) -> bool:<TAB><TAB>\"\"\" Detect <extra_id_0><TAB><TAB><TAB><TAB>pass<TAB><TAB>return False",
        "target_block": "if the given function node is decorated with a property. \"\"\"if not node.decorators:return Falsefor decorator in node.decorators.nodes:if not isinstance(decorator, astroid.Name):continuetry:if _is_property_decorator(decorator):return Trueexcept astroid.InferenceError:"
    },
    {
        "input_method": "def _file_lines(fname):<TAB><TAB>\"\"\"Return the contents of a named file as a list of lines.<TAB><TAB><TAB>This function never raises an IOError exception: <extra_id_0><TAB><TAB><TAB>out = outfile.readlines()<TAB><TAB><TAB>outfile.close()<TAB><TAB><TAB>return out",
        "target_block": "if the file can't beread, it simply returns an empty list.\"\"\"try:outfile = open(fname)except IOError:return []else:"
    },
    {
        "input_method": "def _find_multiproc_options(parser):<TAB><TAB><TAB>\"\"\" Searches for multiprocessing options within a ConfigParser.<TAB><TAB><TAB><TAB>If such options are found, they are copied (without the `'multiproc_'` prefix)<TAB><TAB><TAB>into a new parser.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>sections = parser.sections()<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>val = parser.get(section, option, raw=True)<TAB><TAB><TAB><TAB><TAB><TAB>mp_parser.set(new_section, option, val)<TAB><TAB><TAB>return mp_parser",
        "target_block": "if not any(section.startswith('multiproc_') for section in sections):return Nonemp_parser = NoInterpolationParser()for section in sections:if section.startswith('multiproc_'):new_section = section.replace('multiproc_', '')mp_parser.add_section(new_section)options = parser.options(section)for option in options:"
    },
    {
        "input_method": "def get_t_tag_content(<TAB><TAB><TAB>t, parent, remove_bold, remove_italics, meta_data):<TAB><TAB>\"\"\"<TAB><TAB>Generate the string data that for this particular t tag.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>text = '<em>%s</em>' % text<TAB><TAB>return text",
        "target_block": "if t is None or t.text is None:return ''# Need to escape the text so that we do not accidentally put in text# that is not valid XML.# cgi will replace things like & < > with &amp; &lt; &gt;text = cgi.escape(t.text)# Wrap the text with any modifiers it might have (bold, italics or# underline)el_is_bold = not remove_bold and (is_bold(parent) oris_underlined(parent))el_is_italics = not remove_italics and is_italics(parent)if el_is_bold:text = '<strong>%s</strong>' % textif el_is_italics:"
    },
    {
        "input_method": "def restore(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Restore data from the given path.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB><extra_id_0> self.backup_content[file_to_restore][<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>alternatives[string]<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>]<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>}<TAB><TAB><TAB><TAB><TAB><TAB><TAB>)",
        "target_block": "if PyFunceble.CONFIGURATION[\"auto_continue\"] and self.backup_content:# The auto_continue subsystem is activated and the backup_content# is not empty.# We get the file we have to restore.file_to_restore = PyFunceble.INTERN[\"file_to_test\"]if file_to_restore in self.backup_content:# The file we are working with is already into the backup content.# We initiate the different status to set.to_initiate = [\"up\", \"down\", \"invalid\", \"tested\"]# Because at some time it was not the current status, we have to map# the new with the old. This way, if someone is running the latest# version but with old data, we still continue like nothing happend.alternatives = {\"up\": \"number_of_up\",\"down\": \"number_of_down\",\"invalid\": \"number_of_invalid\",\"tested\": \"number_of_tested\",}for string in to_initiate:# We loop over the status we have to initiate.try:# We try to update the counters by using the currently read status.PyFunceble.INTERN[\"counter\"][\"number\"].update({string: self.backup_content[file_to_restore][string]})except KeyError:# But if the status is not present, we try with the older index# we mapped previously.PyFunceble.INTERN[\"counter\"][\"number\"].update({string:"
    },
    {
        "input_method": "def stop_channels(self):<TAB><TAB><TAB>\"\"\"Stops all the running channels for this kernel.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.hb_channel.stop()",
        "target_block": "if self.shell_channel.is_alive():self.shell_channel.stop()if self.sub_channel.is_alive():self.sub_channel.stop()if self.stdin_channel.is_alive():self.stdin_channel.stop()if self.hb_channel.is_alive():"
    },
    {
        "input_method": "def autosummary_table_visit_html(self, node):<TAB><TAB>\"\"\"Make the first column of the table non-breaking.\"\"\"<TAB><TAB>try:<TAB><TAB><TAB>tbody = node[0][0][-1]<TAB><TAB><TAB>for row in tbody:<TAB><TAB><TAB><TAB>col1_entry = row[0]<TAB><TAB><TAB><TAB>par = col1_entry[0]<TAB><TAB><TAB><TAB>for j, subnode in enumerate(list(par)):<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>pass",
        "target_block": "if isinstance(subnode, nodes.Text):new_text = unicode(subnode.astext())new_text = new_text.replace(u\" \", u\"\\u00a0\")par[j] = nodes.Text(new_text)except IndexError:"
    },
    {
        "input_method": "def recursive_unicode(obj):<TAB><TAB>\"\"\"Walks a simple data structure, converting byte strings to unicode.<TAB><TAB><TAB>Supports lists, tuples, and dictionaries.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>return obj",
        "target_block": "if isinstance(obj, dict):return dict((recursive_unicode(k), recursive_unicode(v)) for (k,v) in obj.iteritems())elif isinstance(obj, list):return list(recursive_unicode(i) for i in obj)elif isinstance(obj, tuple):return tuple(recursive_unicode(i) for i in obj)elif isinstance(obj, bytes):return to_unicode(obj)else:"
    },
    {
        "input_method": "def activate(backend):<TAB><TAB>'''activate a backend by adding it to the .sregistry configuration file.<TAB><TAB>'''<TAB><TAB>settings = read_client_secrets()<TAB><TAB><extra_id_0><TAB><TAB><TAB>settings['SREGISTRY_CLIENT'] = backend<TAB><TAB><TAB>update_secrets(settings)<TAB><TAB><TAB>print('[activate] %s' %backend)",
        "target_block": "if backend is not None:"
    },
    {
        "input_method": "def mme_delete(case_obj, mme_base_url, mme_token):<TAB><TAB>\"\"\"Delete all affected samples for a case from MatchMaker<TAB><TAB><TAB>Args:<TAB><TAB><TAB>case_obj(dict) a scout case object<TAB><TAB><TAB>mme_base_url(str) base url of the MME server<TAB><TAB><TAB>mme_token(str) auth token of the MME server<TAB><TAB><TAB>Returns:<TAB><TAB><TAB> server_responses(list): a list of object of this type:<TAB><TAB><TAB><TAB><TAB><TAB>{<TAB><TAB><TAB><TAB><TAB><TAB><TAB>'patient_id': patient_id<TAB><TAB><TAB><TAB><TAB><TAB><TAB>'message': server_message,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>'status_code': server_status_code<TAB><TAB><TAB><TAB><TAB><TAB>}<TAB><TAB>\"\"\"<TAB><TAB>server_responses = []<TAB><TAB><TAB><extra_id_0> resp.get('status_code')<TAB><TAB><TAB>})<TAB><TAB><TAB>return server_responses",
        "target_block": "if not mme_base_url or not mme_token:return 'Please check that Matchmaker connection parameters are valid'# for each patient of the case in matchmakerfor patient in case_obj['mme_submission']['patients']:# send delete request to server and capture server's responsepatient_id = patient['id']url = ''.join([mme_base_url, '/patient/delete/', patient_id])resp = matchmaker_request(url=url, token=mme_token, method='DELETE', )server_responses.append({'patient_id': patient_id,'message': resp.get('message'),'status_code':"
    },
    {
        "input_method": "def do_help(self, command):<TAB><TAB><TAB>\"\"\"Display this help message.\"\"\"<TAB><TAB><TAB><extra_id_0>]),<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> cyan(doc.replace(' ' * 8, ' ')<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> .replace('\\n', ''))))",
        "target_block": "if command:doc = getattr(self, 'do_' + command).__doc__print(cyan(doc.replace(' ' * 8, '')))else:print(magenta('Available commands:'))print(magenta('Type \"HELP <command>\" to get more info.'))names = self.get_names()names.sort()for name in names:if name[:3] != 'do_':continuedoc = getattr(self, name).__doc__doc = doc.split('\\n')[0]print('{} {}'.format(yellow(name[3:"
    },
    {
        "input_method": "def get_model(self, project_id, model_name):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Gets a Model. Blocks until finished.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0> %s', e)<TAB><TAB><TAB><TAB><TAB>return None<TAB><TAB><TAB><TAB>raise",
        "target_block": "if not model_name:raise ValueError(\"Model name must be provided and \" \"it could not be an empty string\")full_model_name = 'projects/{}/models/{}'.format(project_id, model_name)request = self._mlengine.projects().models().get(name=full_model_name)try:return request.execute()except HttpError as e:if e.resp.status == 404:self.log.error('Model was not found:"
    },
    {
        "input_method": "def cssfile(url):<TAB><TAB>'''<TAB><TAB>Output a link tag to a css stylesheet.<TAB><TAB>'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>#add media_url for relative paths<TAB><TAB><TAB>url = settings.STATIC_URL + url<TAB><TAB><TAB>return '<link href=\"{src}\" rel=\"stylesheet\">'.format(src=url)",
        "target_block": "if not url.startswith('http://') and not url[:1] == '/':"
    },
    {
        "input_method": "def complete_restore(<TAB><TAB><TAB><TAB>self, location_name, operation_id, last_backup_name, custom_headers=None, raw=False, polling=True, **operation_config):<TAB><TAB><TAB>\"\"\"Completes the restore operation on a managed database.<TAB><TAB><TAB><TAB>:param location_name: The name of the region where the resource is<TAB><TAB><TAB> located.<TAB><TAB><TAB>:type location_name: str<TAB><TAB><TAB>:param operation_id: Management operation id that this request tries<TAB><TAB><TAB> to complete.<TAB><TAB><TAB>:type operation_id: str<TAB><TAB><TAB>:param last_backup_name: The last backup name to apply<TAB><TAB><TAB>:type last_backup_name: str<TAB><TAB><TAB>:param dict custom_headers: headers that will be added to the request<TAB><TAB><TAB>:param bool raw: The poller return type is ClientRawResponse, the<TAB><TAB><TAB> direct response alongside the deserialized response<TAB><TAB><TAB>:param polling: True for ARMPolling, False for no polling, or a<TAB><TAB><TAB> polling object for personal polling strategy<TAB><TAB><TAB>:return: An instance of LROPoller that returns None or<TAB><TAB><TAB> ClientRawResponse<None> <extra_id_0> polling_method = polling<TAB><TAB><TAB>return LROPoller(self._client, raw_result, get_long_running_output, polling_method)",
        "target_block": "if raw==True:rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]:raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`\"\"\"raw_result = self._complete_restore_initial(location_name=location_name,operation_id=operation_id,last_backup_name=last_backup_name,custom_headers=custom_headers,raw=True,**operation_config)def get_long_running_output(response):if raw:client_raw_response = ClientRawResponse(None, response)return client_raw_responselro_delay = operation_config.get('long_running_operation_timeout',self.config.long_running_operation_timeout)if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)elif polling is False: polling_method = NoPolling()else:"
    },
    {
        "input_method": "def receive_subscription_message(self, topic_name, subscription_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> peek_lock=True, timeout=60):<TAB><TAB><TAB>'''<TAB><TAB><TAB>Receive a message from a subscription for processing.<TAB><TAB><TAB><TAB>topic_name:<TAB><TAB><TAB><TAB>Name of the topic.<TAB><TAB><TAB>subscription_name:<TAB><TAB><TAB><TAB>Name of the subscription.<TAB><TAB><TAB>peek_lock:<TAB><TAB><TAB><TAB>Optional. True to retrieve and lock the message. False to read and<TAB><TAB><TAB><TAB>delete the message. Default is True (lock).<TAB><TAB><TAB>timeout:<TAB><TAB><TAB><TAB>Optional. The timeout parameter is expressed in seconds.<TAB><TAB><TAB>'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return self.peek_lock_subscription_message(topic_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   subscription_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   timeout)<TAB><TAB><TAB>return self.read_delete_subscription_message(topic_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> subscription_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> timeout)",
        "target_block": "if peek_lock:"
    },
    {
        "input_method": "def title(languages=None, genders=None):<TAB><TAB>\"\"\"<TAB><TAB>returns a random title<TAB><TAB><TAB>.. code-block:: python<TAB><TAB><TAB><TAB>>>> d.title()<TAB><TAB><TAB>u'Mrs.'<TAB><TAB><TAB>>>> d.title(['es'])<TAB><TAB><TAB>u'El Sr.'<TAB><TAB><TAB>>>> d.title(None, [GENDER_FEMALE])<TAB><TAB><TAB>u'Mrs.'<TAB><TAB><TAB>:param languages: list of allowed languages. ['en'] <extra_id_0>1}[random.choice(genders)]<TAB><TAB><TAB>return random.choice(choices)[gender]",
        "target_block": "if None:param genders: list of allowed genders. (GENDER_FEMALE, GENDER_MALE) if None\"\"\"languages = languages or ['en']genders = genders or (GENDER_FEMALE, GENDER_MALE)choices = _get_titles(languages)gender = {'m':0, 'f':"
    },
    {
        "input_method": "def pretty_print(input_word, anagrams, by_length=False):<TAB><TAB>\"\"\"Prints the anagram results sorted by score to stdout.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>input_word: the base word we searched on<TAB><TAB><TAB>anagrams: generator of (word, score) from anagrams_in_word<TAB><TAB><TAB>by_length: a boolean to declare printing by length instead of score<TAB><TAB>\"\"\"<TAB><TAB><TAB>scores = {}<TAB><TAB><extra_id_0> {2}\".format(key, noun, \", \".join(value)))",
        "target_block": "if by_length:noun = \"tiles\"for word, score in anagrams:try:scores[len(word)].append(\"{0} ({1:d})\".format(word, score))except KeyError:scores[len(word)] = [\"{0} ({1:d})\".format(word, score)]else:noun = \"points\"for word, score in anagrams:try:scores[score].append(word)except KeyError:scores[score] = [word]print(\"Anagrams for {0}{1}:\".format(input_word, \" (score)\" * by_length))if not valid_scrabble_word(input_word):print(\"{0} is not possible in Scrabble.\".format(input_word))for key, value in sorted(scores.items(), reverse=True):print(\"{0:d} {1}:"
    },
    {
        "input_method": "def valid(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>``True`` <extra_id_0><TAB><TAB><TAB><TAB>return True",
        "target_block": "if credentials are valid, ``False`` if expired.\"\"\"if self.expiration_time:return self.expiration_time > int(time.time())else:"
    },
    {
        "input_method": "def copy_expert(self, sql, filename, open=open):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Executes SQL using psycopg2 copy_expert method.<TAB><TAB><TAB>Necessary to execute COPY command without access to a superuser.<TAB><TAB><TAB><TAB>Note: <extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>cur.copy_expert(sql, f)<TAB><TAB><TAB><TAB><TAB><TAB>f.truncate(f.tell())<TAB><TAB><TAB><TAB><TAB><TAB>conn.commit()",
        "target_block": "if this method is called with a \"COPY FROM\" statement andthe specified input file does not exist, it creates an emptyfile and no data is loaded, but the operation succeeds.So if users want to be aware when the input file does not exist,they have to check its existence by themselves.\"\"\"if not os.path.isfile(filename):with open(filename, 'w'):passwith open(filename, 'r+') as f:with closing(self.get_conn()) as conn:with closing(conn.cursor()) as cur:"
    },
    {
        "input_method": "def sample_chain(<TAB><TAB>num_results,<TAB><TAB>current_state,<TAB><TAB>previous_kernel_results=None,<TAB><TAB>kernel=None,<TAB><TAB>num_burnin_steps=0,<TAB><TAB>num_steps_between_results=0,<TAB><TAB>trace_fn=lambda current_state, kernel_results: kernel_results,<TAB><TAB>return_final_kernel_results=False,<TAB><TAB>parallel_iterations=10,<TAB><TAB>name=None,<TAB>):<TAB>  \"\"\"Implements Markov chain Monte Carlo via repeated `TransitionKernel` steps.<TAB><TAB>  This function samples from an Markov chain at `current_state` and whose<TAB>  stationary distribution is governed by the supplied `TransitionKernel`<TAB>  instance (`kernel`).<TAB><TAB>  This function can sample from multiple chains, in parallel. (Whether or not<TAB>  there are multiple chains is dictated by the `kernel`.)<TAB><TAB>  The `current_state` can be represented as a single `Tensor` or a `list` of<TAB>  `Tensors` which collectively represent the current state.<TAB><TAB>  Since MCMC states are correlated, it is sometimes desirable to produce<TAB>  additional intermediate states, and then discard them, ending up with a set of<TAB>  states with decreased autocorrelation.  See [Owen (2017)][1]. Such \"thinning\"<TAB>  is made possible by setting `num_steps_between_results > 0`. The chain then<TAB>  takes `num_steps_between_results` extra steps between the steps that make it<TAB>  into the results. The extra steps are never materialized (in calls to<TAB>  `sess.run`), and thus do not increase memory requirements.<TAB><TAB>  Warning: when setting a `seed` in the `kernel`, ensure that `sample_chain`'s<TAB>  `parallel_iterations=1`, otherwise results will not be reproducible.<TAB><TAB>  In addition to returning the chain state, this function supports tracing of<TAB>  auxiliary variables used by the kernel. The traced values are selected by<TAB>  specifying `trace_fn`. By default, all kernel results are traced but in the<TAB>  future the default will be changed to no results being traced, so plan<TAB>  accordingly. See below for some examples of this feature.<TAB><TAB>  Args:<TAB><TAB>num_results: Integer number of Markov chain draws.<TAB><TAB>current_state: `Tensor` or Python `list` of `Tensor`s representing the<TAB><TAB>  current state(s) of the Markov chain(s).<TAB><TAB>previous_kernel_results: A `Tensor` or a nested collection of `Tensor`s<TAB><TAB>  representing internal calculations made within the previous call to this<TAB><TAB>  function (or as returned by `bootstrap_results`).<TAB><TAB>kernel: An instance of `tfp.mcmc.TransitionKernel` which implements one step<TAB><TAB>  of the Markov chain.<TAB><TAB>num_burnin_steps: Integer number of chain steps to take before starting to<TAB><TAB>  collect results.<TAB><TAB>  Default value: 0 (i.e., no burn-in).<TAB><TAB>num_steps_between_results: Integer number of chain steps between collecting<TAB><TAB>  a result. Only one out of every `num_steps_between_samples + 1` steps is<TAB><TAB>  included in the returned results.  The number of returned chain states is<TAB><TAB>  still equal to `num_results`.  Default value: 0 (i.e., no thinning).<TAB><TAB>trace_fn: A callable that takes in the current chain state and the previous<TAB><TAB>  kernel results and return a `Tensor` or a nested collection of `Tensor`s<TAB><TAB>  that is then traced along with the chain state.<TAB><TAB>return_final_kernel_results: If `True`, then the final kernel results are<TAB><TAB>  returned alongside the chain state and the trace specified by the<TAB><TAB>  `trace_fn`.<TAB><TAB>parallel_iterations: The number of iterations allowed to run in parallel. It<TAB><TAB>  must be a positive integer. See `tf.while_loop` for more details.<TAB><TAB>name: Python `str` name prefixed to Ops created by this function.<TAB><TAB>  Default value: `None` (i.e., \"mcmc_sample_chain\").<TAB><TAB>  Returns:<TAB><TAB>checkpointable_states_and_trace: <extra_id_0><TAB><TAB><TAB>return StatesAndTrace(all_states=all_states, trace=trace)",
        "target_block": "if `return_final_kernel_results` is  `True`. The return value is an instance of  `CheckpointableStatesAndTrace`.all_states: if `return_final_kernel_results` is `False` and `trace_fn` is  `None`. The return value is a `Tensor` or Python list of `Tensor`s  representing the state(s) of the Markov chain(s) at each result step. Has  same shape as input `current_state` but with a prepended  `num_results`-size dimension.states_and_trace: if `return_final_kernel_results` is `False` and  `trace_fn` is not `None`. The return value is an instance of  `StatesAndTrace`.  #### Examples  ##### Sample from a diagonal-variance Gaussian.  I.e.,  ```none  for i=1..n:x[i] ~ MultivariateNormal(loc=0, scale=diag(true_stddev))  # likelihood  ```  ```python  import tensorflow as tf  import tensorflow_probability as tfp  tfd = tfp.distributions  dims = 10  true_stddev = np.sqrt(np.linspace(1., 3., dims))  likelihood = tfd.MultivariateNormalDiag(loc=0., scale_diag=true_stddev)  states = tfp.mcmc.sample_chain(  num_results=1000,  num_burnin_steps=500,  current_state=tf.zeros(dims),  kernel=tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=likelihood.log_prob,step_size=0.5,num_leapfrog_steps=2),  trace_fn=None)  sample_mean = tf.reduce_mean(states, axis=0)  # ==> approx all zeros  sample_stddev = tf.sqrt(tf.reduce_mean(  tf.squared_difference(states, sample_mean),  axis=0))  # ==> approx equal true_stddev  ```  ##### Sampling from factor-analysis posteriors with known factors.  I.e.,  ```none  # prior  w ~ MultivariateNormal(loc=0, scale=eye(d))  for i=1..n:# likelihoodx[i] ~ Normal(loc=w^T F[i], scale=1)  ```  where `F` denotes factors.  ```python  import tensorflow as tf  import tensorflow_probability as tfp  tfd = tfp.distributions  # Specify model.  def make_prior(dims):return tfd.MultivariateNormalDiag(loc=tf.zeros(dims))  def make_likelihood(weights, factors):return tfd.MultivariateNormalDiag(loc=tf.matmul(weights, factors, adjoint_b=True))  def joint_log_prob(num_weights, factors, x, w):return (make_prior(num_weights).log_prob(w) +make_likelihood(w, factors).log_prob(x))  def unnormalized_log_posterior(w):# Posterior is proportional to: `p(W, X=x | factors)`.return joint_log_prob(num_weights, factors, x, w)  # Setup data.  num_weights = 10 # == d  num_factors = 40 # == n  num_chains = 100  weights = make_prior(num_weights).sample(1)  factors = tf.random_normal([num_factors, num_weights])  x = make_likelihood(weights, factors).sample()  # Sample from Hamiltonian Monte Carlo Markov Chain.  # Get `num_results` samples from `num_chains` independent chains.  chains_states, kernels_results = tfp.mcmc.sample_chain(  num_results=1000,  num_burnin_steps=500,  current_state=tf.zeros([num_chains, num_weights], name='init_weights'),  kernel=tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=unnormalized_log_posterior,step_size=0.1,num_leapfrog_steps=2))  # Compute sample stats.  sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])  # ==> approx equal to weights  sample_var = tf.reduce_mean(  tf.squared_difference(chains_states, sample_mean),  axis=[0, 1])  # ==> less than 1  ```  ##### Custom tracing functions.  ```python  import tensorflow as tf  import tensorflow_probability as tfp  tfd = tfp.distributions  likelihood = tfd.Normal(loc=0., scale=1.)  def sample_chain(trace_fn):return tfp.mcmc.sample_chain(  num_results=1000,  num_burnin_steps=500,  current_state=0.,  kernel=tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=likelihood.log_prob,step_size=0.5,num_leapfrog_steps=2),  trace_fn=trace_fn)  def trace_log_accept_ratio(states, previous_kernel_results):return previous_kernel_results.log_accept_ratio  def trace_everything(states, previous_kernel_results):return previous_kernel_results  _, log_accept_ratio = sample_chain(trace_fn=trace_log_accept_ratio)  _, kernel_results = sample_chain(trace_fn=trace_everything)  acceptance_prob = tf.exp(tf.minimum(log_accept_ratio_, 0.))  # Equivalent to, but more efficient than:  acceptance_prob = tf.exp(tf.minimum(kernel_results.log_accept_ratio_, 0.))  ```  #### References  [1]: Art B. Owen. Statistically efficient thinning of a Markov chain sampler.   _Technical Report_, 2017.   http://statweb.stanford.edu/~owen/reports/bestthinning.pdf  \"\"\"  if not kernel.is_calibrated:warnings.warn(\"supplied `TransitionKernel` is not calibrated. Markov \"  \"chain may not converge to intended target distribution.\")  with tf.compat.v1.name_scope(  name, \"mcmc_sample_chain\",  [num_results, num_burnin_steps, num_steps_between_results]):num_results = tf.convert_to_tensor(value=num_results, dtype=tf.int32, name=\"num_results\")num_burnin_steps = tf.convert_to_tensor(value=num_burnin_steps, dtype=tf.int32, name=\"num_burnin_steps\")num_steps_between_results = tf.convert_to_tensor(value=num_steps_between_results,dtype=tf.int32,name=\"num_steps_between_results\")current_state = tf.nest.map_structure(lambda x: tf.convert_to_tensor(value=x, name=\"current_state\"),current_state)if previous_kernel_results is None:  previous_kernel_results = kernel.bootstrap_results(current_state)if trace_fn is None:  # It simplifies the logic to use a dummy function here.  trace_fn = lambda *args: ()  no_trace = Trueelse:  no_trace = Falseif trace_fn is sample_chain.__defaults__[4]:  warnings.warn(\"Tracing all kernel results by default is deprecated. Set \"\"the `trace_fn` argument to None (the future default \"\"value) or an explicit callback that traces the values \"\"you are interested in.\")def _trace_scan_fn(state_and_results, num_steps):  next_state, current_kernel_results = mcmc_util.smart_for_loop(  loop_num_iter=num_steps,  body_fn=kernel.one_step,  initial_loop_vars=list(state_and_results),  parallel_iterations=parallel_iterations)  return next_state, current_kernel_results(_, final_kernel_results), (all_states, trace) = mcmc_util.trace_scan(loop_fn=_trace_scan_fn,initial_state=(current_state, previous_kernel_results),elems=tf.one_hot(indices=0,depth=num_results,on_value=1 + num_burnin_steps,off_value=1 + num_steps_between_results,dtype=tf.int32),# pylint: disable=g-long-lambdatrace_fn=lambda state_and_results: (state_and_results[0],trace_fn(*state_and_results)),# pylint: enable=g-long-lambdaparallel_iterations=parallel_iterations)if return_final_kernel_results:  return CheckpointableStatesAndTrace(  all_states=all_states,  trace=trace,  final_kernel_results=final_kernel_results)else:  if no_trace:return all_states  else:"
    },
    {
        "input_method": "def _create_any_param_or_result(self, parent_node, name, type_name, instance, constructor,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>args, kwargs):<TAB><TAB><TAB>\"\"\"Generically creates a novel parameter or result instance inferring from the `type_name`.<TAB><TAB><TAB><TAB>If the instance is already supplied it is NOT constructed new.<TAB><TAB><TAB><TAB>:param parent_node:<TAB><TAB><TAB><TAB><TAB>Parent trajectory node<TAB><TAB><TAB><TAB>:param name:<TAB><TAB><TAB><TAB><TAB>Name of the new result or parameter. Here the name no longer contains colons.<TAB><TAB><TAB><TAB>:param type_name:<TAB><TAB><TAB><TAB><TAB>Whether it is a parameter below parameters, config, derived parameters or whether<TAB><TAB><TAB><TAB>it is a result.<TAB><TAB><TAB><TAB>:param instance:<TAB><TAB><TAB><TAB><TAB>The instance <extra_id_0><TAB><TAB><TAB><TAB>instance._explored = True  # Mark this parameter as explored.<TAB><TAB><TAB><TAB>self._root_instance._explored_parameters[full_name] = instance<TAB><TAB><TAB><TAB>self._logger.debug('Added `%s` to trajectory.' % full_name)<TAB><TAB><TAB><TAB>return instance",
        "target_block": "if it has been constructed somewhere else, otherwise None.:param constructor:A constructor used if instance needs to be constructed. If None the current standardconstructor is chosen.:param args:Additional arguments passed to the constructor:param kwargs:Additional keyword arguments passed to the constructor:return: The new instance\"\"\"root = self._root_instancefull_name = self._make_full_name(parent_node.v_full_name, name)if instance is None:if constructor is None:if type_name == RESULT:constructor = root._standard_resultelif type_name in [PARAMETER, CONFIG, DERIVED_PARAMETER]:constructor = root._standard_parameterelse:constructor = root._standard_leafinstance = root._construct_instance(constructor, full_name, *args, **kwargs)else:instance._rename(full_name)self._set_details_tree_node(parent_node, name, instance)where_dict = self._map_type_to_dict(type_name)full_name = instance._full_nameif full_name in where_dict:raise AttributeError(full_name + ' is already part of trajectory,')if type_name != RESULT and full_name in root._changed_default_parameters:self._logger.info('You have marked parameter %s for change before, so here you go!' %full_name)change_args, change_kwargs = root._changed_default_parameters.pop(full_name)instance.f_set(*change_args, **change_kwargs)where_dict[full_name] = instanceself._add_to_nodes_and_leaves(instance)parent_node._children[name] = instanceparent_node._leaves[name] = instanceif full_name in self._root_instance._explored_parameters:"
    },
    {
        "input_method": "def add_message(<TAB><TAB><TAB>self,<TAB><TAB><TAB>msg_descr,<TAB><TAB><TAB>line=None,<TAB><TAB><TAB>node=None,<TAB><TAB><TAB>args=None,<TAB><TAB><TAB>confidence=UNDEFINED,<TAB><TAB><TAB>col_offset=None,<TAB><TAB>):<TAB><TAB><TAB>\"\"\"Adds a message given by ID or name.<TAB><TAB><TAB><TAB>If provided, the message string is expanded using args.<TAB><TAB><TAB><TAB>AST checkers must provide the node argument (but may optionally<TAB><TAB><TAB>provide line <extra_id_0><TAB><TAB><TAB><TAB>self.add_one_message(<TAB><TAB><TAB><TAB><TAB>message_definition, line, node, args, confidence, col_offset<TAB><TAB><TAB><TAB>)",
        "target_block": "if the line number is different), raw and token checkersmust provide the line argument.\"\"\"message_definitions = self.msgs_store.get_message_definitions(msg_descr)for message_definition in message_definitions:"
    },
    {
        "input_method": "def post(self, request):<TAB><TAB><TAB>\"\"\"Create auth token. Differs from DRF that it always creates new token<TAB><TAB><TAB>but not re-using them.\"\"\"<TAB><TAB><TAB>serializer = self.serializer_class(data=request.data)<TAB><TAB><TAB><extra_id_0> token.key})<TAB><TAB><TAB><TAB>return response.Response(<TAB><TAB><TAB><TAB>serializer.errors, status=status.HTTP_400_BAD_REQUEST)",
        "target_block": "if serializer.is_valid():user = serializer.validated_data['user']signals.user_logged_in.send(type(self), user=user, request=request)token = self.model.objects.create(user=user)token.update_expiry()return response.Response({'token':"
    },
    {
        "input_method": "def cp_single_file(self, pool, source, target, delete_source):<TAB><TAB>'''Copy a single file or a directory by adding a task into queue'''<TAB><TAB><extra_id_0><TAB><TAB>  pool.copy(source, target, delete_source=delete_source)",
        "target_block": "if source[-1] == PATH_SEP:  if self.opt.recursive:basepath = S3URL(source).pathfor f in (f for f in self.s3walk(source) if not f['is_dir']):  pool.copy(f['name'], os.path.join(target, os.path.relpath(S3URL(f['name']).path, basepath)), delete_source=delete_source)  else:message('omitting directory \"%s\".' % source)else:"
    },
    {
        "input_method": "def copy_dir(bucket_name, src_path, dest_path,<TAB><TAB><TAB><TAB> aws_access_key_id=None, aws_secret_access_key=None,<TAB><TAB><TAB><TAB> aws_profile=None,<TAB><TAB><TAB><TAB> surrogate_key=None, cache_control=None,<TAB><TAB><TAB><TAB> surrogate_control=None,<TAB><TAB><TAB><TAB> create_directory_redirect_object=True):<TAB><TAB>\"\"\"Copy objects from one directory in a bucket to another directory in<TAB><TAB>the same bucket.<TAB><TAB><TAB>Object metadata is preserved while copying, with the following exceptions:<TAB><TAB><TAB>- If a new surrogate key is provided it will replace the original one.<TAB><TAB>- If ``cache_control`` and ``surrogate_control`` values are provided they<TAB><TAB>  will replace the old one.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>bucket_name : `str`<TAB><TAB><TAB>Name of an S3 bucket.<TAB><TAB>src_path : `str`<TAB><TAB><TAB>Source directory in the S3 bucket. The ``src_path`` should ideally end<TAB><TAB><TAB>in a trailing `'/'`. E.g. `'dir/dir2/'`.<TAB><TAB>dest_path : `str`<TAB><TAB><TAB>Destination directory in the S3 bucket. The ``dest_path`` should<TAB><TAB><TAB>ideally end in a trailing `'/'`. E.g. `'dir/dir2/'`. The destination<TAB><TAB><TAB>path cannot contain the source path.<TAB><TAB>aws_access_key_id : `str`<TAB><TAB><TAB>The access key for your AWS account. Also set<TAB><TAB><TAB>``aws_secret_access_key``.<TAB><TAB>aws_secret_access_key : `str`<TAB><TAB><TAB>The secret key for your AWS account.<TAB><TAB>aws_profile : `str`, optional<TAB><TAB><TAB>Name of AWS profile in :file:`~/.aws/credentials`. Use this instead<TAB><TAB><TAB>of ``aws_access_key_id`` and ``aws_secret_access_key`` for file-based<TAB><TAB><TAB>credentials.<TAB><TAB>surrogate_key : `str`, optional<TAB><TAB><TAB>The surrogate key to insert in the header of all objects in the<TAB><TAB><TAB>``x-amz-meta-surrogate-key`` field. This key is used to purge<TAB><TAB><TAB>builds from the Fastly CDN when Editions change.<TAB><TAB><TAB>If `None` then no header will be set.<TAB><TAB><TAB>If the object already has a ``x-amz-meta-surrogate-key`` header then<TAB><TAB><TAB>it will be replaced.<TAB><TAB>cache_control : `str`, optional<TAB><TAB><TAB>This sets (and overrides) the ``Cache-Control`` header on the copied<TAB><TAB><TAB>files. The ``Cache-Control`` header specifically dictates how content<TAB><TAB><TAB>is cached by the browser (<extra_id_0> 'true'}<TAB><TAB><TAB>obj.put(Body='',<TAB><TAB><TAB><TAB><TAB>ACL='public-read',<TAB><TAB><TAB><TAB><TAB>Metadata=metadata,<TAB><TAB><TAB><TAB><TAB>CacheControl=cache_control)",
        "target_block": "if ``surrogate_control`` is also set).surrogate_control : `str`, optionalThis sets (and overrides) the ``x-amz-meta-surrogate-control`` headeron the copied files. The ``Surrogate-Control``or ``x-amz-meta-surrogate-control`` header is used in priority byFastly to givern it's caching. This caching policy is *not* passedto the browser.create_directory_redirect_object : `bool`, optionalCreate a directory redirect object for the root directory. Thedirectory redirect object is an empty S3 object named after thedirectory (without a trailing slash) that contains a``x-amz-meta-dir-redirect=true`` HTTP header. LSST the Docs' FastlyVCL is configured to redirect requests for a directory path to thedirectory's ``index.html`` (known as *courtesy redirects*).Raises------ltdconveyor.s3.S3ErrorThrown by any unexpected faults from the S3 API.RuntimeErrorThrown when the source and destination directories are the same.\"\"\"if not src_path.endswith('/'):src_path += '/'if not dest_path.endswith('/'):dest_path += '/'# Ensure the src_path and dest_path don't contain each othercommon_prefix = os.path.commonprefix([src_path, dest_path])if common_prefix == src_path:msg = 'Common prefix {0} is same as source dir {1}'.format(common_prefix, src_path)raise RuntimeError(msg)if common_prefix == dest_path:msg = 'Common prefix {0} is same as dest dir {1}'.format(common_prefix, dest_path)raise RuntimeError(msg)# Delete any existing objects in the destinationdelete_dir(bucket_name, dest_path,   aws_access_key_id, aws_secret_access_key)session = boto3.session.Session(aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,profile_name=aws_profile)s3 = session.resource('s3')bucket = s3.Bucket(bucket_name)# Copy each object from source to destinationfor src_obj in bucket.objects.filter(Prefix=src_path):src_rel_path = os.path.relpath(src_obj.key, start=src_path)dest_key_path = os.path.join(dest_path, src_rel_path)# the src_obj (ObjectSummary) doesn't include headers afaikhead = s3.meta.client.head_object(Bucket=bucket_name,  Key=src_obj.key)metadata = head['Metadata']content_type = head['ContentType']# try to use original Cache-Control header if new one is not setif cache_control is None and 'CacheControl' in head:cache_control = head['CacheControl']if surrogate_control is not None:metadata['surrogate-control'] = surrogate_controlif surrogate_key is not None:metadata['surrogate-key'] = surrogate_keys3.meta.client.copy_object(Bucket=bucket_name,Key=dest_key_path,CopySource={'Bucket': bucket_name, 'Key': src_obj.key},MetadataDirective='REPLACE',Metadata=metadata,ACL='public-read',CacheControl=cache_control,ContentType=content_type)if create_directory_redirect_object:dest_dirname = dest_path.rstrip('/')obj = bucket.Object(dest_dirname)metadata = {'dir-redirect':"
    },
    {
        "input_method": "def get(self, k, wait=False, wait_index=False, timeout='5m'):<TAB><TAB><TAB>\"\"\"Get the value of a given key\"\"\"<TAB><TAB><TAB>k = k.lstrip('/')<TAB><TAB><TAB>url = '{}/{}'.format(self.endpoint, k)<TAB><TAB><TAB>params = {}<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB># Value was empty and wild None appeared<TAB><TAB><TAB><TAB>return \"\"",
        "target_block": "if wait:params['index'] = wait_indexparams['wait'] = timeoutr = requests.get(url, params=params)if r.status_code == 404:raise KeyDoesNotExist(\"Key \" + k + \" does not exist\")if r.status_code != 200:raise KVStoreError('GET returned {}'.format(r.status_code))try:return base64.b64decode(r.json()[0]['Value'])except TypeError as e:"
    },
    {
        "input_method": "def get_model_url_name(model_nfo, page, with_namespace=False):<TAB><TAB>\"\"\"Returns a URL for a given Tree admin page type.\"\"\"<TAB><TAB>prefix = ''<TAB><TAB><extra_id_0>'<TAB><TAB>return ('%s%s_%s' % (prefix, '%s_%s' % model_nfo, page)).lower()",
        "target_block": "if with_namespace:prefix = 'admin:"
    },
    {
        "input_method": "def almost_everything(self, clean_all=False):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Delete almost all discovered files.<TAB><TAB><TAB><TAB>:param clean_all:<TAB><TAB><TAB><TAB>Tell the subsystem <extra_id_0> no cover<TAB><TAB><TAB><TAB>Load(PyFunceble.CURRENT_DIRECTORY)",
        "target_block": "if we have to clean everything instesdof almost everything.:type clean_all: bool\"\"\"# We get the list of file to delete.to_delete = self.file_to_delete()if clean_all:  # pragma: no coverto_delete.extend(self.databases_to_delete())for file in to_delete:# We loop through the list of file to delete.# And we delete the currently read file.File(file).delete()if clean_all:  # pragma:"
    },
    {
        "input_method": "def arcs(self):<TAB><TAB><TAB>\"\"\"Get information about the arcs available in the code.<TAB><TAB><TAB><TAB>Returns a sorted list of line number pairs.  Line numbers have been<TAB><TAB><TAB>normalized to the first line of multiline statements.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>all_arcs = []<TAB><TAB><TAB>for l1, l2 in self.byte_parser._all_arcs():<TAB><TAB><TAB><TAB>fl1 = self.first_line(l1)<TAB><TAB><TAB><TAB>fl2 = self.first_line(l2)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>all_arcs.append((fl1, fl2))<TAB><TAB><TAB>return sorted(all_arcs)",
        "target_block": "if fl1 != fl2:"
    },
    {
        "input_method": "def get_setting(connection, key):<TAB><TAB>\"\"\"Get key from connection or default to settings.\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>return getattr(settings, key)",
        "target_block": "if key in connection.settings_dict:return connection.settings_dict[key]else:"
    },
    {
        "input_method": "def validate(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Verify that the contents of the OpaqueObject are valid.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>TypeError: <extra_id_0><TAB><TAB><TAB><TAB><TAB>position = \"({0} in list)\".format(i)<TAB><TAB><TAB><TAB><TAB>raise TypeError(\"opaque data name {0} must be a string\".format(<TAB><TAB><TAB><TAB><TAB><TAB>position))",
        "target_block": "if the types of any OpaqueObject attributes are invalid.\"\"\"if not isinstance(self.value, bytes):raise TypeError(\"opaque value must be bytes\")elif not isinstance(self.opaque_type, enums.OpaqueDataType):raise TypeError(\"opaque data type must be an OpaqueDataType \"\"enumeration\")name_count = len(self.names)for i in range(name_count):name = self.names[i]if not isinstance(name, six.string_types):"
    },
    {
        "input_method": "def set_attr_from_config_option(self, cp, attr, where, type_=''):<TAB><TAB><TAB>\"\"\"Set an attribute on self <extra_id_0><TAB><TAB><TAB><TAB>method = getattr(cp, 'get'+type_)<TAB><TAB><TAB><TAB>setattr(self, attr, method(section, option))",
        "target_block": "if it exists in the ConfigParser.\"\"\"section, option = where.split(\":\")if cp.has_option(section, option):"
    },
    {
        "input_method": "def put_new_symbol(  # pylint: disable=too-many-arguments<TAB><TAB><TAB>self,<TAB><TAB><TAB>s: sym.Symbol,<TAB><TAB><TAB>binding: Binding,<TAB><TAB><TAB>warn_on_shadowed_name: bool = True,<TAB><TAB><TAB>warn_on_shadowed_var: bool = True,<TAB><TAB><TAB>warn_if_unused: bool = True,<TAB><TAB>):<TAB><TAB><TAB>\"\"\"Add a new symbol to the symbol table.<TAB><TAB><TAB><TAB>This function allows individual warnings to be disabled for one run<TAB><TAB><TAB>by supplying keyword arguments temporarily disabling those warnings.<TAB><TAB><TAB>In certain cases, we do not want to issue warnings again for a<TAB><TAB><TAB>previously checked case, so this is a simple way of disabling these<TAB><TAB><TAB>warnings for those cases.<TAB><TAB><TAB><TAB>If WARN_ON_SHADOWED_NAME compiler option is active and the<TAB><TAB><TAB>warn_on_shadowed_name keyword argument is True, then a warning will be<TAB><TAB><TAB>emitted <extra_id_0><TAB><TAB><TAB><TAB>warn_if_unused = False<TAB><TAB><TAB>st.new_symbol(s, binding, warn_if_unused=warn_if_unused)",
        "target_block": "if a local name is shadowed by another local name. Note thatWARN_ON_SHADOWED_NAME implies WARN_ON_SHADOWED_VAR.If WARN_ON_SHADOWED_VAR compiler option is active and thewarn_on_shadowed_var keyword argument is True, then a warning will beemitted if a named var is shadowed by a local name.\"\"\"st = self.symbol_tableif warn_on_shadowed_name and self.warn_on_shadowed_name:if st.find_symbol(s) is not None:logger.warning(f\"name '{s}' shadows name from outer scope\")if (warn_on_shadowed_name or warn_on_shadowed_var) and self.warn_on_shadowed_var:if self.current_ns.find(s) is not None:logger.warning(f\"name '{s}' shadows def'ed Var from outer scope\")if s.meta is not None and s.meta.entry(SYM_NO_WARN_WHEN_UNUSED_META_KEY, None):"
    },
    {
        "input_method": "def parse_contexts(contexts):<TAB><TAB>\"\"\"<TAB><TAB>Convert a contexts JSON to an Elasticsearch-compatible list of key-value pairs<TAB><TAB>For example, the JSON<TAB><TAB><TAB>{<TAB><TAB>  \"data\": [<TAB><TAB><TAB>{<TAB><TAB><TAB>  \"data\": {<TAB><TAB><TAB><TAB>\"unique\": true<TAB><TAB><TAB>  },<TAB><TAB><TAB>  \"schema\": \"iglu:com.acme/unduplicated/jsonschema/1-0-0\"<TAB><TAB><TAB>},<TAB><TAB><TAB>{<TAB><TAB><TAB>  \"data\": {<TAB><TAB><TAB><TAB>\"value\": 1<TAB><TAB><TAB>  },<TAB><TAB><TAB>  \"schema\": \"iglu:com.acme/duplicated/jsonschema/1-0-0\"<TAB><TAB><TAB>},<TAB><TAB><TAB>{<TAB><TAB><TAB>  \"data\": {<TAB><TAB><TAB><TAB>\"value\": 2<TAB><TAB><TAB>  },<TAB><TAB><TAB>  \"schema\": \"iglu:com.acme/duplicated/jsonschema/1-0-0\"<TAB><TAB><TAB>}<TAB><TAB>  ],<TAB><TAB>  \"schema\": \"iglu:com.snowplowanalytics.snowplow/contexts/jsonschema/1-0-0\"<TAB><TAB>}<TAB><TAB><TAB>would become<TAB><TAB><TAB>[<TAB><TAB>  (\"context_com_acme_duplicated_1\", [{\"value\": 1}, {\"value\": 2}]),<TAB><TAB>  (\"context_com_acme_unduplicated_1\", [{\"unique\": true}])<TAB><TAB>]<TAB><TAB>\"\"\"<TAB><TAB>my_json = json.loads(contexts)<TAB><TAB>data = my_json['data']<TAB><TAB>distinct_contexts = {}<TAB><TAB>for context in data:<TAB><TAB><TAB>schema = fix_schema(\"contexts\", context['schema'])<TAB><TAB><TAB>inner_data = context['data']<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>output.append((key, distinct_contexts[key]))<TAB><TAB>return output",
        "target_block": "if schema not in distinct_contexts:distinct_contexts[schema] = [inner_data]else:distinct_contexts[schema].append(inner_data)output = []for key in distinct_contexts:"
    },
    {
        "input_method": "def transpose(self, *axes):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Return an array with the axes transposed.<TAB><TAB><TAB><TAB>This operation will incur a swap unless the<TAB><TAB><TAB>desiured permutation can be obtained<TAB><TAB><TAB>only by transpoing the keys or the values.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>axes : None, tuple of ints, or n ints<TAB><TAB><TAB><TAB>If None, will reverse axis order.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0>]-split<TAB><TAB><TAB><TAB># perform the swap and the the within key/value permutations<TAB><TAB><TAB>arr = self.swap(swapping_keys, swapping_values-split)<TAB><TAB><TAB>arr = arr.keys.transpose(tuple(p_keys.tolist()))<TAB><TAB><TAB>arr = arr.values.transpose(tuple(p_values.tolist()))<TAB><TAB><TAB><TAB>return arr",
        "target_block": "if len(axes) == 0:p = arange(self.ndim-1, -1, -1)else:p = asarray(argpack(axes))istransposeable(p, range(self.ndim))split = self.split# compute the keys/value axes that need to be swappednew_keys, new_values = p[:split], p[split:]swapping_keys = sort(new_values[new_values < split])swapping_values = sort(new_keys[new_keys >= split])stationary_keys = sort(new_keys[new_keys < split])stationary_values = sort(new_values[new_values >= split])# compute the permutation that the swap causesp_swap = r_[stationary_keys, swapping_values, swapping_keys, stationary_values]# compute the extra permutation (p_x)  on top of this that# needs to happen to get the full permutation desiredp_swap_inv = argsort(p_swap)p_x = p_swap_inv[p]p_keys, p_values = p_x[:split], p_x[split:"
    },
    {
        "input_method": "def __collect_file(self, filename, keep_original=False):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Move or copy single file to artifacts dir<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>dest = self.artifacts_dir + '/' + os.path.basename(filename)<TAB><TAB><TAB>logger.debug(\"Collecting file: %s to %s\", filename, dest)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>shutil.move(filename, self.artifacts_dir)<TAB><TAB><TAB><TAB>os.chmod(dest, 0o644)",
        "target_block": "if not filename or not os.path.exists(filename):logger.warning(\"File not found to collect: %s\", filename)returnif os.path.exists(dest):# FIXME: 3 find a way to store artifacts anywaylogger.warning(\"File already exists: %s\", dest)returnif keep_original:shutil.copy(filename, self.artifacts_dir)else:"
    },
    {
        "input_method": "def standard_package_names():<TAB><TAB>\"\"\"Yield standard module names.\"\"\"<TAB><TAB>for name in standard_paths():<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>continue<TAB><TAB><TAB><TAB>yield name.split('.')[0]",
        "target_block": "if name.startswith('_') or '-' in name:continueif '.' in name and name.rsplit('.')[-1] not in ['so', 'py', 'pyc']:"
    },
    {
        "input_method": "def fftp(wave, npoints=None, indep_min=None, indep_max=None, unwrap=True, rad=True):<TAB><TAB>r\"\"\"<TAB><TAB>Return the phase of the Fast Fourier Transform of a waveform.<TAB><TAB><TAB>:param wave: Waveform<TAB><TAB>:type  wave: :py:class:`peng.eng.Waveform`<TAB><TAB><TAB>:param npoints: Number of points to use in the transform. If **npoints**<TAB><TAB><TAB><TAB><TAB><TAB>is less than the size of the independent variable vector<TAB><TAB><TAB><TAB><TAB><TAB>the waveform is truncated; <extra_id_0><TAB><TAB> * RuntimeError (Argument \\`indep_max\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Argument \\`indep_min\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Argument \\`npoints\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Argument \\`rad\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Argument \\`unwrap\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Argument \\`wave\\` is not valid)<TAB><TAB><TAB> * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`<TAB><TAB>   arguments)<TAB><TAB><TAB> * RuntimeError (Non-uniform sampling)<TAB><TAB><TAB>.. [[[end]]]<TAB><TAB>\"\"\"<TAB><TAB>return phase(fft(wave, npoints, indep_min, indep_max), unwrap=unwrap, rad=rad)",
        "target_block": "if **npoints** is greater thanthe size of the independent variable vector, the waveformis zero-padded:type  npoints: positive integer:param indep_min: Independent vector start point of computation:type  indep_min: integer or float:param indep_max: Independent vector stop point of computation:type  indep_max: integer or float:param unwrap: Flag that indicates whether phase should change phase shifts   to their :code:`2*pi` complement (True) or not (False):type  unwrap: boolean:param rad: Flag that indicates whether phase should be returned in radians(True) or degrees (False):type  rad: boolean:rtype: :py:class:`peng.eng.Waveform`.. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]].. Auto-generated exceptions documentation for.. peng.wave_functions.fftp:raises:"
    },
    {
        "input_method": "def _get_rate(self, currency, date):<TAB><TAB><TAB>\"\"\"Get a rate for a given currency and date.<TAB><TAB><TAB><TAB>:type date: datetime.date<TAB><TAB><TAB><TAB>>>> from datetime import date<TAB><TAB><TAB>>>> c = CurrencyConverter()<TAB><TAB><TAB>>>> c._get_rate('USD', date=date(2014, 3, 28))<TAB><TAB><TAB>1.375...<TAB><TAB><TAB>>>> c._get_rate('BGN', date=date(2010, 11, 21))<TAB><TAB><TAB>Traceback (most recent call last):<TAB><TAB><TAB>RateNotFoundError: BGN has no rate for 2010-11-21<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise RateNotFoundError('{0} has no rate for {1}'.format(currency, date))<TAB><TAB><TAB>return rate",
        "target_block": "if currency == self.ref_currency:return 1.0if date not in self._rates[currency]:first_date, last_date = self.bounds[currency]if not self.fallback_on_wrong_date:raise RateNotFoundError('{0} not in {1} bounds {2}/{3}'.format(date, currency, first_date, last_date))if date < first_date:fallback_date = first_dateelif date > last_date:fallback_date = last_dateelse:raise AssertionError('Should never happen, bug in the code!')if self.verbose:print(r'/!\\ {0} not in {1} bounds {2}/{3}, falling back to {4}'.format(date, currency, first_date, last_date, fallback_date))date = fallback_daterate = self._rates[currency][date]if rate is None:"
    },
    {
        "input_method": "def del_arg(self, name: str) -> None:<TAB><TAB><TAB>\"\"\"Delete all arguments with the given then.\"\"\"<TAB><TAB><TAB>for arg in reversed(self.arguments):<TAB><TAB><TAB><TAB><extra_id_0>]",
        "target_block": "if arg.name.strip(WS) == name.strip(WS):del arg[:"
    },
    {
        "input_method": "def is_subdomain(domain):  # pragma: no cover<TAB><TAB>\"\"\"<TAB><TAB>Check <extra_id_0><TAB><TAB><TAB># * The given domain is not empty nor None.<TAB><TAB><TAB># and<TAB><TAB><TAB># * The given domain is a string.<TAB><TAB><TAB><TAB># We silently load the configuration.<TAB><TAB><TAB>load_config(True)<TAB><TAB><TAB><TAB>return Check(domain).is_subdomain()<TAB><TAB><TAB># We return None, there is nothing to check.<TAB><TAB>return None",
        "target_block": "if the given domain is a subdomain.:param domain: The domain we are checking.:type domain: str:return: The subdomain state.:rtype: bool.. warning::If an empty or a non-string :code:`domain` is given, we return :code:`None`.\"\"\"if domain and isinstance(domain, str):"
    },
    {
        "input_method": "def initialize(self):<TAB><TAB><TAB>\"\"\" Load the view on first load could also load based on session, group, etc.. <TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0> Set initial view properties<TAB><TAB><TAB>self.__class__.view = View(<TAB><TAB><TAB><TAB>company=current_company,<TAB><TAB><TAB><TAB>request=self.request,<TAB><TAB><TAB><TAB>handler=self,<TAB><TAB><TAB>)",
        "target_block": "if self.__class__.view:self.view.handler = selfself.view.request = self.requestreturn#: Load the View class from the dotted view namewith enaml.imports():from views.index import View#:"
    },
    {
        "input_method": "def _should_trace(self, filename, frame):<TAB><TAB><TAB>\"\"\"Decide whether to trace execution in `filename`.<TAB><TAB><TAB><TAB>Calls `_should_trace_with_reason`, and returns just the decision.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>canonical, reason = self._should_trace_with_reason(filename, frame)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>msg = \"Tracing %r\" % (filename,)<TAB><TAB><TAB><TAB>self.debug.write(msg)<TAB><TAB><TAB>return canonical",
        "target_block": "if self.debug.should('trace'):if not canonical:msg = \"Not tracing %r: %s\" % (filename, reason)else:"
    },
    {
        "input_method": "def icosphere(script, radius=1.0, diameter=None, subdivisions=3, color=None):<TAB><TAB>\"\"\"create an icosphere mesh<TAB><TAB><TAB>radius Radius of the sphere<TAB><TAB># subdivisions = Subdivision level; Number of the recursive subdivision of the<TAB><TAB># surface. Default is 3 (a sphere approximation composed by 1280 faces).<TAB><TAB># Admitted values are in the range 0 (an icosahedron) to 8 (a 1.3 MegaTris<TAB><TAB># approximation of a sphere). Formula for number of faces: F=20*4^subdiv<TAB><TAB># color = specify a color name to apply vertex colors to the newly<TAB><TAB># created mesh\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>vert_color.function(script, color=color)<TAB><TAB>return None",
        "target_block": "if diameter is not None:radius = diameter / 2filter_xml = ''.join(['  <filter name=\"Sphere\">\\n','<Param name=\"radius\" ','value=\"%s\" ' % radius,'description=\"Radius\" ','type=\"RichFloat\" ','/>\\n','<Param name=\"subdiv\" ','value=\"%d\" ' % subdivisions,'description=\"Subdiv. Level\" ','type=\"RichInt\" ','/>\\n','  </filter>\\n'])util.write_filter(script, filter_xml)if isinstance(script, FilterScript):script.add_layer('Sphere', change_layer=True)if color is not None:"
    },
    {
        "input_method": "def validate_response_type(self, client_id, response_type, client, request,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   *args, **kwargs):<TAB><TAB><TAB>\"\"\"Ensure client is authorized to use the response type requested.<TAB><TAB><TAB><TAB>It will allow any of the two (`code`, `token`) response types by<TAB><TAB><TAB>default. Implemented `allowed_response_types` for client object<TAB><TAB><TAB>to authorize the request.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return response_type in client.allowed_response_types<TAB><TAB><TAB>return True",
        "target_block": "if response_type not in ('code', 'token'):return Falseif hasattr(client, 'allowed_response_types'):"
    },
    {
        "input_method": "def get_relationship(self, from_object, relation_type):<TAB><TAB><TAB>\"\"\"return a relation ship or None<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>for rel in self.relationships.get(relation_type, ()):<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>return rel<TAB><TAB><TAB>raise KeyError(relation_type)",
        "target_block": "if rel.from_object is from_object:"
    },
    {
        "input_method": "def _init_properties(self):<TAB><TAB><TAB>\"\"\" Loop through the list of Properties,<TAB><TAB><TAB>extract the derived and required properties and do the<TAB><TAB><TAB>appropriate book-keeping<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self._missing = {}<TAB><TAB><TAB>for k, p in self.params.items():<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>p.loader = self.__getattribute__(p.loader)",
        "target_block": "if p.required:self._missing[k] = pif isinstance(p, Derived):if p.loader is None:# Default to using _<param_name>p.loader = self.__getattribute__(\"_%s\" % k)elif isinstance(p.loader, str):"
    },
    {
        "input_method": "def silence_without_namespace(f):<TAB><TAB>\"\"\"Decorator to silence template tags <extra_id_0><TAB><TAB><TAB><TAB>return f(home_label)<TAB><TAB>return wrapped",
        "target_block": "if 'PROJECT_HOME_NAMESPACE' isnot defined in settings.Usage Example:from django import templateregister = template.Library()@register.simple_tag@silence_without_namespacedef a_template_tag(*args):...\"\"\"@wraps(f)def wrapped(label=None):if not home_namespace:return ''if label:return f(label)else:"
    },
    {
        "input_method": "def _event_filter_page_keypress(self, event):<TAB><TAB><TAB>\"\"\" Filter key events for the paging widget to create console-like<TAB><TAB><TAB><TAB>interface.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>key = event.key()<TAB><TAB><TAB>ctrl_down = self._control_key_down(event.modifiers())<TAB><TAB><TAB>alt_down = event.modifiers() & QtCore.Qt.AltModifier<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>QtCore.Qt.Key_PageUp,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>QtCore.Qt.NoModifier)<TAB><TAB><TAB><TAB>QtGui.qApp.sendEvent(self._page_control, new_event)<TAB><TAB><TAB><TAB>return True<TAB><TAB><TAB><TAB>return False",
        "target_block": "if ctrl_down:if key == QtCore.Qt.Key_O:self._control.setFocus()intercept = Trueelif alt_down:if key == QtCore.Qt.Key_Greater:self._page_control.moveCursor(QtGui.QTextCursor.End)intercepted = Trueelif key == QtCore.Qt.Key_Less:self._page_control.moveCursor(QtGui.QTextCursor.Start)intercepted = Trueelif key in (QtCore.Qt.Key_Q, QtCore.Qt.Key_Escape):if self._splitter:self._page_control.hide()self._control.setFocus()else:self.layout().setCurrentWidget(self._control)return Trueelif key in (QtCore.Qt.Key_Enter, QtCore.Qt.Key_Return, QtCore.Qt.Key_Tab):new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,QtCore.Qt.Key_PageDown,QtCore.Qt.NoModifier)QtGui.qApp.sendEvent(self._page_control, new_event)return Trueelif key == QtCore.Qt.Key_Backspace:"
    },
    {
        "input_method": "def verify_request(self, scopes):<TAB><TAB><TAB>\"\"\"Verify current request, get the oauth data.<TAB><TAB><TAB><TAB>If you can't use the ``require_oauth`` decorator, you can fetch<TAB><TAB><TAB>the data in your request body::<TAB><TAB><TAB><TAB><TAB>def your_handler():<TAB><TAB><TAB><TAB><TAB>valid, req = oauth.verify_request(['email'])<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>return jsonify(user=req.user)<TAB><TAB><TAB><TAB><TAB>return jsonify(status='error')<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>uri, http_method, body, headers = extract_params()<TAB><TAB><TAB>return self.server.verify_request(<TAB><TAB><TAB><TAB>uri, http_method, body, headers, scopes<TAB><TAB><TAB>)",
        "target_block": "if valid:"
    },
    {
        "input_method": "def _format_table(fmt, headers, rows, colwidths, colaligns):<TAB><TAB>\"\"\"Produce a plain-text representation of the table.\"\"\"<TAB><TAB>lines = []<TAB><TAB>hidden = fmt.with_header_hide <extra_id_0><TAB><TAB><TAB>lines.append(_build_line(colwidths, pad, *fmt.linebelow))<TAB><TAB><TAB>return \"\\n\".join(lines)",
        "target_block": "if headers else fmt.without_header_hidepad = fmt.paddingheaderrow = fmt.headerrow if fmt.headerrow else fmt.datarowif fmt.lineabove and \"lineabove\" not in hidden:lines.append(_build_line(colwidths, pad, *fmt.lineabove))if headers:lines.append(_build_row(headers, pad, *headerrow))if fmt.linebelowheader and \"linebelowheader\" not in hidden:begin, fill, sep, end = fmt.linebelowheaderif fmt.usecolons:segs = [_line_segment_with_colons(fmt.linebelowheader, a, w + 2 * pad)for w, a in zip(colwidths, colaligns)]lines.append(_build_row(segs, 0, begin, sep, end))else:lines.append(_build_line(colwidths, pad, *fmt.linebelowheader))if rows and fmt.linebetweenrows and \"linebetweenrows\" not in hidden:# initial rows with a line belowfor row in rows[:-1]:lines.append(_build_row(row, pad, *fmt.datarow))lines.append(_build_line(colwidths, pad, *fmt.linebetweenrows))# the last row without a line belowlines.append(_build_row(rows[-1], pad, *fmt.datarow))else:for row in rows:lines.append(_build_row(row, pad, *fmt.datarow))if fmt.linebelow and \"linebelow\" not in hidden:"
    },
    {
        "input_method": "def _create_directory(cls, directory, loop=False):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Creates the given directory <extra_id_0><TAB><TAB><TAB><TAB># The given directory does not exist.<TAB><TAB><TAB><TAB><TAB># We update the permission.<TAB><TAB><TAB><TAB># (Only if we are under Travis CI.)<TAB><TAB><TAB><TAB>AutoSave.travis_permissions()<TAB><TAB><TAB><TAB><TAB># We create the directory.<TAB><TAB><TAB><TAB>PyFunceble.mkdir(directory)<TAB><TAB><TAB><TAB><TAB># We update the permission.<TAB><TAB><TAB><TAB># (Only if we are under Travis CI.)<TAB><TAB><TAB><TAB>AutoSave.travis_permissions()",
        "target_block": "if it does not exists.:param directory: The directory to create.:type directory: str:param loop: Tell us if we are in the creation loop or not.:type loop: bool\"\"\"if not loop and PyFunceble.directory_separator in directory:# * We are not in the loop.# and# * The directory separator in the given directory.# We split the directories separator.splited_directory = directory.split(PyFunceble.directory_separator)# We initiate a variable which will save the full path to create.full_path_to_create = \"\"for single_directory in splited_directory:# We loop through each directory.# We append the currently read directory to the full path.full_path_to_create += single_directory + PyFunceble.directory_separator# And we create the directory if it does not exist.cls._create_directory(full_path_to_create, True)if not PyFunceble.path.isdir(directory):"
    },
    {
        "input_method": "def _parse_assembly(self, assembly_file):<TAB><TAB><TAB>\"\"\"Parse an assembly file in fasta format.<TAB><TAB><TAB><TAB>This is a Fasta parsing method that populates the<TAB><TAB><TAB>:py:attr:`Assembly.contigs` attribute with data for each contig in the<TAB><TAB><TAB> assembly.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>assembly_file : str<TAB><TAB><TAB><TAB>Path to the assembly fasta file.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>with open(assembly_file) as fh:<TAB><TAB><TAB><TAB><TAB>header = None<TAB><TAB><TAB><TAB>logger.debug(\"Starting iteration of assembly file: {}\".format(<TAB><TAB><TAB><TAB><TAB>assembly_file))<TAB><TAB><TAB><TAB><TAB>for line in fh:<TAB><TAB><TAB><TAB><TAB><TAB># Skip empty lines<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB># Add sequence string for the current contig<TAB><TAB><TAB><TAB><TAB><TAB>self.contigs[header].append(line.strip())<TAB><TAB><TAB><TAB><TAB># After populating the contigs dictionary, convert the values<TAB><TAB><TAB><TAB># list into a string sequence<TAB><TAB><TAB><TAB>self.contigs = OrderedDict(<TAB><TAB><TAB><TAB><TAB>(header, \"\".join(seq)) for header, seq in self.contigs.items())",
        "target_block": "if not line.strip():continueif line.startswith(\">\"):# Add contig header to contig dictionaryheader = line[1:].strip()self.contigs[header] = []else:"
    },
    {
        "input_method": "def draw(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Redraws the menu and refreshes the screen. Should be called whenever something changes that needs to be redrawn.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>self.screen.border(0)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>top_row = 6 + len(self.items) - screen_rows<TAB><TAB><TAB><TAB>self.screen.refresh(top_row, 0, 0, 0, screen_rows - 1, screen_cols - 1)",
        "target_block": "if self.title is not None:self.screen.addstr(2, 2, self.title, curses.A_STANDOUT)if self.subtitle is not None:self.screen.addstr(4, 2, self.subtitle, curses.A_BOLD)for index, item in enumerate(self.items):if self.current_option == index:text_style = self.highlightelse:text_style = self.normalself.screen.addstr(5 + index, 4, item.show(index), text_style)screen_rows, screen_cols = CursesMenu.stdscr.getmaxyx()top_row = 0if 6 + len(self.items) > screen_rows:if screen_rows + self.current_option < 6 + len(self.items):top_row = self.current_optionelse:"
    },
    {
        "input_method": "def finalize():<TAB>  \"\"\"A function that should be called after parsing all Gin config files.<TAB><TAB>  Calling this function allows registered \"finalize hooks\" to inspect (and<TAB>  potentially modify) the Gin config, to provide additional functionality. Hooks<TAB>  should not modify the configuration object they receive directly; instead,<TAB>  they should return a dictionary mapping Gin binding keys to (new or updated)<TAB>  values. This way, all hooks see the config as originally parsed.<TAB><TAB>  Raises:<TAB><TAB>RuntimeError: If the config is already locked.<TAB><TAB>ValueError: If two or more hooks attempt to modify or introduce bindings for<TAB><TAB>  the same key. Since it is difficult to control the order in which hooks<TAB><TAB>  are registered, allowing this could yield unpredictable behavior.<TAB>  \"\"\"<TAB>  <extra_id_0><TAB><TAB>bind_parameter(pbk, value)<TAB><TAB>  _set_config_is_locked(True)",
        "target_block": "if config_is_locked():raise RuntimeError('Finalize called twice (config already locked).')  bindings = {}  for hook in _FINALIZE_HOOKS:new_bindings = hook(_CONFIG)if new_bindings is not None:  for key, value in six.iteritems(new_bindings):pbk = ParsedBindingKey(key)if pbk in bindings:  err_str = 'Received conflicting updates when running {}.'  raise ValueError(err_str.format(hook))bindings[pbk] = value  for pbk, value in six.iteritems(bindings):"
    },
    {
        "input_method": "def _check_consider_merging_isinstance(self, node):<TAB><TAB><TAB>\"\"\"Check isinstance calls which can be merged together.\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>names = sorted(name for name in class_names)<TAB><TAB><TAB><TAB>self.add_message(<TAB><TAB><TAB><TAB><TAB>\"consider-merging-isinstance\",<TAB><TAB><TAB><TAB><TAB>node=node,<TAB><TAB><TAB><TAB><TAB>args=(duplicated_name, \", \".join(names)),<TAB><TAB><TAB><TAB>)",
        "target_block": "if node.op != \"or\":returnfirst_args = self._duplicated_isinstance_types(node)for duplicated_name, class_names in first_args.items():"
    },
    {
        "input_method": "def dict_from_file(filename, key_type=str):<TAB><TAB>\"\"\"Load a text file and parse the content as a dict.<TAB><TAB><TAB>Each line of the text file will be two or more columns splited by<TAB><TAB>whitespaces or tabs. The first column will be parsed as dict keys, and<TAB><TAB>the following columns will be parsed as dict values.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>filename(str): Filename.<TAB><TAB><TAB>key_type(type): Type of the dict's keys. str is user by default and<TAB><TAB><TAB><TAB>type conversion will be performed <extra_id_0>] if len(items) > 2 else items[1]<TAB><TAB><TAB><TAB>mapping[key] = val<TAB><TAB>return mapping",
        "target_block": "if specified.Returns:dict: The parsed contents.\"\"\"mapping = {}with open(filename, 'r') as f:for line in f:items = line.rstrip('\\n').split()assert len(items) >= 2key = key_type(items[0])val = items[1:"
    },
    {
        "input_method": "def cluster_status(self, cluster_identifier):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Return status of a cluster<TAB><TAB><TAB><TAB>:param cluster_identifier: unique identifier of a cluster<TAB><TAB><TAB>:type cluster_identifier: str<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>conn = self.get_conn()<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>response = conn.describe_clusters(<TAB><TAB><TAB><TAB><TAB>ClusterIdentifier=cluster_identifier)['Clusters']<TAB><TAB><TAB><TAB>return response[0]['ClusterStatus'] <extra_id_0><TAB><TAB><TAB><TAB>return 'cluster_not_found'",
        "target_block": "if response else Noneexcept conn.exceptions.ClusterNotFoundFault:"
    },
    {
        "input_method": "def activate_async(fn, _engine):<TAB><TAB>\"\"\"<TAB><TAB>Async version of activate decorator<TAB><TAB><TAB>Arguments:<TAB><TAB><TAB>fn (function): function that be wrapped by decorator.<TAB><TAB><TAB>_engine (Engine): pook engine instance<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>function: decorator wrapper function.<TAB><TAB>\"\"\"<TAB><TAB>@coroutine<TAB><TAB>@functools.wraps(fn)<TAB><TAB>def wrapper(*args, **kw):<TAB><TAB><TAB>_engine.activate()<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>_engine.disable()<TAB><TAB><TAB>return wrapper",
        "target_block": "if iscoroutinefunction(fn):yield from fn(*args, **kw)  # noqaelse:fn(*args, **kw)finally:"
    },
    {
        "input_method": "def save_to_file_like(self, flo, format=None, **kwargs):<TAB><TAB><TAB>\"\"\" Save the object to a given file like object in the given format.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>format = self.format <extra_id_0><TAB><TAB><TAB><TAB>raise ValueError(\"Unknown format '%s'.\" % format)<TAB><TAB><TAB>save(flo, **kwargs)",
        "target_block": "if format is None else formatsave = getattr(self, \"save_%s\" % format, None)if save is None:"
    },
    {
        "input_method": "def list_endpoint(self, endpoint, query=None):<TAB><TAB>'''An endpoint is required here to list files within. Optionally, we can<TAB><TAB>   take a path relative to the endpoint root.<TAB><TAB><TAB>   Parameters<TAB><TAB>   ==========<TAB><TAB>   endpoint: a single endpoint ID or an endpoint id and relative path.<TAB><TAB><TAB><TAB><TAB> If no path is provided, we use '', which defaults to scratch.<TAB><TAB><TAB>   query: <extra_id_0><TAB><TAB><TAB>bot.info('No content was found at the selected endpoint.')<TAB><TAB>return rows",
        "target_block": "if defined, limit files to those that have query match'''if not hasattr(self, 'transfer_client'):self._init_transfer_client()# Separate endpoint id from the desired pathendpoint, path = self._parse_endpoint_name(endpoint)# Get a list of files at endpoint, under specific pathtry:result = self.transfer_client.operation_ls(endpoint, path=path)except TransferAPIError as err:# Tell the user what went wrong!bot.custom(prefix='ERROR', message=err, color='RED')sys.exit(1)rows = []for filey in result:# Highlight container contenders with purplename = filey['name']if query is None or query in name:if name.endswith('img'):name = bot.addColor('PURPLE',name)rows.append([filey['type'], filey['permissions'], str(filey['size']), name ])   if len(rows) > 0:rows = [[\"type\",\"[perm]\",\"[size]\",\"[name]\"]] + rowsbot.custom(prefix=\"Endpoint Listing %s\" %path, message='', color=\"CYAN\")bot.table(rows)else:"
    },
    {
        "input_method": "def xml_find(xpath):<TAB><TAB>\"\"\"Find a XML element via xpath.\"\"\"<TAB><TAB>def xpath_find(value):<TAB><TAB><TAB>validate(ET.iselement, value)<TAB><TAB><TAB>value = value.find(xpath)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise ValueError(\"XPath '{0}' did not return an element\".format(xpath))<TAB><TAB><TAB><TAB>return validate(ET.iselement, value)<TAB><TAB><TAB>return transform(xpath_find)",
        "target_block": "if value is None:"
    },
    {
        "input_method": "def ranking_metric(df, method, pos, neg, classes, ascending):<TAB><TAB>\"\"\"The main function to rank an expression table.<TAB><TAB><TAB>   :param df:<TAB>  gene_expression DataFrame.<TAB><TAB>   :param method:  The method used to calculate a correlation or ranking. Default: 'log2_ratio_of_classes'.<TAB><TAB><TAB><TAB><TAB><TAB>   Others methods are:<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   1. 'signal_to_noise'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  You must have at least three samples for each phenotype to use this metric.<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  The larger the signal-to-noise ratio, the larger the differences of the means (scaled by the standard deviations);<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  that is, the more distinct the gene expression is in each phenotype and the more the gene acts as a \u201cclass marker.\u201d<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   2. 't_test'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Uses the difference of means scaled by the standard deviation and number of samples.<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Note: You must have at least three samples for each phenotype to use this metric.<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  The larger the tTest ratio, the more distinct the gene expression is in each phenotype<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  and the more the gene acts as a \u201cclass marker.\u201d<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   3. 'ratio_of_classes' (also referred to as fold change).<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Uses the ratio of class means to calculate fold change for natural scale data.<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   4. 'diff_of_classes'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Uses the difference of class means to calculate fold change for natural scale data<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   5. 'log2_ratio_of_classes'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Uses the log2 ratio of class means to calculate fold change for natural scale data.<TAB><TAB><TAB><TAB><TAB><TAB><TAB>  This is the recommended statistic for calculating fold change for log scale data.<TAB><TAB><TAB><TAB>   :param str pos: one of labels of phenotype's names.<TAB><TAB>   :param str neg: one of labels of phenotype's names.<TAB><TAB>   :param list classes:  a list of phenotype labels, to specify which column of dataframe belongs to what category of phenotype.<TAB><TAB>   :param bool ascending:  bool or list of bool. Sort ascending vs. descending.<TAB><TAB>   :return:<TAB><TAB><TAB><TAB><TAB>returns a pd.Series of correlation to class of each variable. Gene_name is index, and value is rankings.<TAB><TAB><TAB><TAB><TAB>visit here for more docs: http://software.broadinstitute.org/gsea/doc/GSEAUserGuideFrame.html<TAB><TAB>\"\"\"<TAB><TAB><TAB># exclude any zero stds.<TAB><TAB>df_mean = df.groupby(by=classes, axis=1).mean()<TAB><TAB>df_std =  df.groupby(by=classes, axis=1).std()<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>logging.error(\"Please provide correct method name!!!\")<TAB><TAB><TAB>sys.exit(0)<TAB><TAB>ser = ser.sort_values(ascending=ascending)<TAB><TAB><TAB>return ser",
        "target_block": "if method == 'signal_to_noise':ser = (df_mean[pos] - df_mean[neg])/(df_std[pos] + df_std[neg])elif method == 't_test':ser = (df_mean[pos] - df_mean[neg])/ np.sqrt(df_std[pos]**2/len(df_std)+df_std[neg]**2/len(df_std) )elif method == 'ratio_of_classes':ser = df_mean[pos] / df_mean[neg]elif method == 'diff_of_classes':ser  = df_mean[pos] - df_mean[neg]elif method == 'log2_ratio_of_classes':ser  =  np.log2(df_mean[pos] / df_mean[neg])else:"
    },
    {
        "input_method": "def expire(self, age):<TAB><TAB><TAB>\"\"\"Expire any items in the cache older than `age` seconds\"\"\"<TAB><TAB><TAB>now = time.time()<TAB><TAB><TAB>cache = self._acquire_cache()<TAB><TAB><TAB><TAB><TAB><TAB>expired = [k for k, v in cache.items() <extra_id_0><TAB><TAB><TAB><TAB><TAB>del self.store[k]<TAB><TAB><TAB><TAB>self._write_cache(cache)",
        "target_block": "if (now - v[0]) > age]for k in expired:if k in cache:del cache[k]if k in self.store:"
    },
    {
        "input_method": "def _get_entry(self, entry, entry_tree):<TAB><TAB><TAB>'''Helper function for retrieving a particular entry from the prefix trees'''<TAB><TAB><TAB>for e in entry_tree[entry.filename]:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>return e",
        "target_block": "if entry == e:"
    },
    {
        "input_method": "def debug_application(self, environ, start_response):<TAB><TAB><TAB>\"\"\"Run the application and conserve the traceback frames.\"\"\"<TAB><TAB><TAB>app_iter = None<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>app_iter = self.app(environ, start_response)<TAB><TAB><TAB><TAB>for item in app_iter:<TAB><TAB><TAB><TAB><TAB>yield item<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>yield traceback.render_full(evalex=self.evalex,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>secret=self.secret) \\<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   .encode('utf-8', 'replace')<TAB><TAB><TAB><TAB><TAB>traceback.log(environ['wsgi.errors'])",
        "target_block": "if hasattr(app_iter, 'close'):app_iter.close()except Exception:if hasattr(app_iter, 'close'):app_iter.close()traceback = get_current_traceback(skip=1, show_hidden_frames=  self.show_hidden_frames,  ignore_system_exceptions=True)for frame in traceback.frames:self.frames[frame.id] = frameself.tracebacks[traceback.id] = tracebacktry:start_response('500 INTERNAL SERVER ERROR', [('Content-Type', 'text/html; charset=utf-8'),# Disable Chrome's XSS protection, the debug# output can cause false-positives.('X-XSS-Protection', '0'),])except Exception:# if we end up here there has been output but an error# occurred.  in that situation we can do nothing fancy any# more, better log something into the error log and fall# back gracefully.environ['wsgi.errors'].write('Debugging middleware caught exception in streamed ''response at a point where response headers were already ''sent.\\n')else:"
    },
    {
        "input_method": "def extract(self, log, basis, name, function=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>'Extract' a log into the components of a striplog.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>log (array_like). A log or other 1D data.<TAB><TAB><TAB><TAB>basis (array_like). The depths or elevations of the log samples.<TAB><TAB><TAB><TAB>name (str). The name of the attribute to store in the components.<TAB><TAB><TAB><TAB>function (function). A function that takes an array as the only<TAB><TAB><TAB><TAB><TAB>input, and returns whatever you want to store in the 'name'<TAB><TAB><TAB><TAB><TAB>attribute of the primary component.<TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>None. The function works on the striplog in place.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB># Build a dict of {index: [log values]} to keep track.<TAB><TAB><TAB>intervals = {}<TAB><TAB><TAB>previous_ix = -1<TAB><TAB><TAB>for i, z in enumerate(basis):<TAB><TAB><TAB><TAB>ix = self.read_at(z, index=True)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>f = function or utils.null<TAB><TAB><TAB><TAB>d = f(np.array(data))<TAB><TAB><TAB><TAB>self[ix].data[name] = d<TAB><TAB><TAB><TAB>return None",
        "target_block": "if ix is None:continueif ix == previous_ix:intervals[ix].append(log[i])else:intervals[ix] = [log[i]]previous_ix = ix# Set the requested attribute in the primary comp of each interval.for ix, data in intervals.items():"
    },
    {
        "input_method": "def emphasis(obj, align=True):<TAB><TAB>''' Clearer data printing '''<TAB><TAB><extra_id_0><TAB><TAB><TAB>return obj<TAB><TAB>return pretty_msg",
        "target_block": "if isinstance(obj, dict):if align:pretty_msg = os.linesep.join([\"%25s: %s\" % (k, obj[k]) for k in sorted(obj.keys())])else:pretty_msg = json.dumps(obj, indent=4, sort_keys=True)else:"
    },
    {
        "input_method": "def cov(self, x, y=None, binby=[], limits=None, shape=default_shape, selection=False, delay=False, progress=None):<TAB><TAB><TAB>\"\"\"Calculate the covariance matrix for x and y or more expressions, possibly on a grid defined by binby.<TAB><TAB><TAB><TAB>Either x and y are expressions, e.g:<TAB><TAB><TAB><TAB>>>> df.cov(\"x\", \"y\")<TAB><TAB><TAB><TAB>Or only the x argument is given with a list of expressions, e,g.:<TAB><TAB><TAB><TAB>>>> df.cov([\"x, \"y, \"z\"])<TAB><TAB><TAB><TAB>Example:<TAB><TAB><TAB><TAB>>>> df.cov(\"x\", \"y\")<TAB><TAB><TAB>array([[ 53.54521742,  -3.8123135 ],<TAB><TAB><TAB>[ -3.8123135 ,  60.62257881]])<TAB><TAB><TAB>>>> df.cov([\"x\", \"y\", \"z\"])<TAB><TAB><TAB>array([[ 53.54521742,  -3.8123135 ,  -0.98260511],<TAB><TAB><TAB>[ -3.8123135 ,  60.62257881,   1.21381057],<TAB><TAB><TAB>[ -0.98260511,   1.21381057,  25.55517638]])<TAB><TAB><TAB><TAB>>>> df.cov(\"x\", \"y\", binby=\"E\", shape=2)<TAB><TAB><TAB>array([[[  9.74852878e+00,  -3.02004780e-02],<TAB><TAB><TAB>[ -3.02004780e-02,   9.99288215e+00]],<TAB><TAB><TAB>[[  8.43996546e+01,  -6.51984181e+00],<TAB><TAB><TAB>[ -6.51984181e+00,   9.68938284e+01]]])<TAB><TAB><TAB><TAB><TAB>:param x: {expression}<TAB><TAB><TAB>:param y: {expression_single}<TAB><TAB><TAB>:param binby: {binby}<TAB><TAB><TAB>:param limits: {limits}<TAB><TAB><TAB>:param shape: {shape}<TAB><TAB><TAB>:param selection: {selection}<TAB><TAB><TAB>:param delay: {delay}<TAB><TAB><TAB>:return: {return_stat_scalar}, the last dimensions are of shape (2,2)<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>selection = _ensure_strings_from_expressions(selection)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>moments2 = sums / counts<TAB><TAB><TAB><TAB>cov_matrix = moments2 - meansxy<TAB><TAB><TAB><TAB>return cov_matrix<TAB><TAB><TAB>progressbar = vaex.utils.progressbars(progress)<TAB><TAB><TAB>values = calculate(expressions, limits)<TAB><TAB><TAB>cov_matrix = finish(values)<TAB><TAB><TAB>return self._delay(delay, cov_matrix)",
        "target_block": "if y is None:if not _issequence(x):raise ValueError(\"if y argument is not given, x is expected to be sequence, not %r\", x)expressions = xelse:expressions = [x, y]N = len(expressions)binby = _ensure_list(binby)shape = _expand_shape(shape, len(binby))progressbar = vaex.utils.progressbars(progress)limits = self.limits(binby, limits, selection=selection, delay=True)@delayeddef calculate(expressions, limits):# print('limits', limits)task = tasks.TaskStatistic(self, binby, shape, limits, weights=expressions, op=tasks.OP_COV, selection=selection)self.executor.schedule(task)progressbar.add_task(task, \"covariance values for %r\" % expressions)return task@delayeddef finish(values):N = len(expressions)counts = values[..., :N]sums = values[..., N:2 * N]with np.errstate(divide='ignore', invalid='ignore'):means = sums / counts# matrix of means * means.Tmeansxy = means[..., None] * means[..., None, :]counts = values[..., 2 * N:2 * N + N**2]sums = values[..., 2 * N + N**2:]shape = counts.shape[:-1] + (N, N)counts = counts.reshape(shape)sums = sums.reshape(shape)with np.errstate(divide='ignore', invalid='ignore'):"
    },
    {
        "input_method": "def __recv(self, size=4096):<TAB><TAB><TAB>\"\"\"Reads data from the socket.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>NNTPError: When connection times out or read from socket fails.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>data = self.socket.recv(size)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise NNTPError(\"Failed to read from socket\")<TAB><TAB><TAB>self.__buffer.write(data)",
        "target_block": "if not data:"
    },
    {
        "input_method": "def sweep(self, min_freq, max_freq, bins, repeats, runs=0, time_limit=0, overlap=0,<TAB><TAB><TAB><TAB>  fft_window='hann', fft_overlap=0.5, crop=False, log_scale=True, remove_dc=False, detrend=None, lnb_lo=0,<TAB><TAB><TAB><TAB>  tune_delay=0, reset_stream=False, base_buffer_size=0, max_buffer_size=0, max_threads=0, max_queue_size=0):<TAB><TAB><TAB>\"\"\"Sweep spectrum using frequency hopping\"\"\"<TAB><TAB><TAB>self.setup(<TAB><TAB><TAB><TAB>bins, repeats, base_buffer_size, max_buffer_size,<TAB><TAB><TAB><TAB>fft_window=fft_window, fft_overlap=fft_overlap, crop_factor=overlap <extra_id_0>.3f} s'.format(t_stop - t_start))",
        "target_block": "if crop else 0,log_scale=log_scale, remove_dc=remove_dc, detrend=detrend, lnb_lo=lnb_lo, tune_delay=tune_delay,reset_stream=reset_stream, max_threads=max_threads, max_queue_size=max_queue_size)try:freq_list = self.freq_plan(min_freq - lnb_lo, max_freq - lnb_lo, bins, overlap)t_start = time.time()run = 0while not _shutdown and (runs == 0 or run < runs):run += 1t_run_start = time.time()logger.debug('Run: {}'.format(run))for freq in freq_list:# Tune to new frequency, acquire samples and compute Power Spectral Densitypsd_future, acq_time_start, acq_time_stop = self.psd(freq)# Write PSD to stdout (in another thread)self._writer.write_async(psd_future, acq_time_start, acq_time_stop, len(self._buffer) * self._buffer_repeats)if _shutdown:break# Write end of measurement marker (in another thread)write_next_future = self._writer.write_next_async()t_run = time.time()logger.debug('  Total run time: {:.3f} s'.format(t_run - t_run_start))# End measurement if time limit is exceededif time_limit and (time.time() - t_start) >= time_limit:logger.info('Time limit of {} s exceeded, completed {} runs'.format(time_limit, run))break# Wait for last write to be finishedwrite_next_future.result()# Debug thread pool queueslogging.debug('Number of USB buffer overflow errors: {}'.format(self.device.buffer_overflow_count))logging.debug('PSD worker threads: {}'.format(self._psd._executor._max_workers))logging.debug('Max. PSD queue size: {} / {}'.format(self._psd._executor.max_queue_size_reached,self._psd._executor.max_queue_size))logging.debug('Writer worker threads: {}'.format(self._writer._executor._max_workers))logging.debug('Max. Writer queue size: {} / {}'.format(self._writer._executor.max_queue_size_reached,   self._writer._executor.max_queue_size))finally:# Shutdown SDRself.stop()t_stop = time.time()logger.info('Total time: {:"
    },
    {
        "input_method": "def create_archive(self):<TAB><TAB><TAB>\"\"\"Create a new archive.<TAB><TAB><TAB><TAB>The method creates in the filesystem a brand new archive with<TAB><TAB><TAB>a random SHA1 as its name. The first byte of the hashcode will<TAB><TAB><TAB>be the name of the subdirectory; the remaining bytes, the<TAB><TAB><TAB>archive name.<TAB><TAB><TAB><TAB>:returns: a new `Archive` object<TAB><TAB><TAB><TAB>:raises ArchiveManagerError: when an error occurs creating the<TAB><TAB><TAB><TAB>new archive<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>hashcode = uuid.uuid4().hex<TAB><TAB><TAB>archive_dir = os.path.join(self.dirpath, hashcode[0:2])<TAB><TAB><TAB>archive_name = hashcode[2:] + self.STORAGE_EXT<TAB><TAB><TAB>archive_path = os.path.join(archive_dir, archive_name)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise ArchiveManagerError(cause=str(e))<TAB><TAB><TAB><TAB>return archive",
        "target_block": "if not os.path.exists(archive_dir):os.makedirs(archive_dir)try:archive = Archive.create(archive_path)except ArchiveError as e:"
    },
    {
        "input_method": "def start_event_loop_qt4(app=None):<TAB><TAB>\"\"\"Start the qt4 event loop in a consistent manner.\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>app._in_event_loop = True",
        "target_block": "if app is None:app = get_app_qt4([''])if not is_event_loop_running_qt4(app):app._in_event_loop = Trueapp.exec_()app._in_event_loop = Falseelse:"
    },
    {
        "input_method": "def run_cmake(arg=\"\"):<TAB><TAB>\"\"\"<TAB><TAB>Forcing to run cmake<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>print \"Error while running cmake\"<TAB><TAB><TAB>print \"run 'setup.py build --help' for build options\"<TAB><TAB><TAB>print \"You may also try editing the settings in CMakeLists.txt file and re-running setup\"<TAB><TAB><TAB>sys.exit(-1)",
        "target_block": "if ds.find_executable('cmake') is None:print \"CMake  is required to build zql\"print \"Please install cmake version >= 2.8 and re-run setup\"sys.exit(-1)print \"Configuring zql build with CMake.... \"cmake_args = argtry:build_dir = op.join(op.split(__file__)[0], 'build')dd.mkpath(build_dir)os.chdir(\"build\")ds.spawn(['cmake', '..'] + cmake_args.split())ds.spawn(['make', 'clean'])ds.spawn(['make'])os.chdir(\"..\")except ds.DistutilsExecError:"
    },
    {
        "input_method": "def inverse(self):<TAB><TAB><TAB>\"\"\"Invert this circuit.<TAB><TAB><TAB><TAB>This is done by recursively inverting all gates.<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>QuantumCircuit: the inverted circuit<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>QiskitError: <extra_id_0><TAB><TAB><TAB><TAB>inverse_circ.data.append((inst.inverse(), qargs, cargs))<TAB><TAB><TAB>return inverse_circ",
        "target_block": "if the circuit cannot be inverted.\"\"\"inverse_circ = self.copy(name=self.name+'_dg')inverse_circ.data = []for inst, qargs, cargs in reversed(self.data):"
    },
    {
        "input_method": "def add_parameters(self, traj):<TAB><TAB><TAB>\"\"\"Adds parameters for a network simulation.<TAB><TAB><TAB><TAB>Calls :func:`~pypet.brian2.network.NetworkComponent.add_parameters` for all components,<TAB><TAB><TAB>analyser, and the network runner (in this order).<TAB><TAB><TAB><TAB>:param traj:  Trajectory container<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self._logger.info('Adding Parameters of Components')<TAB><TAB><TAB><TAB>for component in self.components:<TAB><TAB><TAB><TAB>component.add_parameters(traj)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>analyser.add_parameters(traj)<TAB><TAB><TAB><TAB>self._logger.info('Adding Parameters of Runner')<TAB><TAB><TAB><TAB>self.network_runner.add_parameters(traj)",
        "target_block": "if self.analysers:self._logger.info('Adding Parameters of Analysers')for analyser in self.analysers:"
    },
    {
        "input_method": "def parse_xdot_data(self, data):<TAB><TAB><TAB>\"\"\" Parses xdot data and returns the associated components. \"\"\"<TAB><TAB><TAB><TAB>parser = self.parser<TAB>#<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return []",
        "target_block": "if pyparsing_version >= \"1.2\":#parser.parseWithTabs()if data:return parser.parseString(data)else:"
    },
    {
        "input_method": "def annotate(self, **annotations):<TAB><TAB><TAB>\"\"\"Custom version of the standard annotate function<TAB><TAB><TAB>that allows using field names as annotated fields.<TAB><TAB><TAB><TAB>Normally, the annotate function doesn't allow you<TAB><TAB><TAB>to use the name of an existing field on the model<TAB><TAB><TAB>as the alias name. This version of the function does<TAB><TAB><TAB>allow that.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB>fields = {<TAB><TAB><TAB><TAB>field.name: field<TAB><TAB><TAB><TAB>for field in self.model._meta.get_fields()<TAB><TAB><TAB>}<TAB><TAB><TAB><TAB># temporarily rename the fields that have the same<TAB><TAB><TAB># name as a field name, we'll rename them back after<TAB><TAB><TAB># the function in the base class ran<TAB><TAB><TAB>new_annotations = {}<TAB><TAB><TAB>renames = {}<TAB><TAB><TAB>for name, value in annotations.items():<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>new_annotations[name] = value<TAB><TAB><TAB><TAB># run the base class's annotate function<TAB><TAB><TAB>result = super().annotate(**new_annotations)<TAB><TAB><TAB><TAB># rename the annotations back to as specified<TAB><TAB><TAB>result.rename_annotations(**renames)<TAB><TAB><TAB>return result",
        "target_block": "if name in fields:new_name = '%s_new' % namenew_annotations[new_name] = valuerenames[new_name] = nameelse:"
    },
    {
        "input_method": "def prepare(self):<TAB><TAB><TAB>\"\"\" Load the view on first load \"\"\"<TAB><TAB><TAB><extra_id_0> Set initial view properties<TAB><TAB><TAB>self.__class__.view = View(<TAB><TAB><TAB><TAB>site=self.site,<TAB><TAB><TAB><TAB>page=self.page,<TAB><TAB><TAB><TAB>request=self.request,<TAB><TAB><TAB>)",
        "target_block": "if self.__class__.view:return#: Load the View class from the dotted view namewith enaml.imports():View = pydoc.locate(self.page.view)assert View, \"Failed to import View: {}\".format(self.page.view)#:"
    },
    {
        "input_method": "def _stylesheet_param_dict(paramsDict, kwargsDict):<TAB><TAB>\"\"\"Return a copy of paramsDict, updated with kwargsDict entries, wrapped as<TAB><TAB>stylesheet arguments.<TAB><TAB>kwargsDict entries with a value of None are ignored.<TAB><TAB>\"\"\"<TAB><TAB># beware of changing mutable default arg<TAB><TAB>paramsDict = dict(paramsDict)<TAB><TAB>for k, v in kwargsDict.items():<TAB><TAB><TAB><extra_id_0> # None values do not override<TAB><TAB><TAB><TAB>paramsDict[k] = v<TAB><TAB>paramsDict = stylesheet_params(**paramsDict)<TAB><TAB>return paramsDict",
        "target_block": "if v is not None:"
    },
    {
        "input_method": "def _handle_execute_reply(self, msg):<TAB><TAB><TAB>\"\"\" Handles replies for code execution.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self.log.debug(\"execute: %s\", msg.get('content', ''))<TAB><TAB><TAB>msg_id = msg['parent_header']['msg_id']<TAB><TAB><TAB>info = self._request_info['execute'].get(msg_id)<TAB><TAB><TAB># unset reading flag, because <extra_id_0><TAB><TAB><TAB><TAB>super(FrontendWidget, self)._handle_execute_reply(msg)",
        "target_block": "if execute finished, raw_input can't# still be pending.self._reading = Falseif info and info.kind == 'user' and not self._hidden:# Make sure that all output from the SUB channel has been processed# before writing a new prompt.self.kernel_manager.sub_channel.flush()# Reset the ANSI style information to prevent bad text in stdout# from messing up our colors. We're not a true terminal so we're# allowed to do this.if self.ansi_codes:self._ansi_processor.reset_sgr()content = msg['content']status = content['status']if status == 'ok':self._process_execute_ok(msg)elif status == 'error':self._process_execute_error(msg)elif status == 'aborted':self._process_execute_abort(msg)self._show_interpreter_prompt_for_reply(msg)self.executed.emit(msg)self._request_info['execute'].pop(msg_id)elif info and info.kind == 'silent_exec_callback' and not self._hidden:self._handle_exec_callback(msg)self._request_info['execute'].pop(msg_id)else:"
    },
    {
        "input_method": "def visit_ExceptHandler(self, node):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Process except blocks.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>name = self.get_except_handler_name(node)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>super(LoggingVisitor, self).generic_visit(node)<TAB><TAB><TAB><TAB>return<TAB><TAB><TAB><TAB>self.current_except_names.append(name)<TAB><TAB><TAB>super(LoggingVisitor, self).generic_visit(node)<TAB><TAB><TAB>self.current_except_names.pop()",
        "target_block": "if not name:"
    },
    {
        "input_method": "def convert_tree(beautiful_soup_tree, makeelement=None):<TAB><TAB>\"\"\"Convert a BeautifulSoup tree to a list of Element trees.<TAB><TAB><TAB>Returns a list instead of a single root Element to support<TAB><TAB>HTML-like soup with more than one root element.<TAB><TAB><TAB>You can pass a different Element factory through the `makeelement`<TAB><TAB>keyword.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>root.remove(child)<TAB><TAB>return children",
        "target_block": "if makeelement is None:makeelement = html.html_parser.makeelementroot = _convert_tree(beautiful_soup_tree, makeelement)children = root.getchildren()for child in children:"
    },
    {
        "input_method": "def _convert_trace_id_to_string(self, trace_id, trace_id_high=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Converts the provided traceId hex value with optional high bits<TAB><TAB><TAB>to a string.<TAB><TAB><TAB><TAB>:param trace_id: the value of the trace ID<TAB><TAB><TAB>:type trace_id: int<TAB><TAB><TAB>:param trace_id_high: the high bits of the trace ID<TAB><TAB><TAB>:type trace_id: int<TAB><TAB><TAB>:returns: trace_id_high + trace_id as a string<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>result = bytearray(32)<TAB><TAB><TAB><TAB>self._write_hex_long(result, 0, trace_id_high)<TAB><TAB><TAB><TAB>self._write_hex_long(result, 16, trace_id)<TAB><TAB><TAB><TAB>return result.decode(\"utf8\")<TAB><TAB><TAB><TAB>result = bytearray(16)<TAB><TAB><TAB>self._write_hex_long(result, 0, trace_id)<TAB><TAB><TAB>return result.decode(\"utf8\")",
        "target_block": "if trace_id_high is not None:"
    },
    {
        "input_method": "def is_met(self, user, filtered=False):<TAB><TAB><TAB>''' Returns True <extra_id_0><TAB><TAB><TAB><TAB>return True  # Why query again?<TAB><TAB><TAB><TAB>return self.passes_filter(user)",
        "target_block": "if this flag condition is met, otherwise returnsFalse. It determines if the condition is met by calling pre_filterwith a queryset containing only self.condition. '''if filtered:"
    },
    {
        "input_method": "def seek_next_line(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Seek next line relative to the current file position.<TAB><TAB><TAB><TAB>:return: Position of the line or -1 <extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>data_where += 1<TAB><TAB><TAB><TAB><TAB>offset += data_len<TAB><TAB><TAB><TAB>self.file.seek(where + offset)<TAB><TAB><TAB><TAB>return -1",
        "target_block": "if next line was not found.\"\"\"where = self.file.tell()offset = 0while True:data_len, data = self.read(self.read_size)data_where = 0if not data_len:break# Consider the following example: Foo\\r | \\nBar where \" | \" denotes current position,# 'Foo\\r' is the read part and '\\nBar' is the remaining part.# We should completely consume terminator \"\\r\\n\" by reading one extra byte.if b'\\r\\n' in self.LINE_TERMINATORS and data[-1] == b'\\r'[0]:terminator_where = self.file.tell()terminator_len, terminator_data = self.read(1)if terminator_len and terminator_data[0] == b'\\n'[0]:data_len += 1data += b'\\n'else:self.file.seek(terminator_where)while data_where < data_len:terminator = self.prefix_line_terminator(data[data_where:])if terminator:self.file.seek(where + offset + data_where + len(terminator))return self.file.tell()else:"
    },
    {
        "input_method": "def get_date(datetime, time_format=None):<TAB><TAB>\"\"\"<TAB><TAB>Return a datetime oject from a string, with optional time format.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>datetime : str<TAB><TAB><TAB>Date-time as string in any sensible format.<TAB><TAB>time_format : datetime str (optional)<TAB><TAB><TAB>String describing the datetime format. If missing uses<TAB><TAB><TAB>dateutil.parser to guess time format.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>t = dt.datetime.strftime(datetime, time_format)<TAB><TAB>return t",
        "target_block": "if time_format is None:t = du.parser.parse(datetime)else:"
    },
    {
        "input_method": "def join_lines(string, strip=Strip.BOTH):<TAB><TAB>'''<TAB><TAB>Join strings together and strip whitespace in between <extra_id_0><TAB><TAB><TAB><TAB>line = line.lstrip()<TAB><TAB><TAB><TAB>lines.append(line)<TAB><TAB><TAB>return ''.join(lines)",
        "target_block": "if needed'''lines = []for line in string.splitlines():if strip & Strip.RIGHT:line = line.rstrip()if strip & Strip.LEFT:"
    },
    {
        "input_method": "def append_stream(self, streamid, stream, encoding=None):<TAB><TAB><TAB>\"\"\"append a file to search for similarities\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>pass",
        "target_block": "if encoding is None:readlines = stream.readlineselse:readlines = decoding_stream(stream, encoding).readlinestry:self.linesets.append(LineSet(streamid,readlines(),self.ignore_comments,self.ignore_docstrings,self.ignore_imports,))except UnicodeDecodeError:"
    },
    {
        "input_method": "def _helpful_failure(method):<TAB><TAB>\"\"\"<TAB><TAB>Decorator for eval_ that prints a helpful error message<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>prettyval = \"<%s instance>\" % (type(val).__name__)<TAB><TAB><TAB><TAB><TAB>msg = \"{0}\\n\\n\\tEncountered when evaluating {1}{2}\".format(<TAB><TAB><TAB><TAB><TAB>inst, prettyval, self)<TAB><TAB><TAB><TAB><TAB>new_exc = exc_cls(msg)<TAB><TAB><TAB><TAB>new_exc._RERAISE = True<TAB><TAB><TAB><TAB>Q.__debug_info__ = QDebug(self, self, val, val)<TAB><TAB><TAB><TAB><TAB>six.reraise(exc_cls, new_exc, tb)<TAB><TAB><TAB>return wrapper",
        "target_block": "if an exception is generated in a Q expression\"\"\"@wraps(method)def wrapper(self, val):try:return method(self, val)except:exc_cls, inst, tb = sys.exc_info()if hasattr(inst, '_RERAISE'):_, expr, _, inner_val = Q.__debug_info__Q.__debug_info__ = QDebug(self, expr, val, inner_val)raiseif issubclass(exc_cls, KeyError):  # Overrides formattingexc_cls = QKeyError# Show val, unless it's too longprettyval = repr(val)if len(prettyval) > 150:"
    },
    {
        "input_method": "def _call_tip(self):<TAB><TAB><TAB>\"\"\" Shows a call tip, <extra_id_0><TAB><TAB><TAB><TAB>return False<TAB><TAB><TAB><TAB># Send the metadata request to the kernel<TAB><TAB><TAB>name = '.'.join(context)<TAB><TAB><TAB>msg_id = self.kernel_manager.shell_channel.object_info(name)<TAB><TAB><TAB>pos = self._get_cursor().position()<TAB><TAB><TAB>self._request_info['call_tip'] = self._CallTipRequest(msg_id, pos)<TAB><TAB><TAB>return True",
        "target_block": "if appropriate, at the current cursor location.\"\"\"# Decide if it makes sense to show a call tipif not self.enable_calltips:return Falsecursor = self._get_cursor()cursor.movePosition(QtGui.QTextCursor.Left)if cursor.document().characterAt(cursor.position()) != '(':return Falsecontext = self._get_context(cursor)if not context:"
    },
    {
        "input_method": "def date_map(doc, datemap_list, time_format=None):<TAB><TAB><TAB>'''<TAB><TAB><TAB>For all the datetime fields in \"datemap\" find that key in doc and map the datetime object to<TAB><TAB><TAB>a strftime string. This pprint and others will print out readable datetimes.<TAB><TAB><TAB>'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>doc=CursorFormatter.date_map_field(doc, i, time_format=time_format)<TAB><TAB><TAB>return doc",
        "target_block": "if datemap_list:for i in datemap_list:if isinstance(i, datetime):"
    },
    {
        "input_method": "def _hoist_operands(self, operands, pred):<TAB><TAB>\"\"\"Flattens a list of optree operands based on a pred.<TAB><TAB><TAB>This is used to convert concatenation([x, concatenation[y, ...]]) (or alternation) to<TAB><TAB>concatenation([x, y, ...]).<TAB><TAB>\"\"\"<TAB><TAB>hopper = list(operands)<TAB><TAB>new_operands = []<TAB><TAB>while hopper:<TAB><TAB>  target = hopper.pop(0)<TAB><TAB>  <extra_id_0><TAB><TAB><TAB>new_operands.append(target)<TAB><TAB>return new_operands",
        "target_block": "if pred(target):hopper = list(target.operands) + hopper  else:"
    },
    {
        "input_method": "def new(self, *args, **kwargs):<TAB><TAB><TAB>'''<TAB><TAB><TAB>Create and return a new instance.<TAB><TAB><TAB>'''<TAB><TAB><TAB>inst = self.clazz()<TAB><TAB><TAB>self.storage.append(inst)<TAB><TAB><TAB><TAB><TAB><TAB># set all attributes with an initial default value<TAB><TAB><TAB>referential_attributes = dict()<TAB><TAB><TAB>for name, ty in self.attributes:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>logger.warning('unable to assign %s to %s', name, inst)<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>return inst",
        "target_block": "if name not in self.referential_attributes:value = self.default_value(ty)setattr(inst, name, value)# set all positional argumentsfor attr, value in zip(self.attributes, args):name, ty = attrif name not in self.referential_attributes:setattr(inst, name, value)else:referential_attributes[name] = value# set all named argumentsfor name, value in kwargs.items():if name not in self.referential_attributes:setattr(inst, name, value)else:referential_attributes[name] = valueif not referential_attributes:return inst# batch relate referential attributes for link in self.links.values():if set(link.key_map.values()) - set(referential_attributes.keys()):continue kwargs = dict()for key, value in link.key_map.items():kwargs[key] = referential_attributes[value]if not kwargs:continuefor other_inst in link.to_metaclass.query(kwargs):relate(other_inst, inst, link.rel_id, link.phrase)for name, value in referential_attributes.items():if getattr(inst, name) != value:"
    },
    {
        "input_method": "def _fitch_state(self, node, pos):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Determine the Fitch profile for a single character of the node's sequence.<TAB><TAB><TAB>The profile is essentially the intersection between the children's<TAB><TAB><TAB>profiles or, <extra_id_0><TAB><TAB><TAB><TAB>state = np.concatenate([k.state[pos] for k in node.clades])<TAB><TAB><TAB>return state",
        "target_block": "if the former is empty, the union of the profiles.Parameters---------- node : PhyloTree.Clade:Internal node which the profiles are to be determined pos : intPosition in the node's sequence which the profiles shouldbe determinedf for.Returns------- state : numpy.arrayFitch profile for the character at position pos of the given node.\"\"\"state = self._fitch_intersect([k.state[pos] for k in node.clades])if len(state) == 0:"
    },
    {
        "input_method": "def explicit_rel_links(self, rels=('homepage', 'download')):<TAB><TAB><TAB>\"\"\"Yields all links with the given relations\"\"\"<TAB><TAB><TAB>rels = set(rels)<TAB><TAB><TAB><TAB>for anchor in self.parsed.findall(\".//a\"):<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>href = anchor.get(\"href\")<TAB><TAB><TAB><TAB><TAB><TAB>url = self.clean_link(<TAB><TAB><TAB><TAB><TAB><TAB><TAB>urllib_parse.urljoin(self.base_url, href)<TAB><TAB><TAB><TAB><TAB><TAB>)<TAB><TAB><TAB><TAB><TAB><TAB>yield Link(url, self, trusted=False)",
        "target_block": "if anchor.get(\"rel\") and anchor.get(\"href\"):found_rels = set(anchor.get(\"rel\").split())# Determine the intersection between what rels were found and#   what rels were being looked forif found_rels & rels:"
    },
    {
        "input_method": "def heartbeat_callback(self, session=None):<TAB><TAB><TAB>\"\"\"Self destruct task <extra_id_0><TAB><TAB><TAB><TAB>self.log.warning(<TAB><TAB><TAB><TAB><TAB>\"State of this instance has been externally set to %s. \"<TAB><TAB><TAB><TAB><TAB>\"Taking the poison pill.\",<TAB><TAB><TAB><TAB><TAB>ti.state<TAB><TAB><TAB><TAB>)<TAB><TAB><TAB><TAB>self.task_runner.terminate()<TAB><TAB><TAB><TAB>self.terminating = True",
        "target_block": "if state has been moved away from running externally\"\"\"if self.terminating:# ensure termination if processes are created laterself.task_runner.terminate()returnself.task_instance.refresh_from_db()ti = self.task_instancefqdn = get_hostname()same_hostname = fqdn == ti.hostnamesame_process = ti.pid == os.getpid()if ti.state == State.RUNNING:if not same_hostname:self.log.warning(\"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\", ti.hostname, fqdn)raise AirflowException(\"Hostname of job runner does not match\")elif not same_process:current_pid = os.getpid()self.log.warning(\"Recorded pid %s does not match \" \"the current pid %s\", ti.pid, current_pid)raise AirflowException(\"PID of job runner does not match\")elif (self.task_runner.return_code() is None andhasattr(self.task_runner, 'process')):"
    },
    {
        "input_method": "def validate_url(url):<TAB><TAB>\"\"\"validate a url for zeromq\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB># only validate tcp urls currently<TAB><TAB><TAB>pass<TAB><TAB><TAB><TAB>return True",
        "target_block": "if not isinstance(url, basestring):raise TypeError(\"url must be a string, not %r\"%type(url))url = url.lower()proto_addr = url.split('://')assert len(proto_addr) == 2, 'Invalid url: %r'%urlproto, addr = proto_addrassert proto in ['tcp','pgm','epgm','ipc','inproc'], \"Invalid protocol: %r\"%proto# domain pattern adapted from http://www.regexlib.com/REDetails.aspx?regexp_id=391# author: Remi Sabourinpat = re.compile(r'^([\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?\\.)*[\\w\\d]([\\w\\d\\-]{0,61}[\\w\\d])?$')if proto == 'tcp':lis = addr.split(':')assert len(lis) == 2, 'Invalid url: %r'%urladdr,s_port = listry:port = int(s_port)except ValueError:raise AssertionError(\"Invalid port %r in url: %r\"%(port, url))assert addr == '*' or pat.match(addr) is not None, 'Invalid url: %r'%urlelse:"
    },
    {
        "input_method": "def check_conflicts(self, dist):<TAB><TAB><TAB>\"\"\"Verify that there are no conflicting \"old-style\" packages\"\"\"<TAB><TAB><TAB><TAB>return dist<TAB> # XXX temporarily disable until new strategy is stable<TAB><TAB><TAB>from imp import find_module, get_suffixes<TAB><TAB><TAB>from glob import glob<TAB><TAB><TAB><TAB>blockers = []<TAB><TAB><TAB>names = dict.fromkeys(dist._get_metadata('top_level.txt')) # XXX private attr<TAB><TAB><TAB><TAB>exts = {'.pyc':1, '.pyo':1}<TAB> # get_suffixes() might leave one out<TAB><TAB><TAB>for ext,mode,typ in get_suffixes():<TAB><TAB><TAB><TAB>exts[ext] = 1<TAB><TAB><TAB><TAB>for path,files in expand_paths([self.install_dir]+self.all_site_dirs):<TAB><TAB><TAB><TAB>for filename in files:<TAB><TAB><TAB><TAB><TAB>base,ext = os.path.splitext(filename)<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.found_conflicts(dist, blockers)<TAB><TAB><TAB><TAB>return dist",
        "target_block": "if base in names:if not ext:# no extension, check for packagetry:f, filename, descr = find_module(base, [path])except ImportError:continueelse:if f: f.close()if filename not in blockers:blockers.append(filename)elif ext in exts and base!='site':  # XXX ughblockers.append(os.path.join(path,filename))if blockers:"
    },
    {
        "input_method": "def _all_cut_string(string, max_length, logger):<TAB><TAB><TAB>\"\"\"Cuts string data to the maximum length allowed in a pytables column<TAB><TAB><TAB><extra_id_0>max_length - 3] + '...'.encode('utf-8')<TAB><TAB><TAB><TAB>return string",
        "target_block": "if string is too long.:param string: String to be cut:param max_length: Maximum allowed string length:param logger: Logger where messages about truncating should be written:return: String, cut if too long\"\"\"if len(string) > max_length:logger.debug('The string `%s` was too long I truncated it to' ' %d characters' % (string, max_length))string = string[0:"
    },
    {
        "input_method": "def indent(instr,nspaces=4, ntabs=0, flatten=False):<TAB><TAB>\"\"\"Indent a string a given number of spaces or tabstops.<TAB><TAB><TAB>indent(str,nspaces=4,ntabs=0) -> indent str by ntabs+nspaces.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB><TAB>instr : basestring<TAB><TAB><TAB>The string to be indented.<TAB><TAB>nspaces : int (default: 4)<TAB><TAB><TAB>The number of spaces to be indented.<TAB><TAB>ntabs : int (default: 0)<TAB><TAB><TAB>The number of tabs to be indented.<TAB><TAB>flatten : bool (default: False)<TAB><TAB><TAB>Whether to scrub existing indentation.  If True, all lines will be<TAB><TAB><TAB>aligned to the same indentation.  If False, existing indentation will<TAB><TAB><TAB>be strictly increased.<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB><TAB>str|unicode : string indented by ntabs and nspaces.<TAB><TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>return outstr",
        "target_block": "if instr is None:returnind = '\\t'*ntabs+' '*nspacesif flatten:pat = re.compile(r'^\\s*', re.MULTILINE)else:pat = re.compile(r'^', re.MULTILINE)outstr = re.sub(pat, ind, instr)if outstr.endswith(os.linesep+ind):return outstr[:-len(ind)]else:"
    },
    {
        "input_method": "def lookup_signame(num):<TAB><TAB>\"\"\"Find the corresponding signal name for 'num'. Return None<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return signame<TAB><TAB><TAB>pass<TAB><TAB># Something went wrong. Should have returned above<TAB><TAB>return None",
        "target_block": "if 'num' is invalid.\"\"\"signames = signal.__dict__num = abs(num)for signame in list(signames.keys()):if signame.startswith('SIG') and signames[signame] == num:"
    },
    {
        "input_method": "def receive_nak_renewing(self, pkt):<TAB><TAB><TAB>\"\"\"Receive NAK in RENEWING state.\"\"\"<TAB><TAB><TAB>logger.debug(\"C3.1. Received NAK?, in RENEWING state.\")<TAB><TAB><TAB><extra_id_0> T. Received NAK, in RENEWING state, \"<TAB><TAB><TAB><TAB><TAB><TAB><TAB> \" raise INIT.\")<TAB><TAB><TAB><TAB>raise self.INIT()",
        "target_block": "if self.process_received_nak(pkt):logger.debug(\"C3.1:"
    },
    {
        "input_method": "def available_products(cls, user, category=None, products=None):<TAB><TAB><TAB>''' Returns a list of all of the products that are available per<TAB><TAB><TAB>flag conditions from the given categories. '''<TAB><TAB><TAB><extra_id_0> product.order)<TAB><TAB><TAB><TAB>return out",
        "target_block": "if category is None and products is None:raise ValueError(\"You must provide products or a category\")if category is not None:all_products = inventory.Product.objects.filter(category=category)all_products = all_products.select_related(\"category\")else:all_products = []if products is not None:all_products = set(itertools.chain(all_products, products))category_remainders = CategoryController.user_remainders(user)product_remainders = ProductController.user_remainders(user)passed_limits = set(productfor product in all_productsif category_remainders[product.category.id] > 0if product_remainders[product.id] > 0)failed_and_messages = FlagController.test_flags(user, products=passed_limits)failed_conditions = set(i[0] for i in failed_and_messages)out = list(passed_limits - failed_conditions)out.sort(key=lambda product:"
    },
    {
        "input_method": "def _call_extractor(extractor, text):<TAB>  \"\"\"This method calls an extractor on some text.<TAB><TAB>  If extractor is just a string, it is passed as the first value to _get_terminal. Otherwise it is<TAB>  treated as a callable and text is passed directly to it.<TAB><TAB>  This makes it so you can have a shorthand of terminal(val) <-> val.<TAB>  \"\"\"<TAB>  <extra_id_0><TAB><TAB>return extractor(text)",
        "target_block": "if isinstance(extractor, str):return _get_terminal(extractor, text)  else:"
    },
    {
        "input_method": "def _fetch_from_archive(self, method, args):<TAB><TAB><TAB>\"\"\"Fetch data from the archive<TAB><TAB><TAB><TAB>:param method: the name of the command to execute<TAB><TAB><TAB>:param args: the arguments required by the command<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise data<TAB><TAB><TAB><TAB>return data",
        "target_block": "if not self.archive:raise ArchiveError(cause=\"Archive not provided\")data = self.archive.retrieve(method, args, None)if isinstance(data, nntplib.NNTPTemporaryError):"
    },
    {
        "input_method": "async def parse_character_results(soup):<TAB><TAB>\"\"\"<TAB><TAB>Parse a page of character results.<TAB><TAB><TAB>:param soup: The BS4 class object<TAB><TAB>:return: Returns a list of dictionaries containing a name, gender and list of dictionaries containing a game name/id pair<TAB><TAB><TAB><TAB> for games they appeared in.<TAB><TAB>\"\"\"<TAB><TAB>soup = list(soup.find_all('table', class_='stripe')[0].children)[1:]<TAB><TAB>characters = []<TAB><TAB>for item in soup:<TAB><TAB><TAB>temp_c = {'gender': None, 'name': None, 'games': {}}<TAB><TAB><TAB>temp_c['gender'] = item.abbr.get('title')<TAB><TAB><TAB>temp_c['name'] = list(item.children)[1].a.string<TAB><TAB><TAB>temp_c['games'] = []<TAB><TAB><TAB>for game in list(list(list(item.children)[1].children)[1].children):<TAB><TAB><TAB><TAB><extra_id_0> game.get('href').split('/')[1]})<TAB><TAB><TAB>characters.append(temp_c)<TAB><TAB><TAB>del temp_c<TAB><TAB>return characters",
        "target_block": "if isinstance(game, NavigableString):continuetemp_c['games'].append({'name': game.string, 'id':"
    },
    {
        "input_method": "def merge_las(*las_files):<TAB><TAB>\"\"\" Merges multiple las files into one<TAB><TAB><TAB>merged = merge_las(las_1, las_2)<TAB><TAB>merged = merge_las([las_1, las_2, las_3])<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>las_files: Iterable of LasData or LasData<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>pylas.lasdatas.base.LasBase<TAB><TAB><TAB>The result of the merging<TAB><TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>slc = slice(offset, offset + len(las.points))<TAB><TAB><TAB>merged.points[slc] = las.points<TAB><TAB><TAB>merged_x[slc] = las.x<TAB><TAB><TAB>merged_y[slc] = las.y<TAB><TAB><TAB>merged_z[slc] = las.z<TAB><TAB><TAB>merged['point_source_id'][slc] = i<TAB><TAB><TAB>offset += len(las.points)<TAB><TAB><TAB>merged.x = merged_x<TAB><TAB>merged.y = merged_y<TAB><TAB>merged.z = merged_z<TAB><TAB><TAB>return merged",
        "target_block": "if len(las_files) == 1:las_files = las_files[0]if not las_files:raise ValueError(\"No files to merge\")if not utils.files_have_same_dtype(las_files):raise ValueError(\"All files must have the same point format\")header = las_files[0].headernum_pts_merged = sum(len(las.points) for las in las_files)# scaled x, y, z have to be set manually# to be sure to have a good offset in the headermerged = create_from_header(header)# TODO extra dimensions should be manged better herefor dim_name, dim_type in las_files[0].points_data.point_format.extra_dims:merged.add_extra_dim(dim_name, dim_type)merged.points = np.zeros(num_pts_merged, merged.points.dtype)merged_x = np.zeros(num_pts_merged, np.float64)merged_y = np.zeros(num_pts_merged, np.float64)merged_z = np.zeros(num_pts_merged, np.float64)offset = 0for i, las in enumerate(las_files, start=1):"
    },
    {
        "input_method": "def parse_json(data, name=\"JSON\", exception=PluginError, schema=None):<TAB><TAB>\"\"\"Wrapper around json.loads.<TAB><TAB><TAB>Wraps errors in custom exception with a snippet of the data in the message.<TAB><TAB>\"\"\"<TAB><TAB>try:<TAB><TAB><TAB>json_data = json.loads(data)<TAB><TAB>except ValueError as err:<TAB><TAB><TAB>snippet = repr(data)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>json_data = schema.validate(json_data, name=name, exception=exception)<TAB><TAB><TAB>return json_data",
        "target_block": "if len(snippet) > 35:snippet = snippet[:35] + \" ...\"else:snippet = dataraise exception(\"Unable to parse {0}: {1} ({2})\".format(name, err, snippet))if schema:"
    },
    {
        "input_method": "def verify_cert(signature_chain_url: str) -> Optional[crypto.X509]:<TAB><TAB>\"\"\"Conducts series of Alexa SSL certificate verifications against Amazon Alexa requirements.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.<TAB><TAB>Returns:<TAB><TAB><TAB>result: Amazon certificate <extra_id_0><TAB><TAB><TAB>log.error(f'Certificates chain verification for ({signature_chain_url}) certificate failed')<TAB><TAB><TAB>result = (sc_url_verification and expired_verification and sans_verification and chain_verification)<TAB><TAB><TAB>return amazon_cert if result else None",
        "target_block": "if verification was successful, None if not.\"\"\"try:certs_chain_get = requests.get(signature_chain_url)except requests.exceptions.ConnectionError as e:log.error(f'Amazon signature chain get error: {e}')return Nonecerts_chain_txt = certs_chain_get.textcerts_chain = extract_certs(certs_chain_txt)amazon_cert: crypto.X509 = certs_chain.pop(0)# verify signature chain urlsc_url_verification = verify_sc_url(signature_chain_url)if not sc_url_verification:log.error(f'Amazon signature url {signature_chain_url} was not verified')# verify not expiredexpired_verification = not amazon_cert.has_expired()if not expired_verification:log.error(f'Amazon certificate ({signature_chain_url}) expired')# verify subject alternative namessans_verification = verify_sans(amazon_cert)if not sans_verification:log.error(f'Subject alternative names verification for ({signature_chain_url}) certificate failed')# verify certs chainchain_verification = verify_certs_chain(certs_chain, amazon_cert)if not chain_verification:"
    },
    {
        "input_method": "def connectPacked(srcPacked, dstInterface, exclude=None):<TAB><TAB>\"\"\"<TAB><TAB>Connect 1D vector signal to this structuralized interface<TAB><TAB><TAB>:param packedSrc: vector which should be connected<TAB><TAB>:param dstInterface: structuralized interface where should<TAB><TAB><TAB>packedSrc be connected to<TAB><TAB>:param exclude: sub interfaces of self which should be excluded<TAB><TAB>\"\"\"<TAB><TAB>offset = 0<TAB><TAB>connections = []<TAB><TAB>for i in reversed(list(walkPhysInterfaces(dstInterface))):<TAB><TAB><TAB><extra_id_0> offset]<TAB><TAB><TAB><TAB>offset += w<TAB><TAB><TAB>connections.append(sig(s))<TAB><TAB><TAB>return connections",
        "target_block": "if exclude is not None and i in exclude:continuesig = i._sigt = sig._dtypeif t == BIT:s = srcPacked[offset]offset += 1else:w = t.bit_length()s = srcPacked[(w + offset):"
    },
    {
        "input_method": "def _log_end_transaction(self, start_time, response):<TAB><TAB><TAB>\"\"\"Log response from an API request.\"\"\"<TAB><TAB><TAB><extra_id_0> %s\\n\" % response.headers[\"Content-Type\"]<TAB><TAB><TAB>msg += response.text<TAB><TAB><TAB>self._log_message(msg + \"\\n\\n\")",
        "target_block": "if not self._is_logging: returnelapsed_time = int((time.time() - start_time) * 1000)msg = \"<<< HTTP %d %s   (%d ms)\\n\" % (response.status_code, response.reason, elapsed_time)if \"Content-Type\" in response.headers:msg += \"Content-Type:"
    },
    {
        "input_method": "def _select_file(user_id, api_path, fields, limit):<TAB><TAB>\"\"\"<TAB><TAB>Return a SELECT statement that returns the latest N versions of a file.<TAB><TAB>\"\"\"<TAB><TAB>query = select(fields).where(<TAB><TAB><TAB>_file_where(user_id, api_path),<TAB><TAB>).order_by(<TAB><TAB><TAB>_file_creation_order(),<TAB><TAB>)<TAB><TAB><extra_id_0><TAB><TAB><TAB>query = query.limit(limit)<TAB><TAB><TAB>return query",
        "target_block": "if limit is not None:"
    },
    {
        "input_method": "def timeout_requesting(self):<TAB><TAB><TAB>\"\"\"Timeout requesting in REQUESTING state.<TAB><TAB><TAB><TAB>Not specifiyed in [:rfc:`7844`]<TAB><TAB><TAB><TAB>[:rfc:`2131#section-3.1`]::<TAB><TAB><TAB><TAB><TAB>might retransmit the<TAB><TAB><TAB><TAB>DHCPREQUEST message four times, for a total delay of 60 seconds<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>logger.debug(\"C3.2: T. In %s, timeout receiving response to request, \",<TAB><TAB><TAB><TAB><TAB><TAB> self.current_state)<TAB><TAB><TAB><extra_id_0> F. Maximum number of REQUESTs retries not reached,\"<TAB><TAB><TAB><TAB><TAB><TAB> \"raise REQUESTING.\")<TAB><TAB><TAB>raise self.REQUESTING()",
        "target_block": "if self.discover_requests >= MAX_ATTEMPTS_REQUEST:logger.debug('C2.3: T. Maximum number %s of REQUESTs ' 'reached, already sent %s, raise ERROR.', MAX_ATTEMPTS_REQUEST, self.disover_requests)raise self.ERROR()logger.debug(\"C2.3:"
    },
    {
        "input_method": "def validate_sceneInfo(self):<TAB><TAB><TAB>\"\"\"Check scene name and whether remote file exists. Raises<TAB><TAB><TAB>WrongSceneNameError <extra_id_0> Prefix of %s (%s) is invalid'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  % (self.sceneInfo.name, self.sceneInfo.prefix))",
        "target_block": "if the scene name is wrong.\"\"\"if self.sceneInfo.prefix not in self.__satellitesMap:raise WrongSceneNameError('USGS Downloader:"
    },
    {
        "input_method": "def value(self):<TAB><TAB><TAB>\"\"\"Return the current value.<TAB><TAB><TAB><TAB>This first checks <extra_id_0><TAB><TAB><TAB><TAB><TAB>msg = \"Loader must return variable of type %s or None, got %s\" % (self.__dict__['dtype'], type(val))<TAB><TAB><TAB><TAB><TAB>raise TypeError(msg)<TAB><TAB><TAB>return self.__value__",
        "target_block": "if the value is cached (i.e., if`self.__value__` is not None)If it is not cached then it invokes the `loader` function tocompute the value, and caches the computed value\"\"\"if self.__value__ is None:try: loader = self.__dict__['loader']except KeyError:raise AttributeError(\"Loader is not defined\")# Try to run the loader.# Don't catch expections here, let the Model class figure it outval = loader()# Try to set the valuetry:self.set_value(val)except TypeError:"
    },
    {
        "input_method": "def update_dict_recursive(editable_dict: dict, editing_dict: dict) -> None:<TAB><TAB>\"\"\"Updates dict recursively<TAB><TAB><TAB>You need to use this function to update dictionary <extra_id_0><TAB><TAB><TAB><TAB>editable_dict[k] = v",
        "target_block": "if depth of editing_dict is more then 1Args:editable_dict: dictionary, that will be editedediting_dict: dictionary, that contains editsReturns:None\"\"\"for k, v in editing_dict.items():if isinstance(v, collections.Mapping):update_dict_recursive(editable_dict.get(k, {}), v)else:"
    },
    {
        "input_method": "def import_sql_table(connection_url, table, username, password, columns=None, optimize=True, fetch_mode=None):<TAB><TAB>\"\"\"<TAB><TAB>Import SQL table to H2OFrame in memory.<TAB><TAB><TAB>Assumes that the SQL table is not being updated and is stable.<TAB><TAB>Runs multiple SELECT SQL queries concurrently for parallel ingestion.<TAB><TAB>Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::<TAB><TAB><TAB><TAB>java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp<TAB><TAB><TAB>Also see :func:`import_sql_select`.<TAB><TAB>Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle and Microsoft SQL.<TAB><TAB><TAB>:param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)<TAB><TAB><TAB>Driver. For example, \"jdbc:mysql://localhost:3306/menagerie?&useSSL=false\"<TAB><TAB>:param table: name of SQL table<TAB><TAB>:param columns: a list of column names to import from SQL table. Default is to import all columns.<TAB><TAB>:param username: username for SQL server<TAB><TAB>:param password: password for SQL server<TAB><TAB>:param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.<TAB><TAB>:param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node<TAB><TAB><TAB>from the database.<TAB><TAB><TAB>:returns: an :class:`H2OFrame` containing data of the specified SQL table.<TAB><TAB><TAB>:examples:<TAB><TAB><TAB>>>> conn_url = \"jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false\"<TAB><TAB><TAB>>>> table = \"citibike20k\"<TAB><TAB><TAB>>>> username = \"root\"<TAB><TAB><TAB>>>> password = \"abc123\"<TAB><TAB><TAB>>>> my_citibike_data = h2o.import_sql_table(conn_url, table, username, password)<TAB><TAB>\"\"\"<TAB><TAB>assert_is_type(connection_url, str)<TAB><TAB>assert_is_type(table, str)<TAB><TAB>assert_is_type(username, str)<TAB><TAB>assert_is_type(password, str)<TAB><TAB>assert_is_type(columns, [str], None)<TAB><TAB>assert_is_type(optimize, bool)<TAB><TAB>assert_is_type(fetch_mode, str, None)<TAB><TAB>p = {\"connection_url\": connection_url, \"table\": table, \"username\": username, \"password\": password,<TAB><TAB><TAB> \"fetch_mode\": fetch_mode}<TAB><TAB><extra_id_0><TAB><TAB><TAB>p[\"columns\"] = \", \".join(columns)<TAB><TAB>j = H2OJob(api(\"POST /99/ImportSQLTable\", data=p), \"Import SQL Table\").poll()<TAB><TAB>return get_frame(j.dest_key)",
        "target_block": "if columns:"
    },
    {
        "input_method": "def visit_import(self, node):<TAB><TAB><TAB>\"\"\"Checks to see <extra_id_0><TAB><TAB><TAB><TAB><TAB>self._logging_names.add(as_name or module)",
        "target_block": "if this module uses Python's built-in logging.\"\"\"for module, as_name in node.names:if module in self._logging_modules:"
    },
    {
        "input_method": "def formatday(self, day, weekday):<TAB><TAB><TAB>\"\"\"Set some commonly used variables.\"\"\"<TAB><TAB><TAB>self.wkday_not_today = '<td class=\"%s\"><div class=\"td-inner\">' % (<TAB><TAB><TAB><TAB>self.cssclasses[weekday])<TAB><TAB><TAB><TAB>self.wkday_today = (<TAB><TAB><TAB><TAB>'<td class=\"%s calendar-today\"><div class=\"td-inner\">' % (<TAB><TAB><TAB><TAB><TAB>self.cssclasses[weekday])<TAB><TAB><TAB>)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>url_name = 'day_list'<TAB><TAB><TAB><TAB>self.day_url = reverse(url_name, args=(self.yr, self.mo, day))<TAB><TAB><TAB>self.day = day<TAB><TAB><TAB><TAB>self.anch = '<a href=\"%s\">%d</a>' % (<TAB><TAB><TAB><TAB>self.day_url, day<TAB><TAB><TAB>)<TAB><TAB><TAB><TAB>self.end = '</div></td>'",
        "target_block": "if URLS_NAMESPACE:url_name = '%s:day_list' % (URLS_NAMESPACE)else:"
    },
    {
        "input_method": "def _update_with_rollback(self, on_dup, *args, **kw):<TAB><TAB><TAB>\"\"\"Update, rolling back on failure.\"\"\"<TAB><TAB><TAB>writelog = []<TAB><TAB><TAB>appendlog = writelog.append<TAB><TAB><TAB>dedup_item = self._dedup_item<TAB><TAB><TAB>write_item = self._write_item<TAB><TAB><TAB>for (key, val) in _iteritems_args_kw(*args, **kw):<TAB><TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB><TAB>dedup_result = dedup_item(key, val, on_dup)<TAB><TAB><TAB><TAB>except DuplicationError:<TAB><TAB><TAB><TAB><TAB>undo_write = self._undo_write<TAB><TAB><TAB><TAB><TAB>for dedup_result, write_result in reversed(writelog):<TAB><TAB><TAB><TAB><TAB><TAB>undo_write(dedup_result, write_result)<TAB><TAB><TAB><TAB><TAB>raise<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>write_result = write_item(key, val, dedup_result)<TAB><TAB><TAB><TAB><TAB>appendlog((dedup_result, write_result))",
        "target_block": "if dedup_result is not _NOOP:"
    },
    {
        "input_method": "def iter_chunks(self, fd=None, buf=None, skip_header=None):<TAB><TAB><TAB>\"\"\"Reads FLV tags from fd or buf and returns them with adjusted<TAB><TAB><TAB>   timestamps.\"\"\"<TAB><TAB><TAB>timestamps = dict(self.timestamps_add)<TAB><TAB><TAB>tag_iterator = self.iter_tags(fd=fd, buf=buf, skip_header=skip_header)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>self.timestamps_add = timestamps<TAB><TAB><TAB><TAB>self.tags = []",
        "target_block": "if not self.flv_header_written:analyzed_tags = self.analyze_tags(tag_iterator)else:analyzed_tags = []for tag in chain(analyzed_tags, tag_iterator):if not self.flv_header_written:flv_header = Header(has_video=self.has_video,has_audio=self.has_audio)yield flv_header.serialize()self.flv_header_written = Trueif self.verify_tag(tag):self.adjust_tag_gap(tag)self.adjust_tag_timestamp(tag)if self.duration:norm_timestamp = tag.timestamp / 1000if norm_timestamp > self.duration:breakyield tag.serialize()timestamps[tag.type] = tag.timestampif not self.flatten_timestamps:"
    },
    {
        "input_method": "def list_prefixes(self, bucket_name, prefix='', delimiter='',<TAB><TAB><TAB><TAB><TAB><TAB>  page_size=None, max_items=None):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Lists prefixes in a bucket under prefix<TAB><TAB><TAB><TAB>:param bucket_name: the name of the bucket<TAB><TAB><TAB>:type bucket_name: str<TAB><TAB><TAB>:param prefix: a key prefix<TAB><TAB><TAB>:type prefix: str<TAB><TAB><TAB>:param delimiter: the delimiter marks key hierarchy.<TAB><TAB><TAB>:type delimiter: str<TAB><TAB><TAB>:param page_size: pagination size<TAB><TAB><TAB>:type page_size: int<TAB><TAB><TAB>:param max_items: maximum items to return<TAB><TAB><TAB>:type max_items: int<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>config = {<TAB><TAB><TAB><TAB>'PageSize': page_size,<TAB><TAB><TAB><TAB>'MaxItems': max_items,<TAB><TAB><TAB>}<TAB><TAB><TAB><TAB>paginator = self.get_conn().get_paginator('list_objects_v2')<TAB><TAB><TAB>response = paginator.paginate(Bucket=bucket_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Prefix=prefix,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  Delimiter=delimiter,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>  PaginationConfig=config)<TAB><TAB><TAB><TAB>has_results = False<TAB><TAB><TAB>prefixes = []<TAB><TAB><TAB>for page in response:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return prefixes",
        "target_block": "if 'CommonPrefixes' in page:has_results = Truefor p in page['CommonPrefixes']:prefixes.append(p['Prefix'])if has_results:"
    },
    {
        "input_method": "def queue_status(self, targets='all', verbose=False):<TAB><TAB><TAB>\"\"\"Fetch the status of engine queues.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB><TAB>targets : int/str/list of ints/strs<TAB><TAB><TAB><TAB><TAB>the engines whose states are to be queried.<TAB><TAB><TAB><TAB><TAB>default : all<TAB><TAB><TAB>verbose : bool<TAB><TAB><TAB><TAB><TAB>Whether to return lengths only, or lists of ids for each element<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return content",
        "target_block": "if targets == 'all':# allow 'all' to be evaluated on the engineengine_ids = Noneelse:engine_ids = self._build_targets(targets)[1]content = dict(targets=engine_ids, verbose=verbose)self.session.send(self._query_socket, \"queue_request\", content=content)idents,msg = self.session.recv(self._query_socket, 0)if self.debug:pprint(msg)content = msg['content']status = content.pop('status')if status != 'ok':raise self._unwrap_exception(content)content = rekey(content)if isinstance(targets, int):return content[targets]else:"
    },
    {
        "input_method": "def _init_params(self, amplitude, length_scale, validate_args):<TAB><TAB>\"\"\"Shared init logic for `amplitude` and `length_scale` params.<TAB><TAB><TAB>Args:<TAB><TAB>  amplitude: `Tensor` (or convertible) or `None` to convert, validate.<TAB><TAB>  length_scale: `Tensor` (or convertible) or `None` to convert, validate.<TAB><TAB>  validate_args: If `True`, parameters are checked for validity despite<TAB><TAB><TAB>possibly degrading runtime performance<TAB><TAB><TAB>Returns:<TAB><TAB>  dtype: The common `DType` of the parameters.<TAB><TAB>\"\"\"<TAB><TAB>dtype = util.maybe_get_common_dtype(<TAB><TAB><TAB>[amplitude, length_scale])<TAB><TAB><extra_id_0><TAB><TAB>  length_scale = tf.convert_to_tensor(<TAB><TAB><TAB>  value=length_scale, name='length_scale', dtype=dtype)<TAB><TAB>self._length_scale = _validate_arg_if_not_none(<TAB><TAB><TAB>length_scale, tf.compat.v1.assert_positive, validate_args)<TAB><TAB>return dtype",
        "target_block": "if amplitude is not None:  amplitude = tf.convert_to_tensor(  value=amplitude, name='amplitude', dtype=dtype)self._amplitude = _validate_arg_if_not_none(amplitude, tf.compat.v1.assert_positive, validate_args)if length_scale is not None:"
    },
    {
        "input_method": "def credit_note(request, note_id, access_code=None):<TAB><TAB>''' Displays a credit note.<TAB><TAB><TAB>If ``request`` is a ``POST`` request, forms for applying or refunding<TAB><TAB>a credit note will be processed.<TAB><TAB><TAB>This view requires a login, and the logged in user must be staff.<TAB><TAB><TAB>Arguments:<TAB><TAB><TAB>note_id (castable to int): The ID of the credit note to view.<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>render or redirect:<TAB><TAB><TAB><TAB>If the \"apply to invoice\" form is correctly processed, redirect to<TAB><TAB><TAB><TAB>that invoice, otherwise, render ``registration/credit_note.html``<TAB><TAB><TAB><TAB>with the following data::<TAB><TAB><TAB><TAB><TAB><TAB>{<TAB><TAB><TAB><TAB><TAB><TAB>\"credit_note\": models.commerce.CreditNote(),<TAB><TAB><TAB><TAB><TAB><TAB>\"apply_form\": form,  # A form for applying credit note<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> # to an invoice.<TAB><TAB><TAB><TAB><TAB><TAB>\"refund_form\": form, # A form for applying a *manual*<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> # refund of the credit note.<TAB><TAB><TAB><TAB><TAB><TAB>\"cancellation_fee_form\" : form, # A form for generating an<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB># invoice with a<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB># cancellation fee<TAB><TAB><TAB><TAB><TAB>}<TAB><TAB><TAB>'''<TAB><TAB><TAB>note_id = int(note_id)<TAB><TAB>current_note = CreditNoteController.for_id_or_404(note_id)<TAB><TAB><TAB>apply_form = forms.ApplyCreditNoteForm(<TAB><TAB><TAB>current_note.credit_note.invoice.user,<TAB><TAB><TAB>request.POST or None,<TAB><TAB><TAB>prefix=\"apply_note\"<TAB><TAB>)<TAB><TAB><TAB>refund_form = forms.ManualCreditNoteRefundForm(<TAB><TAB><TAB>request.POST or None,<TAB><TAB><TAB>prefix=\"refund_note\"<TAB><TAB>)<TAB><TAB><TAB>cancellation_fee_form = forms.CancellationFeeForm(<TAB><TAB><TAB>request.POST or None,<TAB><TAB><TAB>prefix=\"cancellation_fee\"<TAB><TAB>)<TAB><TAB><TAB><extra_id_0> cancellation_fee_form,<TAB><TAB>}<TAB><TAB><TAB>return render(request, \"registrasion/credit_note.html\", data)",
        "target_block": "if request.POST and apply_form.is_valid():inv_id = apply_form.cleaned_data[\"invoice\"]invoice = commerce.Invoice.objects.get(pk=inv_id)current_note.apply_to_invoice(invoice)messages.success(request,\"Applied credit note %d to invoice.\" % note_id,)return redirect(\"invoice\", invoice.id)elif request.POST and refund_form.is_valid():refund_form.instance.entered_by = request.userrefund_form.instance.parent = current_note.credit_noterefund_form.save()messages.success(request,\"Applied manual refund to credit note.\")refund_form = forms.ManualCreditNoteRefundForm(prefix=\"refund_note\",)elif request.POST and cancellation_fee_form.is_valid():percentage = cancellation_fee_form.cleaned_data[\"percentage\"]invoice = current_note.cancellation_fee(percentage)messages.success(request,\"Generated cancellation fee for credit note %d.\" % note_id,)return redirect(\"invoice\", invoice.invoice.id)data = {\"credit_note\": current_note.credit_note,\"apply_form\": apply_form,\"refund_form\": refund_form,\"cancellation_fee_form\":"
    },
    {
        "input_method": "def _determine_function_name_type(node, config=None):<TAB><TAB>\"\"\"Determine the name type whose regex the a function's name should match.<TAB><TAB><TAB>:param node: A function node.<TAB><TAB>:type node: astroid.node_classes.NodeNG<TAB><TAB>:param config: Configuration from which to pull additional property classes.<TAB><TAB>:type config: :class:`optparse.Values`<TAB><TAB><TAB>:returns: One of ('function', 'method', 'attr')<TAB><TAB>:rtype: str<TAB><TAB>\"\"\"<TAB><TAB>property_classes, property_names = _get_properties(config)<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return \"attr\"<TAB><TAB>return \"method\"",
        "target_block": "if not node.is_method():return \"function\"if node.decorators:decorators = node.decorators.nodeselse:decorators = []for decorator in decorators:# If the function is a property (decorated with @property# or @abc.abstractproperty), the name type is 'attr'.if isinstance(decorator, astroid.Name) or (isinstance(decorator, astroid.Attribute)and decorator.attrname in property_names):infered = utils.safe_infer(decorator)if infered and infered.qname() in property_classes:return \"attr\"# If the function is decorated using the prop_method.{setter,getter}# form, treat it like an attribute as well.elif isinstance(decorator, astroid.Attribute) and decorator.attrname in (\"setter\",\"deleter\",):"
    },
    {
        "input_method": "def validate_twilio(attr, value):<TAB><TAB>\"\"\"Twilio input validator function.\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>check_valid(\"Twilio\", attr, value, validus.isurl, \"url\")",
        "target_block": "if attr in (\"from_\", \"to\"):check_valid(\"Twilio\", attr, value, validus.isphone, \"phone number\")elif attr in (\"attachments\"):"
    },
    {
        "input_method": "def modify_event(uid):<TAB><TAB>'''Modify an event specified by its uid. The modifications for the event<TAB><TAB>are expected as JSON with the content type correctly set in the request.<TAB><TAB><TAB>Note that this method works for recorded events only. Upcoming events part<TAB><TAB>of the scheduler cache cannot be modified.<TAB><TAB>'''<TAB><TAB>try:<TAB><TAB><TAB>data = request.get_json()['data'][0]<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>return make_error_response('No event with specified uid', 404)<TAB><TAB>event.start = data['attributes'].get('start', event.start)<TAB><TAB>event.end = data['attributes'].get('end', event.end)<TAB><TAB>event.status = data['attributes'].get('status', event.status)<TAB><TAB>logger.debug('Updating event %s via api', uid)<TAB><TAB>db.commit()<TAB><TAB>return make_data_response(event.serialize())",
        "target_block": "if data['type'] != 'event' or data['id'] != uid:return make_error_response('Invalid data', 400)# Check attributesfor key in data['attributes'].keys():if key not in ('status', 'start', 'end'):return make_error_response('Invalid data', 400)# Check new statusnew_status = data['attributes'].get('status')if new_status:new_status = new_status.upper().replace(' ', '_')data['attributes']['status'] = int(getattr(Status, new_status))except Exception:return make_error_response('Invalid data', 400)db = get_session()event = db.query(RecordedEvent).filter(RecordedEvent.uid == uid).first()if not event:"
    },
    {
        "input_method": "def _cubic_bernstein_extrema(p0, p1, p2, p3):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Find extremas of a function of real domain defined by evaluating<TAB><TAB><TAB>a cubic bernstein polynomial of given bernstein coefficients.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB># compute coefficients of derivative<TAB><TAB><TAB>a = 3.*(p3-p0+3.*(p1-p2))<TAB><TAB><TAB>b = 6.*(p0+p2-2.*p1)<TAB><TAB><TAB>c = 3.*(p1-p0)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return (b / k,)<TAB><TAB><TAB><TAB>r = math.sqrt(d)<TAB><TAB><TAB>return ((b + r) / k, (b - r) / k)",
        "target_block": "if a == 0:if b == 0:return ()  # constantreturn (-c / b,)  # linear# quadratic# compute discriminantd = b*b - 4.*a*cif d < 0:return ()k = -2. * aif d == 0:"
    },
    {
        "input_method": "def get_signedness_for_extra_dim(type_index):<TAB><TAB>\"\"\" Returns the signedness foe the given type index<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>type_index: int<TAB><TAB><TAB>index of the type as defined in the LAS Specification<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>DimensionSignedness,<TAB><TAB><TAB>the enum variant<TAB><TAB>\"\"\"<TAB><TAB>try:<TAB><TAB><TAB>t = _extra_dims_style_2[type_index]<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>raise errors.UnknownExtraType(type_index)",
        "target_block": "if \"uint\" in t:return DimensionSignedness.UNSIGNEDelif \"int\" in t:return DimensionSignedness.SIGNEDelse:return DimensionSignedness.FLOATINGexcept IndexError:"
    },
    {
        "input_method": "async def subscribe(self, channels):<TAB><TAB><TAB>'''Start subscribing channels.<TAB><TAB><TAB>If the necessary connection isn't open yet, it opens now.<TAB><TAB><TAB>'''<TAB><TAB><TAB>ws_channels = []<TAB><TAB><TAB>nats_channels = []<TAB><TAB><TAB>for c in channels:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>await self._ensure_nats()<TAB><TAB><TAB><TAB>await self.polygon.subscribe(nats_channels)",
        "target_block": "if c.startswith(('Q.', 'T.', 'A.', 'AM.',)):nats_channels.append(c)else:ws_channels.append(c)if len(ws_channels) > 0:await self._ensure_ws()await self._ws.send(json.dumps({'action': 'listen','data': {'streams': ws_channels,}}))if len(nats_channels) > 0:"
    },
    {
        "input_method": "def ch_start_time(self, *channels: List[Channel]) -> int:<TAB><TAB><TAB>\"\"\"Return earliest start time in this collection.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>*channels: Channels over which to obtain start_time.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>intervals = list(itertools.chain(*(self._table[chan] for chan in channels<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   <extra_id_0><TAB><TAB><TAB><TAB>return min((interval.begin for interval in intervals))<TAB><TAB><TAB>return 0",
        "target_block": "if chan in self._table)))if intervals:"
    },
    {
        "input_method": "def _validate_input_state(quantum_state):<TAB><TAB>\"\"\"Validates the input to state visualization functions.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>quantum_state (ndarray): Input state / density matrix.<TAB><TAB>Returns:<TAB><TAB><TAB>rho: A 2d numpy array for the density matrix.<TAB><TAB>Raises:<TAB><TAB><TAB>VisualizationError: Invalid input.<TAB><TAB>\"\"\"<TAB><TAB>rho = np.asarray(quantum_state)<TAB><TAB><extra_id_0><TAB><TAB><TAB>raise VisualizationError(\"Input is not a multi-qubit quantum state.\")<TAB><TAB>return rho",
        "target_block": "if rho.ndim == 1:rho = np.outer(rho, np.conj(rho))# Check the shape of the input is a square matrixshape = np.shape(rho)if len(shape) != 2 or shape[0] != shape[1]:raise VisualizationError(\"Input is not a valid quantum state.\")# Check state is an n-qubit statenum = int(np.log2(rho.shape[0]))if 2 ** num != rho.shape[0]:"
    },
    {
        "input_method": "def tscore(sample1, sample2):<TAB><TAB>\"\"\"Calculate a t-test score for the difference between two samples.<TAB><TAB><TAB>Args:<TAB><TAB><TAB>sample1: one sample.<TAB><TAB><TAB>sample2: the other sample.<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>The t-test score, as a float.<TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>raise ValueError(\"different number of values\")<TAB><TAB>error = pooled_sample_variance(sample1, sample2) / len(sample1)<TAB><TAB>diff = statistics.mean(sample1) - statistics.mean(sample2)<TAB><TAB>return diff / math.sqrt(error * 2)",
        "target_block": "if len(sample1) != len(sample2):"
    },
    {
        "input_method": "def date(self):<TAB><TAB><TAB>\"\"\"DATE command.<TAB><TAB><TAB><TAB>Coordinated Universal time from the perspective of the usenet server.<TAB><TAB><TAB>It can be used to provide information that might be useful when using<TAB><TAB><TAB>the NEWNEWS command.<TAB><TAB><TAB><TAB>See <http://tools.ietf.org/html/rfc3977#section-7.1><TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>The UTC time according to the server as a datetime object.<TAB><TAB><TAB><TAB>Raises:<TAB><TAB><TAB><TAB>NNTPDataError: If the timestamp can't be parsed.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>code, message = self.command(\"DATE\")<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise NNTPReplyError(code, message)<TAB><TAB><TAB><TAB>ts = date.datetimeobj(message, fmt=\"%Y%m%d%H%M%S\")<TAB><TAB><TAB><TAB>return ts",
        "target_block": "if code != 111:"
    },
    {
        "input_method": "def lookup_signum(name):<TAB><TAB>\"\"\"Find the corresponding signal number for 'name'. Return None<TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>return getattr(signal, uname)<TAB><TAB><TAB>return None<TAB><TAB>return",
        "target_block": "if 'name' is invalid.\"\"\"uname = name.upper()if (uname.startswith('SIG') and hasattr(signal, uname)):return getattr(signal, uname)else:uname = \"SIG\"+unameif hasattr(signal, uname):"
    },
    {
        "input_method": "def connect(self, host='127.0.0.1', port=3306, user='root', password='', database=None):<TAB><TAB><TAB>\"\"\" Connect to the database specified \"\"\"<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>conn.query('SELECT 1')<TAB><TAB><TAB>return self",
        "target_block": "if database is None:raise exceptions.RequiresDatabase()self._db_args = { 'host': host, 'port': port, 'user': user, 'password': password, 'database': database }with self._db_conn() as conn:"
    },
    {
        "input_method": "def get_conn(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>check <extra_id_0><TAB><TAB><TAB><TAB>self.conn = self.get_client_type('athena')<TAB><TAB><TAB>return self.conn",
        "target_block": "if aws conn exists already or create one and return it:return: boto3 session\"\"\"if not self.conn:"
    },
    {
        "input_method": "def change_path(self, path):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Return a new LuminosoClient for a subpath of this one.<TAB><TAB><TAB><TAB>For example, you might want to start with a LuminosoClient for<TAB><TAB><TAB>`https://analytics.luminoso.com/api/v4/`, then get a new one for<TAB><TAB><TAB>`https://analytics.luminoso.com/api/v4/projects/myaccount/myprojectid`.<TAB><TAB><TAB>You accomplish that with the following call:<TAB><TAB><TAB><TAB><TAB>newclient = client.change_path('projects/myaccount/myproject_id')<TAB><TAB><TAB><TAB>If you start the path with `/`, it will start from the root_url<TAB><TAB><TAB>instead of the current url:<TAB><TAB><TAB><TAB><TAB>project_area = newclient.change_path('/projects/myaccount')<TAB><TAB><TAB><TAB>The advantage of using `.change_path` is that you will not need to<TAB><TAB><TAB>re-authenticate like you would <extra_id_0><TAB><TAB><TAB><TAB>url = self.url + path<TAB><TAB><TAB>return self.__class__(self.session, url)",
        "target_block": "if you ran `.connect` again.You can use `.change_path` to split off as many sub-clients as youwant, and you don't have to stop using the old one just because yougot a new one with `.change_path`.\"\"\"if path.startswith('/'):url = self.root_url + pathelse:"
    },
    {
        "input_method": "def main():<TAB><TAB>'''The main function. Instantiates a GameState object and then<TAB><TAB>enters a REPL-like main loop, waiting for input, updating the state<TAB><TAB>based on the input, then outputting the new state.'''<TAB><TAB><TAB>state = GameState()<TAB><TAB>print(state)<TAB><TAB>while state.running:<TAB><TAB><TAB>input = get_single_char()<TAB><TAB><TAB><TAB>state, should_advance = state.handle_input(input)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>state = state.advance_robots()<TAB><TAB><TAB><TAB>state = state.check_game_end()<TAB><TAB><TAB><TAB>print(state)<TAB><TAB><TAB>print(state.message)",
        "target_block": "if should_advance:"
    },
    {
        "input_method": "def findsource(object):<TAB><TAB>\"\"\"Return the entire source file and starting line number for an object.<TAB><TAB><TAB>The argument may be a module, class, method, function, traceback, frame,<TAB><TAB>or code object.  The source code is returned as a list of all the lines<TAB><TAB>in the file and the line number indexes a line in that list.  An IOError<TAB><TAB>is raised <extra_id_0> break<TAB><TAB><TAB><TAB>lnum -= 1<TAB><TAB><TAB><TAB>return lines, lnum<TAB><TAB>raise IOError('could not find code object')",
        "target_block": "if the source code cannot be retrieved.FIXED version with which we monkeypatch the stdlib to work around a bug.\"\"\"file = getsourcefile(object) or getfile(object)# If the object is a frame, then trying to get the globals dict from its# module won't work. Instead, the frame object itself has the globals# dictionary.globals_dict = Noneif inspect.isframe(object):# XXX: can this ever be false?globals_dict = object.f_globalselse:module = getmodule(object, file)if module:globals_dict = module.__dict__lines = linecache.getlines(file, globals_dict)if not lines:raise IOError('could not get source code')if ismodule(object):return lines, 0if isclass(object):name = object.__name__pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')# make some effort to find the best matching class definition:# use the one with the least indentation, which is the one# that's most probably not inside a function definition.candidates = []for i in range(len(lines)):match = pat.match(lines[i])if match:# if it's at toplevel, it's already the best oneif lines[i][0] == 'c':return lines, i# else add whitespace to candidate listcandidates.append((match.group(1), i))if candidates:# this will sort by whitespace, and by line number,# less whitespace firstcandidates.sort()return lines, candidates[0][1]else:raise IOError('could not find class definition')if ismethod(object):object = object.im_funcif isfunction(object):object = object.func_codeif istraceback(object):object = object.tb_frameif isframe(object):object = object.f_codeif iscode(object):if not hasattr(object, 'co_firstlineno'):raise IOError('could not find function definition')pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')pmatch = pat.match# fperez - fix: sometimes, co_firstlineno can give a number larger than# the length of lines, which causes an error.  Safeguard against that.lnum = min(object.co_firstlineno,len(lines))-1while lnum > 0:if pmatch(lines[lnum]):"
    },
    {
        "input_method": "def calculate_item_depth(self, tree_alias, item_id, depth=0):<TAB><TAB><TAB>\"\"\"Calculates depth of the item in the tree.<TAB><TAB><TAB><TAB>:param str|unicode tree_alias:<TAB><TAB><TAB>:param int item_id:<TAB><TAB><TAB>:param int depth:<TAB><TAB><TAB>:rtype: int<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>item = self.get_item_by_id(tree_alias, item_id)<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>depth = self.calculate_item_depth(tree_alias, item.parent.id, depth + 1)<TAB><TAB><TAB><TAB>return depth",
        "target_block": "if hasattr(item, 'depth'):depth = item.depth + depthelse:if item.parent is not None:"
    },
    {
        "input_method": "def speak(self):<TAB><TAB><TAB>'''<TAB><TAB><TAB>   a function for the client to announce him or herself, depending<TAB><TAB><TAB>   on the level specified. If you want your client to have additional<TAB><TAB><TAB>   announced things here, then implement the class `_speak` for your<TAB><TAB><TAB>   client.<TAB><TAB><TAB><TAB>'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>bot.info('[client|%s] [database|%s]' %(self.client_name,<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB>   self.database))<TAB><TAB><TAB><TAB><TAB>self._speak()",
        "target_block": "if self.quiet is False:"
    },
    {
        "input_method": "def check_next(self, tag):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>If next tag is link with same href, combine them.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>next_text = next_tag.get_text()<TAB><TAB><TAB><TAB><TAB><TAB>tag.append(next_text)<TAB><TAB><TAB><TAB><TAB><TAB>self.tags_blacklist.append(next_tag)",
        "target_block": "if (type(tag.next_sibling) == element.Tag andtag.next_sibling.name == 'a'):next_tag = tag.next_siblingif tag.get('href') and next_tag.get('href'):href = self._parse_href(tag.get('href'))next_href = self._parse_href(next_tag.get('href'))if href == next_href:"
    },
    {
        "input_method": "def _build_unpacked_point_formats_dtypes(<TAB><TAB><TAB>point_formats_dimensions, composed_fields_dict, dimensions_dict<TAB>):<TAB><TAB>\"\"\" Builds the dict mapping point format id to numpy.dtype<TAB><TAB>In the dtypes, bit fields are unpacked and can be accessed directly<TAB><TAB>\"\"\"<TAB><TAB>unpacked_dtypes = {}<TAB><TAB>for fmt_id, dim_names in point_formats_dimensions.items():<TAB><TAB><TAB>composed_dims, dtype = composed_fields_dict[fmt_id], []<TAB><TAB><TAB>for dim_name in dim_names:<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>dtype.append(dimensions_dict[dim_name])<TAB><TAB><TAB>unpacked_dtypes[fmt_id] = np.dtype(dtype)<TAB><TAB>return unpacked_dtypes",
        "target_block": "if dim_name in composed_dims:dtype.extend((f.name, f.type) for f in composed_dims[dim_name])else:"
    },
    {
        "input_method": "def delete_uneeded(self):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Delete the directory which are not registered into our structure.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><TAB># We get the structure we have to apply.<TAB><TAB><TAB>structure = self._get_structure()<TAB><TAB><TAB><TAB># We get the list of key which is implicitly the list of directory we do not bave to delete.<TAB><TAB><TAB>list_of_key = list(structure.keys())<TAB><TAB><TAB><TAB># We move to the content of the parent as we know that we are creating only one directory.<TAB><TAB><TAB># Note: <extra_id_0><TAB><TAB><TAB><TAB><TAB># The currently read directory is not in our structure.<TAB><TAB><TAB><TAB><TAB><TAB># We delete it.<TAB><TAB><TAB><TAB><TAB>PyFunceble.rmtree(root)",
        "target_block": "if one day we will have to create multiple directory, we will have to change# the following.structure = structure[list_of_key[0]]# We also set the parent directory as we are going to construct its childen.parent_path = list_of_key[0]if not parent_path.endswith(PyFunceble.directory_separator):parent_path += PyFunceble.directory_separatorfor root, _, _ in PyFunceble.walk(parent_path):# We loop through each directories of the parent path.# We fix the path in order to avoid issues.root = Directory(root).fix_path()if root.replace(parent_path, \"\") not in structure:"
    },
    {
        "input_method": "def unload(self):<TAB><TAB><TAB>'''Releases renderer resources associated with this image.'''<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>lib.UnloadImage(self._handle)<TAB><TAB><TAB>self._handle = -1",
        "target_block": "if self._handle != -1:"
    },
    {
        "input_method": "def report(self, morfs, outfile=None):<TAB><TAB><TAB>\"\"\"Writes a report summarizing coverage statistics per module.<TAB><TAB><TAB><TAB>`outfile` is a file object to write the summary to.<TAB><TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>self.find_code_units(morfs)<TAB><TAB><TAB><TAB># Prepare the formatting strings<TAB><TAB><TAB>max_name = max([len(cu.name) for cu in self.code_units] + [5])<TAB><TAB><TAB>fmt_name = \"%%- %ds  \" % max_name<TAB><TAB><TAB>fmt_err = \"%s   %s: %s\\n\"<TAB><TAB><TAB>header = (fmt_name % \"Name\") + \" Stmts   Miss\"<TAB><TAB><TAB>fmt_coverage = fmt_name + \"%6d %6d\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>args += (\"\",)<TAB><TAB><TAB><TAB>outfile.write(fmt_coverage % args)<TAB><TAB><TAB><TAB>return total.pc_covered",
        "target_block": "if self.branches:header += \" Branch BrMiss\"fmt_coverage += \" %6d %6d\"width100 = Numbers.pc_str_width()header += \"%*s\" % (width100+4, \"Cover\")fmt_coverage += \"%%%ds%%%%\" % (width100+3,)if self.config.show_missing:header += \"   Missing\"fmt_coverage += \"   %s\"rule = \"-\" * len(header) + \"\\n\"header += \"\\n\"fmt_coverage += \"\\n\"if not outfile:outfile = sys.stdout# Write the headeroutfile.write(header)outfile.write(rule)total = Numbers()for cu in self.code_units:try:analysis = self.coverage._analyze(cu)nums = analysis.numbersargs = (cu.name, nums.n_statements, nums.n_missing)if self.branches:args += (nums.n_branches, nums.n_missing_branches)args += (nums.pc_covered_str,)if self.config.show_missing:args += (analysis.missing_formatted(),)outfile.write(fmt_coverage % args)total += numsexcept KeyboardInterrupt:   # pragma: not coveredraiseexcept:report_it = not self.config.ignore_errorsif report_it:typ, msg = sys.exc_info()[:2]if typ is NotPython and not cu.should_be_python():report_it = Falseif report_it:outfile.write(fmt_err % (cu.name, typ.__name__, msg))if total.n_files > 1:outfile.write(rule)args = (\"TOTAL\", total.n_statements, total.n_missing)if self.branches:args += (total.n_branches, total.n_missing_branches)args += (total.pc_covered_str,)if self.config.show_missing:"
    },
    {
        "input_method": "def bio_read(self, bufsiz):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>If the Connection was created with a memory BIO, this method can be<TAB><TAB><TAB>used to read bytes from the write end of that memory BIO.  Many<TAB><TAB><TAB>Connection methods will add bytes which must be read in this manner or<TAB><TAB><TAB>the buffer will eventually fill up and the Connection will be able to<TAB><TAB><TAB>take no further actions.<TAB><TAB><TAB><TAB>:param bufsiz: The maximum number of bytes to read<TAB><TAB><TAB>:return: The string read.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0>]",
        "target_block": "if self._from_ssl is None:raise TypeError(\"Connection sock was not None\")if not isinstance(bufsiz, integer_types):raise TypeError(\"bufsiz must be an integer\")buf = _no_zero_allocator(\"char[]\", bufsiz)result = _lib.BIO_read(self._from_ssl, buf, bufsiz)if result <= 0:self._handle_bio_errors(self._from_ssl, result)return _ffi.buffer(buf, result)[:"
    },
    {
        "input_method": "def _has_valid_type_annotation(self, tokens, i):<TAB><TAB><TAB>\"\"\"Extended check of PEP-484 type hint presence\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>return False<TAB><TAB><TAB>return False",
        "target_block": "if not self._inside_brackets(\"(\"):return False# token_info# type string start end line#  0  1 234bracket_level = 0for token in tokens[i - 1 :: -1]:if token[1] == \":\":return Trueif token[1] == \"(\":return Falseif token[1] == \"]\":bracket_level += 1elif token[1] == \"[\":bracket_level -= 1elif token[1] == \",\":if not bracket_level:return Falseelif token[1] in (\".\", \"...\"):continueelif token[0] not in (tokenize.NAME, tokenize.STRING, tokenize.NL):"
    },
    {
        "input_method": "def run(self, options, args):<TAB><TAB><TAB>\"\"\"Prints the completion code of the given shell\"\"\"<TAB><TAB><TAB>shells = COMPLETION_SCRIPTS.keys()<TAB><TAB><TAB>shell_options = ['--' + shell for shell in sorted(shells)]<TAB><TAB><TAB><extra_id_0> You must pass %s\\n' % ' or '.join(shell_options)<TAB><TAB><TAB><TAB>)",
        "target_block": "if options.shell in shells:script = COMPLETION_SCRIPTS.get(options.shell, '')print(BASE_COMPLETION % {'script': script, 'shell': options.shell})else:sys.stderr.write('ERROR:"
    },
    {
        "input_method": "def _verify_file_size(self, obj, downloaded_file):<TAB><TAB>'''Verify the file size of the downloaded file.'''<TAB><TAB>file_size = os.path.getsize(downloaded_file)<TAB><TAB><extra_id_0> %s' % (repr(obj)))",
        "target_block": "if int(obj['ContentLength']) != file_size:  raise RetryFailure('Downloaded file size inconsistent:"
    },
    {
        "input_method": "def write_index(self, outdir, froot='gen', relative_to=None):<TAB><TAB><TAB>\"\"\"Make a reST API index file from written files<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>path : string<TAB><TAB><TAB><TAB>Filename to write index to<TAB><TAB><TAB>outdir : string<TAB><TAB><TAB><TAB>Directory to which to write generated index file<TAB><TAB><TAB>froot : string, optional<TAB><TAB><TAB><TAB>root (filename without extension) of filename to write to<TAB><TAB><TAB><TAB>Defaults to 'gen'.  We add ``self.rst_extension``.<TAB><TAB><TAB>relative_to : string<TAB><TAB><TAB><TAB>path to which written filenames are relative.  This<TAB><TAB><TAB><TAB>component of the written file path will be removed from<TAB><TAB><TAB><TAB>outdir, in the generated index.  Default is None, meaning,<TAB><TAB><TAB><TAB>leave path as it is.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>w('   %s\\n' % os.path.join(relpath,f))<TAB><TAB><TAB>idx.close()",
        "target_block": "if self.written_modules is None:raise ValueError('No modules written')# Get full filename pathpath = os.path.join(outdir, froot+self.rst_extension)# Path written into index is relative to rootpathif relative_to is not None:relpath = outdir.replace(relative_to + os.path.sep, '')else:relpath = outdiridx = open(path,'wt')w = idx.writew('.. AUTO-GENERATED FILE -- DO NOT EDIT!\\n\\n')w('.. toctree::\\n\\n')for f in self.written_modules:"
    },
    {
        "input_method": "def get_duration(self, seconds):<TAB><TAB><TAB>\"\"\"Transform duration into a human-readable form.\"\"\"<TAB><TAB><TAB>duration = \"\"<TAB><TAB><TAB>minutes, seconds = divmod(seconds, 60)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>hours, minutes = divmod(minutes, 60)<TAB><TAB><TAB><TAB>duration = \"%sh \" % hours<TAB><TAB><TAB>duration += \"%sm %ss\" % (minutes, seconds)<TAB><TAB><TAB>return duration",
        "target_block": "if minutes >= 60:"
    },
    {
        "input_method": "def __float_window(window_spec):<TAB><TAB>'''Decorator function for windows with fractional input.<TAB><TAB><TAB>This function guarantees that for fractional `x`, the following hold:<TAB><TAB><TAB>1. `__float_window(window_function)(x)` has length `np.ceil(x)`<TAB><TAB>2. all values from `np.floor(x)` are set to 0.<TAB><TAB><TAB>For integer-valued `x`, there should be no change in behavior.<TAB><TAB>'''<TAB><TAB><TAB>def _wrap(n, *args, **kwargs):<TAB><TAB><TAB>'''The wrapped window'''<TAB><TAB><TAB>n_min, n_max = int(np.floor(n)), int(np.ceil(n))<TAB><TAB><TAB><TAB>window = get_window(window_spec, n_min)<TAB><TAB><TAB><TAB><extra_id_0>] = 0.0<TAB><TAB><TAB><TAB>return window<TAB><TAB><TAB>return _wrap",
        "target_block": "if len(window) < n_max:window = np.pad(window, [(0, n_max - len(window))],mode='constant')window[n_min:"
    },
    {
        "input_method": "def format_time(elapsed):<TAB><TAB>\"\"\"Formats elapsed seconds into a human readable format.\"\"\"<TAB><TAB>hours = int(elapsed / (60 * 60))<TAB><TAB>minutes = int((elapsed % (60 * 60)) / 60)<TAB><TAB>seconds = int(elapsed % 60)<TAB><TAB><TAB>rval = \"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>rval += \"{0}m\".format(minutes)<TAB><TAB><TAB>rval += \"{0}s\".format(seconds)<TAB><TAB>return rval",
        "target_block": "if hours:rval += \"{0}h\".format(hours)if elapsed > 60:"
    },
    {
        "input_method": "def add(self, pattern, function, method=None, type_cast=None):<TAB><TAB><TAB>\"\"\"Function for registering a path pattern.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>pattern (str): Regex pattern to match a certain path.<TAB><TAB><TAB><TAB>function (function): Function to associate with this path.<TAB><TAB><TAB><TAB>method (str, optional): Usually used to define one of GET, POST,<TAB><TAB><TAB><TAB><TAB>PUT, DELETE. You may use whatever fits your situation though.<TAB><TAB><TAB><TAB><TAB>Defaults to None.<TAB><TAB><TAB><TAB>type_cast (dict, optional): Mapping between the param name and<TAB><TAB><TAB><TAB><TAB>one of `int`, `float` or `bool`. The value reflected by the<TAB><TAB><TAB><TAB><TAB>provided param name will than be casted to the given type.<TAB><TAB><TAB><TAB><TAB>Defaults to None.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0> type_cast,<TAB><TAB><TAB><TAB>})",
        "target_block": "if not type_cast:type_cast = {}with self._lock:self._data_store.append({'pattern': pattern,'function': function,'method': method,'type_cast':"
    },
    {
        "input_method": "def create_zipfile(context):<TAB><TAB>\"\"\"This is the actual zest.releaser entry point<TAB><TAB><TAB>Relevant items in the context dict:<TAB><TAB><TAB>name<TAB><TAB><TAB>Name of the project being released<TAB><TAB><TAB>tagdir<TAB><TAB><TAB>Directory where the tag checkout is placed (*if* a tag<TAB><TAB><TAB>checkout has been made)<TAB><TAB><TAB>version<TAB><TAB><TAB>Version we're releasing<TAB><TAB><TAB>workingdir<TAB><TAB><TAB>Original working directory<TAB><TAB><TAB>\"\"\"<TAB><TAB><extra_id_0><TAB><TAB><TAB>first_part = zipfile.split('.')[0]<TAB><TAB><TAB>new_name = \"%s.%s.zip\" % (first_part, context['version'])<TAB><TAB><TAB>target = os.path.join(context['workingdir'], new_name)<TAB><TAB><TAB>shutil.copy(zipfile, target)<TAB><TAB><TAB>print(\"Copied %s to %s\" % (zipfile, target))",
        "target_block": "if not prerequisites_ok():return# Create a zipfile.subprocess.call(['make', 'zip'])for zipfile in glob.glob('*.zip'):"
    },
    {
        "input_method": "def since_start(self):<TAB><TAB><TAB>\"\"\"Total time since the timer is started.<TAB><TAB><TAB><TAB>Returns (float): Time in seconds.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>raise TimerError('timer is not running')<TAB><TAB><TAB>self._t_last = time()<TAB><TAB><TAB>return self._t_last - self._t_start",
        "target_block": "if not self._is_running:"
    },
    {
        "input_method": "def parallel(collection, method, processes=None, args=None, **kwargs):<TAB><TAB>'''Processes a collection in parallel.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>collection : list<TAB><TAB><TAB>i.e. list of Record objects<TAB><TAB>method : method to call on each Record<TAB><TAB>processes : int<TAB><TAB><TAB>number of processes to run on [defaults to number of cores on machine]<TAB><TAB>batch_size : int<TAB><TAB><TAB>lenght of each batch [defaults to number of elements / number of processes]<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>collection : list<TAB><TAB><TAB>list of Record objects after going through method called<TAB><TAB><TAB>Example<TAB><TAB>-------<TAB><TAB>adding 2 to every number in a range<TAB><TAB><TAB>>>> import turntable<TAB><TAB>>>> collection = range(100)<TAB><TAB>>>> def jam(record):<TAB><TAB>>>><TAB> return record + 2<TAB><TAB>>>> collection = turntable.spin.parallel(collection, jam)<TAB><TAB><TAB>Note<TAB><TAB>----<TAB><TAB><TAB>lambda functions do not work in parallel<TAB><TAB><TAB>'''<TAB><TAB><TAB><TAB><extra_id_0>-)\\n\" % (str(elapsed), str(processes))<TAB><TAB><TAB>return RES",
        "target_block": "if processes is None:# default to the number of cores, not exceeding 20processes = min(mp.cpu_count(), 20)print \"Running parallel process on \" + str(processes) + \" cores. :-)\"pool = mp.Pool(processes=processes)PROC = []tic = time.time()for main_arg in collection:if args is None:ARGS = (main_arg,)else:if isinstance(args, tuple) == False:args = (args,)ARGS = (main_arg,) + argsPROC.append(pool.apply_async(method, args=ARGS, kwds=kwargs))#RES = [p.get() for p in PROC]RES = []for p in PROC:try:RES.append(p.get())except Exception as e:print \"shit happens...\"print eRES.append(None)pool.close()pool.join()toc = time.time()elapsed = toc - ticprint \"Elapsed time: %s  on %s processes :"
    },
    {
        "input_method": "def lookup_(ctx, tableid, key):<TAB><TAB>'''<TAB><TAB>Yields a sequence of a single value, the result of looking up a value from the tables provided in the context, or an empty sequence <extra_id_0><TAB><TAB><TAB>innerctx = ctx.copy(item=item)<TAB><TAB><TAB>yield from pexpr.compute(innerctx)",
        "target_block": "if lookup is unsuccessful* tableid: id of the lookup table to use* expr: expression to be converted to string, then dynamically evaluated for each item on the sequence to produce the result'''tableid = next(string_arg(ctx, tableid), '')key = next(string_arg(ctx, key), '')#value = ctx.for item in seq:"
    },
    {
        "input_method": "def do_title(s):<TAB><TAB>\"\"\"Return a titlecased version of the value. I.e. words will start with<TAB><TAB>uppercase letters, all remaining characters are lowercase.<TAB><TAB>\"\"\"<TAB><TAB>rv = []<TAB><TAB>for item in re.compile(r'([-\\s]+)(?u)').split(s):<TAB><TAB><TAB><extra_id_0>].lower())<TAB><TAB>return ''.join(rv)",
        "target_block": "if not item:continuerv.append(item[0].upper() + item[1:"
    },
    {
        "input_method": "def _text_checker(job, interval, _interval_set=False, quiet=False, output=sys.stdout):<TAB><TAB>\"\"\"A text-based job status checker<TAB><TAB><TAB>Args:<TAB><TAB><TAB>job (BaseJob): The job to check.<TAB><TAB><TAB>interval (int): The interval at which to check.<TAB><TAB><TAB>_interval_set (bool): Was interval time set by user?<TAB><TAB><TAB>quiet (bool): If True, do not print status messages.<TAB><TAB><TAB>output (file): The file like object to write status messages to.<TAB><TAB><TAB>By default this is sys.stdout.<TAB><TAB><TAB>\"\"\"<TAB><TAB>status = job.status()<TAB><TAB>msg = status.value<TAB><TAB>prev_msg = msg<TAB><TAB>msg_len = len(msg)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>print('', file=output)",
        "target_block": "if not quiet:print('\\r%s: %s' % ('Job Status', msg), end='', file=output)while status.name not in ['DONE', 'CANCELLED', 'ERROR']:time.sleep(interval)status = job.status()msg = status.valueif status.name == 'QUEUED':msg += ' (%s)' % job.queue_position()if not _interval_set:interval = max(job.queue_position(), 2)else:if not _interval_set:interval = 2# Adjust length of message so there are no artifactsif len(msg) < msg_len:msg += ' ' * (msg_len - len(msg))elif len(msg) > msg_len:msg_len = len(msg)if msg != prev_msg and not quiet:print('\\r%s: %s' % ('Job Status', msg), end='', file=output)prev_msg = msgif not quiet:"
    },
    {
        "input_method": "def filter_useless_pass(source):<TAB><TAB>\"\"\"Yield code with useless \"pass\" lines removed.\"\"\"<TAB><TAB>try:<TAB><TAB><TAB>marked_lines = frozenset(useless_pass_line_numbers(source))<TAB><TAB>except (SyntaxError, tokenize.TokenError):<TAB><TAB><TAB>marked_lines = frozenset()<TAB><TAB><TAB>sio = io.StringIO(source)<TAB><TAB>for line_number, line in enumerate(sio.readlines(), start=1):<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>yield line",
        "target_block": "if line_number not in marked_lines:"
    },
    {
        "input_method": "def register_layer(self, layer):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Register the layer so that it's param will be trained.<TAB><TAB><TAB>But the output of the layer will not be stacked.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>layer.fix()<TAB><TAB><TAB>self.parameter_count += layer.parameter_count<TAB><TAB><TAB>self.parameters.extend(layer.parameters)<TAB><TAB><TAB>self.free_parameters.extend(layer.free_parameters)<TAB><TAB><TAB>self.training_monitors.extend(layer.training_monitors)<TAB><TAB><TAB>self.testing_monitors.extend(layer.testing_monitors)<TAB><TAB><TAB>self.updates.extend(layer.updates)<TAB><TAB><TAB>self.training_updates.extend(layer.training_updates)<TAB><TAB><TAB>self.input_variables.extend(layer.external_inputs)<TAB><TAB><TAB>self.target_variables.extend(layer.external_targets)<TAB><TAB><TAB><TAB>self.training_callbacks.extend(layer.training_callbacks)<TAB><TAB><TAB>self.testing_callbacks.extend(layer.testing_callbacks)<TAB><TAB><TAB>self.epoch_callbacks.extend(layer.epoch_callbacks)",
        "target_block": "if type(layer) == Block:"
    },
    {
        "input_method": "def _process_err(self, err_msg):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Processes the raw error message sent by the server<TAB><TAB><TAB>and close connection with current server.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB><extra_id_0> Some errors such as 'Invalid Subscription'<TAB><TAB><TAB># do not cause the server to close the connection.<TAB><TAB><TAB># For now we handle similar as other clients and close.<TAB><TAB><TAB>self._loop.create_task(self._close(Client.CLOSED, do_cbs))",
        "target_block": "if STALE_CONNECTION in err_msg:yield from self._process_op_err(ErrStaleConnection)returnif AUTHORIZATION_VIOLATION in err_msg:self._err = ErrAuthorizationelse:m = b'nats: ' + err_msg[0]self._err = NatsError(m.decode())do_cbs = Falseif not self.is_connecting:do_cbs = True# FIXME:"
    },
    {
        "input_method": "def gradient_crossplot(self, analytes=None, win=15, lognorm=True,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   bins=25, filt=False, samples=None,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   subset=None, figsize=(12, 12), save=False,<TAB><TAB><TAB><TAB><TAB><TAB><TAB>   colourful=True, mode='hist2d', recalc=True, **kwargs):<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>Plot analyte gradients against each other.<TAB><TAB><TAB><TAB>Parameters<TAB><TAB><TAB>----------<TAB><TAB><TAB>analytes : optional, array_like or str<TAB><TAB><TAB><TAB>The analyte(s) to plot. Defaults to all analytes.<TAB><TAB><TAB>lognorm : bool<TAB><TAB><TAB><TAB>Whether or not to log normalise the colour scale<TAB><TAB><TAB><TAB>of the 2D histogram.<TAB><TAB><TAB>bins : int<TAB><TAB><TAB><TAB>The number of bins in the 2D histogram.<TAB><TAB><TAB>filt : str, dict or bool<TAB><TAB><TAB><TAB>Either logical filter expression contained in a str,<TAB><TAB><TAB><TAB>a dict of expressions specifying the filter string to<TAB><TAB><TAB><TAB>use for each analyte or a boolean. Passed to `grab_filt`.<TAB><TAB><TAB>figsize : tuple<TAB><TAB><TAB><TAB>Figure size (width, height) in inches.<TAB><TAB><TAB>save : bool or str<TAB><TAB><TAB><TAB>If True, plot is saves as 'crossplot.png', <extra_id_0><TAB><TAB><TAB><TAB>fig.savefig(self.report_dir + '/g_crossplot.png', dpi=200)<TAB><TAB><TAB><TAB>return fig, axes",
        "target_block": "if str plot issaves as str.colourful : boolWhether or not the plot should be colourful :).mode : str'hist2d' (default) or 'scatter'recalc : boolWhether to re-calculate the gradients, or use existing gradients.Returns-------(fig, axes)\"\"\"if analytes is None:analytes = self.analytesif self.focus_stage in ['ratio', 'calibrated']:analytes = [a for a in analytes if self.internal_standard not in a]# sort analytestry:analytes = sorted(analytes, key=lambda x: float(re.findall('[0-9.-]+', x)[0]))except IndexError:analytes = sorted(analytes)samples = self._get_samples(subset)# calculate gradientsself.get_gradients(analytes=analytes, win=win, filt=filt, subset=subset, recalc=recalc)# self.get_focus(filt=filt, samples=samples, subset=subset)# grads = calc_grads(self.focus.uTime, self.focus, analytes, win)fig, axes = plot.crossplot(dat=self.gradients, keys=analytes, lognorm=lognorm,   bins=bins, figsize=figsize, colourful=colourful,   focus_stage=self.focus_stage, cmap=self.cmaps,   denominator=self.internal_standard, mode=mode)if save:"
    },
    {
        "input_method": "def maybe_run(self, job):<TAB><TAB><TAB>\"\"\"check location dependencies, and run <extra_id_0><TAB><TAB><TAB><TAB>indices = None<TAB><TAB><TAB><TAB>self.submit_task(job, indices)<TAB><TAB><TAB>return True",
        "target_block": "if they are met.\"\"\"msg_id = job.msg_idself.log.debug(\"Attempting to assign task %s\", msg_id)if not self.targets:# no engines, definitely can't runreturn Falseif job.follow or job.targets or job.blacklist or self.hwm:# we need a can_run filterdef can_run(idx):# check hwmif self.hwm and self.loads[idx] == self.hwm:return Falsetarget = self.targets[idx]# check blacklistif target in job.blacklist:return False# check targetsif job.targets and target not in job.targets:return False# check followreturn job.follow.check(self.completed[target], self.failed[target])indices = filter(can_run, range(len(self.targets)))if not indices:# couldn't runif job.follow.all:# check follow for impossibilitydests = set()relevant = set()if job.follow.success:relevant = self.all_completedif job.follow.failure:relevant = relevant.union(self.all_failed)for m in job.follow.intersection(relevant):dests.add(self.destinations[m])if len(dests) > 1:self.depending[msg_id] = jobself.fail_unreachable(msg_id)return Falseif job.targets:# check blacklist+targets for impossibilityjob.targets.difference_update(job.blacklist)if not job.targets or not job.targets.intersection(self.targets):self.depending[msg_id] = jobself.fail_unreachable(msg_id)return Falsereturn Falseelse:"
    },
    {
        "input_method": "def serialCmdPwdAuth(self, password_str):<TAB><TAB><TAB>\"\"\" Password step of set commands<TAB><TAB><TAB><TAB>This method is normally called within another serial command, so it<TAB><TAB><TAB>does not issue a termination string.  Any default password is set<TAB><TAB><TAB>in the caller parameter list, never here.<TAB><TAB><TAB><TAB>Args:<TAB><TAB><TAB><TAB>password_str (str): Required password.<TAB><TAB><TAB><TAB>Returns:<TAB><TAB><TAB><TAB>bool: True on completion and ACK.<TAB><TAB><TAB>\"\"\"<TAB><TAB><TAB>result = False<TAB><TAB><TAB>try:<TAB><TAB><TAB><TAB>req_start = \"0150310228\" + binascii.hexlify(password_str) + \"2903\"<TAB><TAB><TAB><TAB>req_crc = self.calc_crc16(req_start[2:].decode(\"hex\"))<TAB><TAB><TAB><TAB>req_str = req_start + req_crc<TAB><TAB><TAB><TAB>self.m_serial_port.write(req_str.decode(\"hex\"))<TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>ekm_log(\"Password call failure by exception(\" + self.getContext() + \")\")<TAB><TAB><TAB><TAB><TAB>ekm_log(traceback.format_exc(sys.exc_info()))<TAB><TAB><TAB><TAB>return result",
        "target_block": "if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":ekm_log(\"Password accepted (\" + self.getContext() + \")\")result = Trueelse:ekm_log(\"Password call failure no 06(\" + self.getContext() + \")\")except:"
    },
    {
        "input_method": "def depth_file_reader(depth_file):<TAB><TAB>\"\"\"<TAB><TAB>Function that parse samtools depth file and creates 3 dictionaries that<TAB><TAB>will be useful to make the outputs of this script, both the tabular file<TAB><TAB>and the json file that may be imported by pATLAS<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>depth_file: textIO<TAB><TAB><TAB>the path to depth file for each sample<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>depth_dic_coverage: dict<TAB><TAB><TAB><TAB>dictionary with the coverage per position for each plasmid<TAB><TAB>\"\"\"<TAB><TAB><TAB># dict to store the mean coverage for each reference<TAB><TAB>depth_dic_coverage = {}<TAB><TAB><TAB>for line in depth_file:<TAB><TAB><TAB>tab_split = line.split()  # split by any white space<TAB><TAB><TAB>reference = \"_\".join(tab_split[0].strip().split(\"_\")[0:3])  # store<TAB><TAB><TAB># only the gi for the reference<TAB><TAB><TAB>position = tab_split[1]<TAB><TAB><TAB>num_reads_align = float(tab_split[2].rstrip())<TAB><TAB><TAB><TAB><extra_id_0> {} kb\".format(<TAB><TAB><TAB>asizeof(depth_dic_coverage)/1024))<TAB><TAB><TAB>return depth_dic_coverage",
        "target_block": "if reference not in depth_dic_coverage:depth_dic_coverage[reference] = {}depth_dic_coverage[reference][position] = num_reads_alignlogger.info(\"Finished parsing depth file.\")depth_file.close()logger.debug(\"Size of dict_cov:"
    },
    {
        "input_method": "def confirm(pid, record, template, **kwargs):<TAB><TAB>\"\"\"Confirm email address.\"\"\"<TAB><TAB>recid = int(pid.pid_value)<TAB><TAB><TAB>token = request.view_args['token']<TAB><TAB><TAB># Validate token<TAB><TAB>data = EmailConfirmationSerializer.compat_validate_token(token)<TAB><TAB><extra_id_0><TAB><TAB><TAB>abort(404)<TAB><TAB><TAB>r.confirm_email()<TAB><TAB>db.session.commit()<TAB><TAB>flash(_(\"Email validated and access request submitted.\"), category='info')<TAB><TAB><TAB>return redirect(url_for(\"invenio_records_ui.recid\", pid_value=recid))",
        "target_block": "if data is None:flash(_(\"Invalid confirmation link.\"), category='danger')return redirect(url_for(\"invenio_records_ui.recid\", pid_value=recid))# Validate request exists.r = AccessRequest.query.get(data['id'])if not r:abort(404)# Confirm email address.if r.status != RequestStatus.EMAIL_VALIDATION:"
    },
    {
        "input_method": "def record_error(hostname, exc_info, preceding_stack=None, error_threshold=None, additional_info=None):<TAB><TAB>''' Helper function to record errors to the flawless backend '''<TAB><TAB>stack = []<TAB><TAB>exc_type, exc_value, sys_traceback = exc_info<TAB><TAB><TAB>while sys_traceback is not None:<TAB><TAB><TAB>stack.append(sys_traceback)<TAB><TAB><TAB>sys_traceback = sys_traceback.tb_next<TAB><TAB><TAB>stack_lines = []<TAB><TAB>for row in preceding_stack or []:<TAB><TAB><TAB>stack_lines.append(<TAB><TAB><TAB><TAB>api_ttypes.StackLine(filename=os.path.abspath(row[0]), line_number=row[1],<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> function_name=row[2], text=row[3])<TAB><TAB><TAB>)<TAB><TAB><TAB>for index, tb in enumerate(stack):<TAB><TAB><TAB>filename = tb.tb_frame.f_code.co_filename<TAB><TAB><TAB>func_name = tb.tb_frame.f_code.co_name<TAB><TAB><TAB>lineno = tb.tb_lineno<TAB><TAB><TAB>line = linecache.getline(filename, lineno, tb.tb_frame.f_globals)<TAB><TAB><TAB>frame_locals = None<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB>error_count = info.mark_reported()<TAB><TAB><TAB>_send_request(<TAB><TAB><TAB><TAB>api_ttypes.RecordErrorRequest(<TAB><TAB><TAB><TAB><TAB>traceback=stack_lines,<TAB><TAB><TAB><TAB><TAB>exception_message=repr(exc_value),<TAB><TAB><TAB><TAB><TAB>exception_type=exc_type.__module__ + \".\" + exc_type.__name__,<TAB><TAB><TAB><TAB><TAB>hostname=hostname,<TAB><TAB><TAB><TAB><TAB>error_threshold=error_threshold,<TAB><TAB><TAB><TAB><TAB>additional_info=additional_info,<TAB><TAB><TAB><TAB><TAB>error_count=error_count,<TAB><TAB><TAB><TAB>)<TAB><TAB><TAB>)",
        "target_block": "if index >= (len(stack) - NUM_FRAMES_TO_SAVE):# Include some limits on max string length & number of variables to keep things from getting# out of handframe_locals = dict((k, _myrepr(k, v)) for k, v inlist(tb.tb_frame.f_locals.items())[:MAX_LOCALS] if k != \"self\")if \"self\" in tb.tb_frame.f_locals and hasattr(tb.tb_frame.f_locals[\"self\"], \"__dict__\"):frame_locals.update(dict((\"self.\" + k, _myrepr(k, v)) for k, v in list(tb.tb_frame.f_locals[\"self\"].__dict__.items())[:MAX_LOCALS] if k != \"self\"))stack_lines.append(api_ttypes.StackLine(filename=os.path.abspath(filename), line_number=lineno, function_name=func_name, text=line, frame_locals=frame_locals))# Check LRU cache & potentially do not send error report if this client has already reported this error# several times.key = CachedErrorInfo.get_hash_key(stack_lines)info = ERROR_CACHE.get(key) or CachedErrorInfo()info.increment()ERROR_CACHE[key] = infoif info.should_report():"
    },
    {
        "input_method": "def _emit_no_member(node, owner, owner_name, ignored_mixins=True, ignored_none=True):<TAB><TAB>\"\"\"Try to see <extra_id_0><TAB><TAB><TAB><TAB>return True<TAB><TAB>return True",
        "target_block": "if no-member should be emitted for the given owner.The following cases are ignored:* the owner is a function and it has decorators.* the owner is an instance and it has __getattr__, __getattribute__ implemented* the module is explicitly ignored from no-member checks* the owner is a class and the name can be found in its metaclass.* The access node is protected by an except handler, which handles  AttributeError, Exception or bare except.\"\"\"# pylint: disable=too-many-return-statementsif node_ignores_exception(node, AttributeError):return Falseif ignored_none and isinstance(owner, astroid.Const) and owner.value is None:return Falseif is_super(owner) or getattr(owner, \"type\", None) == \"metaclass\":return Falseif ignored_mixins and owner_name[-5:].lower() == \"mixin\":return Falseif isinstance(owner, astroid.FunctionDef) and owner.decorators:return Falseif isinstance(owner, (astroid.Instance, astroid.ClassDef)):if owner.has_dynamic_getattr():# Issue #2565: Don't ignore enums, as they have a `__getattr__` but it's not# invoked at this point.try:metaclass = owner.metaclass()except exceptions.MroError:return Falseif metaclass:return metaclass.qname() == \"enum.EnumMeta\"return Falseif not has_known_bases(owner):return Falseif isinstance(owner, objects.Super):# Verify if we are dealing with an invalid Super object.# If it is invalid, then there's no point in checking that# it has the required attribute. Also, don't fail if the# MRO is invalid.try:owner.super_mro()except (exceptions.MroError, exceptions.SuperError):return Falseif not all(map(has_known_bases, owner.type.mro())):return Falseif isinstance(owner, astroid.Module):try:owner.getattr(\"__getattr__\")return Falseexcept astroid.NotFoundError:passif node.attrname.startswith(\"_\" + owner_name):# Test if an attribute has been mangled ('private' attribute)unmangled_name = node.attrname.split(\"_\" + owner_name)[-1]try:if owner.getattr(unmangled_name, context=None) is not None:return Falseexcept astroid.NotFoundError:"
    },
    {
        "input_method": "def raise_if_freezed(self):<TAB><TAB><TAB>'''raise `InvalidOperationException` <extra_id_0><TAB><TAB><TAB><TAB>name = type(self).__name__<TAB><TAB><TAB><TAB>raise InvalidOperationException('obj {name} is freezed.'.format(name=name))",
        "target_block": "if is freezed.'''if self.is_freezed:"
    },
    {
        "input_method": "def get_context(self, name, value, attrs):<TAB><TAB><TAB>\"\"\"Get the context to render this widget with.\"\"\"<TAB><TAB><TAB><extra_id_0> self.image_preview_id(name),<TAB><TAB><TAB><TAB>})<TAB><TAB><TAB><TAB>return context",
        "target_block": "if self.has_template_widget_rendering:context = super(ClearableFileInputWithImagePreview, self).get_context(name, value, attrs)else:# Build the context manually.context = {}context['widget'] = {'name': name,'is_hidden': self.is_hidden,'required': self.is_required,'value': self._format_value(value),'attrs': self.build_attrs(self.attrs, attrs),'template_name': self.template_name,'type': self.input_type,}# It seems Django 1.11's ClearableFileInput doesn't add everything to the 'widget' key, so we can't use it# in MultiWidget. Add it manually here.checkbox_name = self.clear_checkbox_name(name)checkbox_id = self.clear_checkbox_id(checkbox_name)context['widget'].update({'checkbox_name': checkbox_name,'checkbox_id': checkbox_id,'is_initial': self.is_initial(value),'input_text': self.input_text,'initial_text': self.initial_text,'clear_checkbox_label': self.clear_checkbox_label,})if value and hasattr(value, \"url\"):context['widget'].update({'hidden_field_id': self.get_hidden_field_id(name),'point_stage_id': self.get_point_stage_id(name),'ppoi_id': self.get_ppoi_id(name),'sized_url': self.get_sized_url(value),'image_preview_id':"
    },
    {
        "input_method": "def export_mt_variants(variants, sample_id):<TAB><TAB>\"\"\"Export mitochondrial variants for a case to create a MT excel report<TAB><TAB><TAB>Args:<TAB><TAB><TAB>variants(list): all MT variants for a case, sorted by position<TAB><TAB><TAB>sample_id(str) : the id of a sample within the case<TAB><TAB><TAB>Returns:<TAB><TAB><TAB>document_lines(list): list of lines to include in the document<TAB><TAB>\"\"\"<TAB><TAB>document_lines = []<TAB><TAB>for variant in variants:<TAB><TAB><TAB>line = []<TAB><TAB><TAB>position = variant.get('position')<TAB><TAB><TAB>change = '>'.join([variant.get('reference'),variant.get('alternative')])<TAB><TAB><TAB>line.append(position)<TAB><TAB><TAB>line.append(change)<TAB><TAB><TAB>line.append(str(position)+change)<TAB><TAB><TAB>genes = []<TAB><TAB><TAB>prot_effect = []<TAB><TAB><TAB>for gene in variant.get('genes'):<TAB><TAB><TAB><TAB>genes.append(gene.get('hgnc_symbol',''))<TAB><TAB><TAB><TAB>for transcript in gene.get('transcripts'):<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB>ref_ad = sample['allele_depths'][0]<TAB><TAB><TAB><TAB><TAB>alt_ad = sample['allele_depths'][1]<TAB><TAB><TAB>line.append(ref_ad)<TAB><TAB><TAB>line.append(alt_ad)<TAB><TAB><TAB>document_lines.append(line)<TAB><TAB>return document_lines",
        "target_block": "if transcript.get('is_canonical') and transcript.get('protein_sequence_name'):prot_effect.append(urllib.parse.unquote(transcript.get('protein_sequence_name')))line.append(','.join(prot_effect))line.append(','.join(genes))ref_ad = ''alt_ad = ''for sample in variant['samples']:if sample.get('sample_id') == sample_id:"
    }
]