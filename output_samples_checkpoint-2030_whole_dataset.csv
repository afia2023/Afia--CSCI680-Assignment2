input_method,target_block,generated_output,is_correct
"def remove_builtin(self, key, orig):<TAB><TAB><TAB>""""""Remove an added builtin and re-set the original.""""""<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB>__builtin__.__dict__[key] = orig",if orig is BuiltinUndefined:del __builtin__.__dict__[key]else:,if key in self.__builtin__.__dict__.keys():,FALSE
"def allstack(vals, depth=0):<TAB><TAB>""""""<TAB><TAB>If an ndarray has been split into multiple chunks by splitting it along<TAB><TAB>each axis at a number of locations, this function rebuilds the<TAB><TAB>original array from chunks.<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>vals : nested lists of ndarrays<TAB><TAB><TAB>each level of nesting of the lists representing a dimension of<TAB><TAB><TAB>the original array.<TAB><TAB>""""""<TAB><TAB><extra_id_0><TAB><TAB><TAB>return concatenate([allstack(x, depth+1) for x in vals], axis=depth)","if type(vals[0]) is ndarray:return concatenate(vals, axis=depth)else:",if len(vals) == 1:return valselse:,FALSE
"def _normalize_data(stream, date_format=None):<TAB><TAB>""""""<TAB><TAB>This function is meant to normalize data for upload to the Luminoso<TAB><TAB>Analytics system. Currently it only normalizes dates.<TAB><TAB><TAB>If date_format is not specified, or <extra_id_0><TAB><TAB><TAB><TAB><TAB># ValueErrors cover the cases when date_format does not match<TAB><TAB><TAB><TAB><TAB># the actual format of the date, both for epoch and non-epoch<TAB><TAB><TAB><TAB><TAB># times.<TAB><TAB><TAB><TAB><TAB>logger.exception('%s does not match the date format %s;'<TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB><TAB> % (doc['date'], date_format))<TAB><TAB><TAB>yield doc","if there's no date in a particular doc,the the doc is yielded unchanged.""""""for doc in stream:if 'date' in doc and date_format is not None:try:doc['date'] = _convert_date(doc['date'], date_format)except ValueError:","if date_format is None, it will be used to normalize dates.If date_format is None, it will be used to normalize dates.If date_format is None, it will be used to normalize dates.If date_format",FALSE
"def mel_to_hz(mels, htk=False):<TAB><TAB>""""""Convert mel bin numbers to frequencies<TAB><TAB><TAB>Examples<TAB><TAB>--------<TAB><TAB>>>> librosa.mel_to_hz(3)<TAB><TAB>200.<TAB><TAB><TAB>>>> librosa.mel_to_hz([1,2,3,4,5])<TAB><TAB>array([  66.667,  133.333,  200.   ,  266.667,  333.333])<TAB><TAB><TAB>Parameters<TAB><TAB>----------<TAB><TAB>mels<TAB><TAB>  : np.ndarray [shape=(n,)], float<TAB><TAB><TAB>mel bins to convert<TAB><TAB>htk<TAB><TAB>   : bool<TAB><TAB><TAB>use HTK formula instead of Slaney<TAB><TAB><TAB>Returns<TAB><TAB>-------<TAB><TAB>frequencies   : np.ndarray [shape=(n,)]<TAB><TAB><TAB>input mels in Hz<TAB><TAB><TAB>See Also<TAB><TAB>--------<TAB><TAB>hz_to_mel<TAB><TAB>""""""<TAB><TAB><TAB>mels = np.asanyarray(mels)<TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB># If we have scalar data, check directly<TAB><TAB><TAB>freqs = min_log_hz * np.exp(logstep * (mels - min_log_mel))<TAB><TAB><TAB>return freqs","if htk:return 700.0 * (10.0**(mels / 2595.0) - 1.0)# Fill in the linear scalef_min = 0.0f_sp = 200.0 / 3freqs = f_min + f_sp * mels# And now the nonlinear scalemin_log_hz = 1000.0 # beginning of log region (Hz)min_log_mel = (min_log_hz - f_min) / f_sp   # same (Mels)logstep = np.log(6.4) / 27.0# step size for log regionif mels.ndim:# If we have vector data, vectorizelog_t = (mels >= min_log_mel)freqs[log_t] = min_log_hz * np.exp(logstep * (mels[log_t] - min_log_mel))elif mels >= min_log_mel:",if htk:min_log_hz = np.exp(logstep * (mels - min_log_mel))else:,FALSE
"def setDecode(self, decodeTable):<TAB><TAB><TAB>""""""Store decodeTable,<TAB><TAB><TAB>and compute lengthTable, minLength, maxLength from encodings.<TAB><TAB><TAB>""""""<TAB><TAB><TAB>self.decodeTable = decodeTable<TAB><TAB><TAB>#set of symbols with unknown length<TAB><TAB><TAB>todo = set(decodeTable)<TAB><TAB><TAB>#bit size under investigation<TAB><TAB><TAB>maskLength = 0<TAB><TAB><TAB>lengthTable = {}<TAB><TAB><TAB>while todo:<TAB><TAB><TAB><TAB>mask = (1<<maskLength)-1<TAB><TAB><TAB><TAB>#split the encodings that we didn't find yet using b bits<TAB><TAB><TAB><TAB>splitSymbols = defaultdict(list)<TAB><TAB><TAB><TAB>for s in todo: splitSymbols[s&mask].append(s)<TAB><TAB><TAB><TAB>#unique encodings have a length of maskLength bits<TAB><TAB><TAB><TAB>#set length, and remove from todo list<TAB><TAB><TAB><TAB>for s,subset in splitSymbols.items():<TAB><TAB><TAB><TAB><TAB><extra_id_0><TAB><TAB><TAB><TAB><TAB><TAB>lengthTable[self.decodeTable[s]] = maskLength<TAB><TAB><TAB><TAB><TAB><TAB>todo.remove(s)<TAB><TAB><TAB><TAB>#now investigate with longer mask<TAB><TAB><TAB><TAB>maskLength +=1<TAB><TAB><TAB>#save result<TAB><TAB><TAB>self.lengthTable = lengthTable<TAB><TAB><TAB>self.minLength = min(lengthTable.values())<TAB><TAB><TAB>self.maxLength = max(lengthTable.values())<TAB><TAB><TAB>self.switchToPrefix()",if len(subset)==1:,if s not in lengthTable:,FALSE
Overall Accuracy,,,7.02%
